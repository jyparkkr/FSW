{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c3b92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir=./outputs/demo/dataset=MNIST/seed=10_epoch=1_lr=0.001_alpha=0.001_tau=10\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cl_gym as cl\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "seed = 10\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# torch.backends.cudnn.enabled = False\n",
    "\n",
    "def make_params() -> dict:\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import uuid\n",
    "\n",
    "    params = {\n",
    "            # dataset\n",
    "            'dataset': \"MNIST\",\n",
    "            # 'dataset': \"FMNIST\",\n",
    "            'random_class_idx': True,\n",
    "\n",
    "            # benchmark\n",
    "            'seed': seed,\n",
    "            'num_tasks': 5,\n",
    "            'epochs_per_task': 1,\n",
    "            # 'per_task_examples': np.inf,\n",
    "            'per_task_examples': 10000,\n",
    "            'per_task_memory_examples': 20,\n",
    "            'batch_size_train': 10, # should be smaller than per_task_memory_examples \n",
    "            'batch_size_memory': 10,\n",
    "            'batch_size_validation': 256,\n",
    "            'tau': 10,\n",
    "            # 'tau': 0.0,\n",
    "\n",
    "            # algorithm\n",
    "            'optimizer': 'sgd',\n",
    "            'learning_rate': 0.001,\n",
    "            'momentum': 0.9,\n",
    "            'learning_rate_decay': 1.0,\n",
    "            'criterion': torch.nn.CrossEntropyLoss(),\n",
    "            'device': torch.device('cuda:7' if torch.cuda.is_available() else 'cpu'),\n",
    "             \n",
    "            # sample selection\n",
    "            'alpha':0.001,\n",
    "            'lambda': .01,\n",
    "            'lambda_old': 0.0,\n",
    "              }\n",
    "\n",
    "#     trial_id = str(uuid.uuid4())\n",
    "    trial_id = f\"demo/dataset={params['dataset']}/seed={params['seed']}_epoch={params['epochs_per_task']}_lr={params['learning_rate']}_alpha={params['alpha']}_tau={params['tau']}\"\n",
    "    params['trial_id'] = trial_id\n",
    "    params['output_dir'] = os.path.join(\"./outputs/{}\".format(trial_id))\n",
    "    print(f\"output_dir={params['output_dir']}\")\n",
    "    Path(params['output_dir']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return params\n",
    "\n",
    "params = make_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b43ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 2 5 6 3 1 0 7 4 9]\n"
     ]
    }
   ],
   "source": [
    "from datasets import MNIST\n",
    "from datasets import FashionMNIST\n",
    "from datasets import CIFAR10, CIFAR100\n",
    "\n",
    "if params['dataset'] == 'MNIST':\n",
    "    benchmark = MNIST(num_tasks=params['num_tasks'],\n",
    "                    per_task_memory_examples=params['per_task_memory_examples'],\n",
    "                    per_task_examples = params['per_task_examples'],\n",
    "                    random_class_idx = params['random_class_idx'])\n",
    "    input_dim = (28, 28)\n",
    "elif params['dataset'] == 'FashionMNIST':\n",
    "    benchmark = FashionMNIST(num_tasks=params['num_tasks'],\n",
    "                            per_task_memory_examples=params['per_task_memory_examples'],\n",
    "                            per_task_examples = params['per_task_examples'],\n",
    "                            random_class_idx = params['random_class_idx'])\n",
    "    input_dim = (28, 28)\n",
    "elif params['dataset'] == 'CIFAR10':\n",
    "    benchmark = CIFAR10(num_tasks=params['num_tasks'],\n",
    "                        per_task_memory_examples=params['per_task_memory_examples'],\n",
    "                        per_task_examples = params['per_task_examples'],\n",
    "                        random_class_idx = params['random_class_idx'])\n",
    "    input_dim = (3, 32, 32)\n",
    "elif params['dataset'] == 'CIFAR100':        \n",
    "    benchmark = CIFAR100(num_tasks=params['num_tasks'],\n",
    "                        per_task_memory_examples=params['per_task_memory_examples'],\n",
    "                        per_task_examples = params['per_task_examples'],\n",
    "                        random_class_idx = params['random_class_idx'])\n",
    "    input_dim = (3, 32, 32)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "class_idx = benchmark.class_idx\n",
    "num_classes = len(class_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c7b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms.gss import GSSGreedy as Algorithm\n",
    "from metrics import MetricCollector2\n",
    "from backbones import MLP2Layers2\n",
    "\n",
    "backbone = MLP2Layers2(\n",
    "    input_dim=input_dim, \n",
    "    hidden_dim_1=256, \n",
    "    hidden_dim_2=256, \n",
    "    output_dim=num_classes,\n",
    "    class_idx=class_idx,\n",
    "    config=params\n",
    "    ).to(params['device'])\n",
    "algorithm = Algorithm(backbone, benchmark, params, requires_memory=True)\n",
    "metric_manager_callback = MetricCollector2(num_tasks=params['num_tasks'],\n",
    "                                                        eval_interval='epoch',\n",
    "                                                        epochs_per_task=params['epochs_per_task'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8efa3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainers.baselines import BaseMemoryContinualTrainer as ContinualTrainer\n",
    "trainer = ContinualTrainer(algorithm, params, callbacks=[metric_manager_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c1bb4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Task 1 -----------------------\n",
      "[1] Eval metrics for task 1 >> {'accuracy': 97.22394664374512, 'loss': 0.0003083910094806228, 'fairness': 2.2627063336676025}\n",
      "---------------------------- Task 2 -----------------------\n",
      "[2] Eval metrics for task 1 >> {'accuracy': 65.15169603489169, 'loss': 0.006459504514486934, 'fairness': 5.0741766550467196}\n",
      "[2] Eval metrics for task 2 >> {'accuracy': 97.67464448542836, 'loss': 0.00031341936237908697, 'fairness': 0.028904575114446773}\n",
      "---------------------------- Task 3 -----------------------\n",
      "[3] Eval metrics for task 1 >> {'accuracy': 46.11249064832943, 'loss': 0.011692871362832584, 'fairness': 3.476831733600752}\n",
      "[3] Eval metrics for task 2 >> {'accuracy': 72.0052753775148, 'loss': 0.0050642635049046696, 'fairness': 10.458190175721093}\n",
      "[3] Eval metrics for task 3 >> {'accuracy': 99.15252758756053, 'loss': 0.00015256537649876032, 'fairness': 0.1426265974615093}\n",
      "---------------------------- Task 4 -----------------------\n",
      "[4] Eval metrics for task 1 >> {'accuracy': 42.974806201550386, 'loss': 0.014248252390388953, 'fairness': 7.025193798449614}\n",
      "[4] Eval metrics for task 2 >> {'accuracy': 67.74869636855556, 'loss': 0.006293990354280214, 'fairness': 5.528965426851534}\n",
      "[4] Eval metrics for task 3 >> {'accuracy': 87.75461246565186, 'loss': 0.002197079169444549, 'fairness': 10.13085008941423}\n",
      "[4] Eval metrics for task 4 >> {'accuracy': 98.46839514015723, 'loss': 0.00017509201033211563, 'fairness': 0.511196696577465}\n",
      "---------------------------- Task 5 -----------------------\n",
      "[5] Eval metrics for task 1 >> {'accuracy': 43.06762650621587, 'loss': 0.01537836869716169, 'fairness': 3.3389443356732413}\n",
      "[5] Eval metrics for task 2 >> {'accuracy': 57.003215780259694, 'loss': 0.008293276928566597, 'fairness': 2.182587977569117}\n",
      "[5] Eval metrics for task 3 >> {'accuracy': 84.55314694465042, 'loss': 0.003126309940587113, 'fairness': 13.067998429798926}\n",
      "[5] Eval metrics for task 4 >> {'accuracy': 70.86258238704042, 'loss': 0.00569178247119326, 'fairness': 20.66802985785754}\n",
      "[5] Eval metrics for task 5 >> {'accuracy': 95.65337623304717, 'loss': 0.0004856954943828401, 'fairness': 1.6989659258122958}\n",
      "final avg-acc 70.22798957024273\n",
      "final avg-forget 34.25823555968121\n"
     ]
    }
   ],
   "source": [
    "trainer.run()\n",
    "print(\"final avg-acc\", metric_manager_callback.meters['accuracy'].compute_final())\n",
    "print(\"final avg-forget\", metric_manager_callback.meters['forgetting'].compute_final())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ebfb58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 16.261474225268337,\n",
       " 17.819397588728183,\n",
       " 18.874876258925788,\n",
       " 16.154054741603943]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_manager_callback.meters['fairness'].get_eer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7039ed8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[97.224,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [65.152, 97.675,  0.   ,  0.   ,  0.   ],\n",
       "       [46.112, 72.005, 99.153,  0.   ,  0.   ],\n",
       "       [42.975, 67.749, 87.755, 98.468,  0.   ],\n",
       "       [43.068, 57.003, 84.553, 70.863, 95.653]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_manager_callback.meters['accuracy'].get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "173c4765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.10503304451898"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_manager_callback.meters['accuracy'].compute_overall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "706ad34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:79.10503304451898\n",
      "fairness:13.821960562905252\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy:{np.mean(metric_manager_callback.meters['accuracy'].compute_overall())}\")\n",
    "print(f\"fairness:{np.mean(metric_manager_callback.meters['fairness'].compute_overall())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b848e47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
