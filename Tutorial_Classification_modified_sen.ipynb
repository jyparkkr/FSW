{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c3b92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir=./outputs/seed=1_epoch=1q__\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cl_gym as cl\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "# dataset = 'MNIST'\n",
    "seed = 1\n",
    "dataset = 'FMNIST'\n",
    "# dataset = 'CIFAR10'\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "def make_params() -> dict:\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import uuid\n",
    "\n",
    "    params = {\n",
    "            # dataset\n",
    "            'dataset': \"FMNIST\",\n",
    "\n",
    "            # benchmark\n",
    "            'seed': seed,\n",
    "            'num_tasks': 5,\n",
    "            'epochs_per_task': 1,\n",
    "            'per_task_examples': np.inf,\n",
    "            'per_task_memory_examples': 64,\n",
    "            'batch_size_train': 64,\n",
    "            'batch_size_memory': 64,\n",
    "            'batch_size_validation': 256,\n",
    "            'lambda': 10,\n",
    "\n",
    "            # algorithm\n",
    "            'optimizer': 'Adam',\n",
    "            'learning_rate': 0.001,\n",
    "            'momentum': 0.8,\n",
    "            'learning_rate_decay': 1.0,\n",
    "            'criterion': torch.nn.CrossEntropyLoss(),\n",
    "            'device': torch.device('cuda:6' if torch.cuda.is_available() else 'cpu'), }\n",
    "\n",
    "#     trial_id = str(uuid.uuid4())\n",
    "    trial_id = f\"seed={params['seed']}_epoch={params['epochs_per_task']}q__\"\n",
    "    params['trial_id'] = trial_id\n",
    "    params['output_dir'] = os.path.join(\"./outputs/{}\".format(trial_id))\n",
    "    print(f\"output_dir={params['output_dir']}\")\n",
    "    Path(params['output_dir']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return params\n",
    "\n",
    "params = make_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b43ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if params['dataset'] == 'MNIST':\n",
    "#     benchmark = cl.benchmarks.SplitMNIST(num_tasks=params['num_tasks'],\n",
    "#                                         per_task_memory_examples=params['per_task_memory_examples'])\n",
    "#     label_li = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "# elif params['dataset'] == 'FMNIST':\n",
    "#     from datasets.FashionMNIST import FashionMNIST\n",
    "#     benchmark = FashionMNIST(num_tasks=params['num_tasks'],\n",
    "#                              per_task_memory_examples=params['per_task_memory_examples'],\n",
    "#                              per_task_examples = min(params['per_task_examples'], 12000))\n",
    "\n",
    "#     label_li = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', \n",
    "#                   'Ankel boot']\n",
    "#     # base_transform = transforms.Compose([transforms.ToTensor()])\n",
    "#     train_dataset = benchmark.fashion_mnist_train\n",
    "#     test_dataset = benchmark.fashion_mnist_test\n",
    "    \n",
    "# elif params['dataset'] == 'CIFAR10':\n",
    "#     label_li = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "#     benchmark = cl.benchmarks.SplitCIFAR10(num_tasks=params['num_tasks'],\n",
    "#                                         per_task_memory_examples=params['per_task_memory_examples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c7b886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "---------------------------- Task 1 -----------------------\n",
      "[1] Eval metrics for task 1 >> {'accuracy': 99.76359338061465, 'accuracy_s0': 99.6343692870201, 'accuracy_s1': 99.9020568070519, 'loss': 1.4221462080955295e-05}\n",
      "training_task_end\n",
      "load_memory_joint: len(train_loader.dataset)=64\n",
      "---------------------------- Task 2 -----------------------\n",
      "loss_matrix.shape=torch.Size([12089, 2])\n",
      "len(select_curr_indexes)=12088\n",
      "[2] Eval metrics for task 1 >> {'accuracy': 91.34751773049645, 'accuracy_s0': 89.57952468007312, 'accuracy_s1': 93.24191968658178, 'loss': 0.0015152125705218485}\n",
      "[2] Eval metrics for task 2 >> {'accuracy': 97.01273261508325, 'accuracy_s0': 97.36070381231671, 'accuracy_s1': 96.66339548577037, 'loss': 0.000307492563367824}\n",
      "training_task_end\n",
      "load_memory_joint: len(train_loader.dataset)=128\n",
      "---------------------------- Task 3 -----------------------\n",
      "loss_matrix.shape=torch.Size([11263, 2])\n",
      "len(select_curr_indexes)=11262\n",
      "[3] Eval metrics for task 1 >> {'accuracy': 91.91489361702128, 'accuracy_s0': 89.21389396709324, 'accuracy_s1': 94.80901077375123, 'loss': 0.0013525738149669998}\n",
      "[3] Eval metrics for task 2 >> {'accuracy': 67.8256611165524, 'accuracy_s0': 64.80938416422288, 'accuracy_s1': 70.85377821393523, 'loss': 0.006455890462167349}\n",
      "[3] Eval metrics for task 3 >> {'accuracy': 98.025613660619, 'accuracy_s0': 97.6470588235294, 'accuracy_s1': 98.40255591054313, 'loss': 0.0002276852206111336}\n",
      "training_task_end\n",
      "load_memory_joint: len(train_loader.dataset)=192\n",
      "---------------------------- Task 4 -----------------------\n",
      "loss_matrix.shape=torch.Size([12183, 2])\n",
      "len(select_curr_indexes)=12182\n",
      "[4] Eval metrics for task 1 >> {'accuracy': 86.14657210401892, 'accuracy_s0': 82.81535648994516, 'accuracy_s1': 89.71596474045054, 'loss': 0.0027283414035824175}\n",
      "[4] Eval metrics for task 2 >> {'accuracy': 66.65034280117531, 'accuracy_s0': 65.78690127077223, 'accuracy_s1': 67.5171736997056, 'loss': 0.007786211082447287}\n",
      "[4] Eval metrics for task 3 >> {'accuracy': 69.79722518676627, 'accuracy_s0': 74.33155080213903, 'accuracy_s1': 65.28221512247072, 'loss': 0.005748638316686787}\n",
      "[4] Eval metrics for task 4 >> {'accuracy': 98.6908358509567, 'accuracy_s0': 98.76160990712074, 'accuracy_s1': 98.62340216322517, 'loss': 0.00015715729493989206}\n",
      "training_task_end\n",
      "load_memory_joint: len(train_loader.dataset)=256\n",
      "---------------------------- Task 5 -----------------------\n",
      "loss_matrix.shape=torch.Size([11800, 2])\n",
      "len(select_curr_indexes)=11799\n",
      "[5] Eval metrics for task 1 >> {'accuracy': 77.21040189125296, 'accuracy_s0': 72.02925045703839, 'accuracy_s1': 82.76199804113614, 'loss': 0.004621746269523675}\n",
      "[5] Eval metrics for task 2 >> {'accuracy': 47.74730656219393, 'accuracy_s0': 48.68035190615836, 'accuracy_s1': 46.810598626104024, 'loss': 0.012981660102186661}\n",
      "[5] Eval metrics for task 3 >> {'accuracy': 46.744930629669156, 'accuracy_s0': 54.01069518716577, 'accuracy_s1': 39.51011714589989, 'loss': 0.011134019783492276}\n",
      "[5] Eval metrics for task 4 >> {'accuracy': 80.71500503524673, 'accuracy_s0': 73.6842105263158, 'accuracy_s1': 87.41396263520157, 'loss': 0.0031137591282286554}\n",
      "[5] Eval metrics for task 5 >> {'accuracy': 97.62985375693394, 'accuracy_s0': 97.75280898876404, 'accuracy_s1': 97.50996015936255, 'loss': 0.00027995939454344623}\n",
      "training_task_end\n",
      "load_memory_joint: len(train_loader.dataset)=320\n",
      "final avg-acc 70.00949957505934\n",
      "final avg-forget 35.26878284722771\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "# from torchvision import models\n",
    "# from torchinfo import summary\n",
    "# from resnet import ResNet18\n",
    "import random\n",
    "import copy\n",
    "\n",
    "\n",
    "from datasets.FairMNIST import NoiseMNIST\n",
    "from trainers.FairContinualTrainer import FairContinualTrainer\n",
    "from metrics.fair_metric_manager import FairMetricCollector\n",
    "from algorithms.sensitive import Heuristic3\n",
    "\n",
    "if dataset in ['MNIST', 'FMNIST']:\n",
    "    n_feature = 28*28\n",
    "elif dataset in ['CIFAR10']:\n",
    "    n_feature = 32*32*3\n",
    "    \n",
    "if dataset == 'MNIST':\n",
    "    params['alpha'] = 0.005\n",
    "    params['lambda'] = 50\n",
    "\n",
    "elif dataset == 'FMNIST':\n",
    "    params['alpha'] = 0.005\n",
    "    params['lambda'] = 10\n",
    "\n",
    "# alpha_li = [0.05]\n",
    "# lamb_li = [50]\n",
    "noise = 0.0\n",
    "params['alpha'] = 0.05\n",
    "\n",
    "# alpha_li = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]\n",
    "# lamb_li = [0.01, 0.05, 0.1, 1, 10, 50, 100]\n",
    "benchmark = NoiseMNIST(num_tasks=params['num_tasks'], noise_size=noise, random_class_idx= False,\n",
    "# benchmark = cl.benchmarks.SplitMNIST(num_tasks=params['num_tasks'],\n",
    "                             per_task_examples = min(params['per_task_examples'], 14000),\\\n",
    "                                        per_task_memory_examples=params['per_task_memory_examples'])\n",
    "\n",
    "backbone = cl.backbones.MLP2Layers(input_dim=784, hidden_dim_1=256, hidden_dim_2=256, output_dim=10)\n",
    "algorithm = Heuristic3(backbone, benchmark, params, requires_memory=True)\n",
    "metric_manager_callback = FairMetricCollector(num_tasks=params['num_tasks'],\n",
    "                                                        eval_interval='epoch',\n",
    "                                                        epochs_per_task=params['epochs_per_task'])\n",
    "\n",
    "# from trainers.FairContinualTrainer import FairContinualTrainer\n",
    "# trainer = FairContinualTrainer(algorithm, params, callbacks=[metric_manager_callback])\n",
    "\n",
    "trainer = FairContinualTrainer(algorithm, params, callbacks=[metric_manager_callback])\n",
    "trainer.run()\n",
    "print(\"final avg-acc\", metric_manager_callback.meters['accuracy'].compute_final())\n",
    "print(\"final avg-forget\", metric_manager_callback.meters['forgetting'].compute_final())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "becbcc5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1291979344.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    ---------------------------- Task 1 -----------------------\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "---------------------------- Task 1 -----------------------\n",
    "[1] Eval metrics for task 1 >> {'accuracy': 98.6, 'loss': 0.00015132083231583237}\n",
    "training_task_end\n",
    "load_memory_joint: len(train_loader.dataset)=64\n",
    "---------------------------- Task 2 -----------------------\n",
    "len(select_curr_indexes)=5075\n",
    "[2] Eval metrics for task 1 >> {'accuracy': 85.85, 'loss': 0.0034543287754058836}\n",
    "[2] Eval metrics for task 2 >> {'accuracy': 86.6, 'loss': 0.0027514055371284487}\n",
    "training_task_end\n",
    "load_memory_joint: len(train_loader.dataset)=128\n",
    "---------------------------- Task 3 -----------------------\n",
    "len(select_curr_indexes)=9259\n",
    "[3] Eval metrics for task 1 >> {'accuracy': 82.85, 'loss': 0.005165461122989655}\n",
    "[3] Eval metrics for task 2 >> {'accuracy': 59.0, 'loss': 0.007712359070777893}\n",
    "[3] Eval metrics for task 3 >> {'accuracy': 97.55, 'loss': 0.0003111732173711061}\n",
    "training_task_end\n",
    "load_memory_joint: len(train_loader.dataset)=192\n",
    "---------------------------- Task 4 -----------------------\n",
    "len(select_curr_indexes)=11205\n",
    "[4] Eval metrics for task 1 >> {'accuracy': 66.7, 'loss': 0.010747700095176697}\n",
    "[4] Eval metrics for task 2 >> {'accuracy': 48.0, 'loss': 0.012582610368728637}\n",
    "[4] Eval metrics for task 3 >> {'accuracy': 57.25, 'loss': 0.008546485185623169}\n",
    "[4] Eval metrics for task 4 >> {'accuracy': 92.6, 'loss': 0.0011509314700961113}\n",
    "training_task_end\n",
    "load_memory_joint: len(train_loader.dataset)=256\n",
    "---------------------------- Task 5 -----------------------\n",
    "len(select_curr_indexes)=7916\n",
    "[5] Eval metrics for task 1 >> {'accuracy': 74.3, 'loss': 0.007032525479793548}\n",
    "[5] Eval metrics for task 2 >> {'accuracy': 56.75, 'loss': 0.011757457256317138}\n",
    "[5] Eval metrics for task 3 >> {'accuracy': 63.9, 'loss': 0.00855764478445053}\n",
    "[5] Eval metrics for task 4 >> {'accuracy': 80.9, 'loss': 0.003836171865463257}\n",
    "[5] Eval metrics for task 5 >> {'accuracy': 92.7, 'loss': 0.0013385150507092475}\n",
    "training_task_end\n",
    "load_memory_joint: len(train_loader.dataset)=320\n",
    "final avg-acc 73.71000000000001\n",
    "final avg-forget 24.874999999999993\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1b35c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04a1586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33b583",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'taskd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtaskd\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'taskd' is not defined"
     ]
    }
   ],
   "source": [
    "taskd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d8346",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "delete() missing 1 required positional argument: 'obj'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: delete() missing 1 required positional argument: 'obj'"
     ]
    }
   ],
   "source": [
    "np.delete(np.random.randint(0, 10, size=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7aaa68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 7, 8, 4, 9, 0, 2, 1, 6, 5]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.choice(10, size=10, replace=False).tolist()\n",
    "sorted(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af81222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(5):\n",
    "    if len(classwise_loss[x]) == 0:\n",
    "        del classwise_loss[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b9c92c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324660bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, Subset, ConcatDataset\n",
    "\n",
    "task = 1\n",
    "trainset = Subset(benchmark.trains[task], range(100))\n",
    "batch_size = 32\n",
    "shuffle=False\n",
    "num_workers = 0\n",
    "pin_memory = True\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size, shuffle, num_workers=num_workers,\n",
    "                                  pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bfa274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 1, 1, 1])\n",
      "tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
      "        0, 1, 1, 1, 0, 1, 0, 0])\n",
      "tensor([1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (inp, targ, t_id, *_) in enumerate(train_loader):\n",
    "    print(targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3387f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark.trains[task].targets[32:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d66be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(benchmark.trains[task].targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6eff29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = train_loader.dataset.dataset.targets\n",
    "targets.unique().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd7102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "num_dict = {x:0 for x in targets.unique().cpu().numpy()}\n",
    "for k in num_dict:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c6bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(num_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4187b211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0],\n",
       "        [    1],\n",
       "        [    2],\n",
       "        ...,\n",
       "        [11994],\n",
       "        [11995],\n",
       "        [11999]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(targets==0).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c61ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 1, 1, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda4b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,     1,     2,  ..., 11994, 11995, 11999])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(targets==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14742022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5880fb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m----> 6\u001b[0m     x_train_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43mall_train_data\u001b[49m[i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m:i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m      7\u001b[0m     y_train_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(all_train_label[i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m:i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_train_data' is not defined"
     ]
    }
   ],
   "source": [
    "device = params['device']\n",
    "\n",
    "for i in range(5):\n",
    "    torch.cuda.empty_cache()\n",
    "    if i == 0:\n",
    "        x_train_batch = torch.cat(all_train_data[i*2:i*2+2])\n",
    "        y_train_batch = torch.cat(all_train_label[i*2:i*2+2])\n",
    "\n",
    "    elif i > 0:\n",
    "        x_train_batch = torch.cat(all_train_data[i*2:i*2+2])\n",
    "        y_train_batch = torch.cat(all_train_label[i*2:i*2+2])\n",
    "        if i == 1:\n",
    "            x_prev_buffer1 = torch.cat(all_train_data[i*2-2:i*2-1])\n",
    "            y_prev_buffer1 = torch.cat(all_train_label[i*2-2:i*2-1])\n",
    "            indices1 = torch.randperm(len(x_prev_buffer1))[:buffer_size]\n",
    "            x_prev_buffer1 = x_prev_buffer1[indices1]\n",
    "            y_prev_buffer1 = y_prev_buffer1[indices1]\n",
    "\n",
    "            x_prev_buffer2 = torch.cat(all_train_data[i*2-1:i*2])\n",
    "            y_prev_buffer2 = torch.cat(all_train_label[i*2-1:i*2])\n",
    "            indices2 = torch.randperm(len(x_prev_buffer2))[:buffer_size]\n",
    "            x_prev_buffer2 = x_prev_buffer2[indices2]\n",
    "            y_prev_buffer2 = y_prev_buffer2[indices2]\n",
    "\n",
    "            x_prev_buffer = torch.cat([x_prev_buffer1, x_prev_buffer2])\n",
    "            y_prev_buffer = torch.cat([y_prev_buffer1, y_prev_buffer2])\n",
    "\n",
    "        else:\n",
    "\n",
    "            # select gradient-based herding buffer\n",
    "            x_new_buffer = torch.cat(all_train_data[i*2-2:i*2])\n",
    "            y_new_buffer = torch.cat(all_train_label[i*2-2:i*2])\n",
    "\n",
    "            x_new_buffer = x_new_buffer[select_buffer_indexes]\n",
    "            y_new_buffer = y_new_buffer[select_buffer_indexes]\n",
    "\n",
    "            x_prev_buffer = torch.cat([x_prev_buffer, x_new_buffer])\n",
    "            y_prev_buffer = torch.cat([y_prev_buffer, y_new_buffer])\n",
    "            \n",
    "#                     print('memory2:', torch.cuda.memory_allocated(device=device)/1024/1024/1024)\n",
    "\n",
    "        # buffer에 있는 class별 loss가 buffer_losses list에 들어감\n",
    "        buffer_losses = []\n",
    "        \n",
    "\n",
    "        # computation of mean gradients and losses for buffers \n",
    "        for n in range(i*2):\n",
    "            buffer_ind = [m for m in range(len(y_prev_buffer)) if y_prev_buffer[m] == n]\n",
    "            x_buffer = x_prev_buffer[buffer_ind]\n",
    "            y_buffer = y_prev_buffer[buffer_ind]\n",
    "            x_buffer = torch.Tensor(x_buffer).to(device, dtype=torch.float32)\n",
    "            y_buffer = torch.Tensor(y_buffer).to(device, dtype=torch.int64)\n",
    "\n",
    "            if n == 0:\n",
    "                model.zero_grad()\n",
    "\n",
    "                # mean gradient computation\n",
    "                out, emb = model(x_buffer)\n",
    "\n",
    "                init_out_buffer = out\n",
    "                init_emb_buffer = emb\n",
    "                init_y_buffer = y_buffer.view(-1, 1)\n",
    "\n",
    "\n",
    "                loss = loss_sample(out, y_buffer).sum()\n",
    "                buffer_l0_grads = torch.autograd.grad(loss, out)[0] #torch.Size([32, 10])\n",
    "                if dataset in ['MNIST', 'FMNIST']:\n",
    "                    buffer_l0_expand = torch.repeat_interleave(buffer_l0_grads, 256, dim=1)\n",
    "                else:\n",
    "                    buffer_l0_expand = torch.repeat_interleave(buffer_l0_grads, 512, dim=1)\n",
    "                buffer_l1_grads = buffer_l0_expand * emb.repeat(1, n_class)\n",
    "\n",
    "                buffer_l0_grads = buffer_l0_grads.mean(dim=0).view(1, -1)\n",
    "                buffer_l1_grads = buffer_l1_grads.mean(dim=0).view(1, -1)\n",
    "\n",
    "                # mean loss computation\n",
    "                buffer_ds = TensorDataset(x_buffer, y_buffer)\n",
    "                buffer_loader = DataLoader(dataset=buffer_ds, batch_size=batch_size, shuffle=True)\n",
    "                buffer_output, buffer_loss = clf.evaluate(buffer_loader)\n",
    "                buffer_losses.append(buffer_loss)\n",
    "\n",
    "            else:\n",
    "                model.zero_grad()\n",
    "\n",
    "                # mean gradient computation\n",
    "                out, emb = model(x_buffer)\n",
    "\n",
    "                init_out_buffer = torch.cat((init_out_buffer, out), dim=0)\n",
    "                init_emb_buffer = torch.cat((init_emb_buffer, emb), dim=0)\n",
    "                init_y_buffer = torch.cat((init_y_buffer, y_buffer.view(-1, 1)), dim=0)\n",
    "\n",
    "                loss = loss_sample(out, y_buffer).sum()\n",
    "                batch_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "                if dataset in ['MNIST', 'FMNIST']:\n",
    "                    batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 256, dim=1)\n",
    "                else:\n",
    "                    batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 512, dim=1)\n",
    "\n",
    "                batch_l1_grads = batch_l0_expand * emb.repeat(1, n_class)\n",
    "\n",
    "                batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)\n",
    "                batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)\n",
    "\n",
    "                buffer_l0_grads = torch.cat((buffer_l0_grads, batch_l0_grads), dim=0)\n",
    "                buffer_l1_grads = torch.cat((buffer_l1_grads, batch_l1_grads), dim=0)\n",
    "\n",
    "                # mean loss computation\n",
    "                buffer_ds = TensorDataset(x_buffer, y_buffer)\n",
    "                buffer_loader = DataLoader(dataset=buffer_ds, batch_size=batch_size, shuffle=True)\n",
    "                buffer_output, buffer_loss = clf.evaluate(buffer_loader)\n",
    "                buffer_losses.append(buffer_loss)\n",
    "                \n",
    "        \n",
    "        # initialize individual sample gradients of new data\n",
    "        new_l0_grads = torch.empty((0, n_class), device=device, dtype=torch.float32)\n",
    "        if dataset in ['MNIST', 'FMNIST']:\n",
    "            new_l1_grads = torch.empty((0, n_class*256), device=device, dtype=torch.float32)\n",
    "        else:\n",
    "            new_l1_grads = torch.empty((0, n_class*512), device=device, dtype=torch.float32)\n",
    "\n",
    "        # computation of mean and individual gradients and mean losses for new data \n",
    "        for n in range(2):\n",
    "            model.zero_grad()\n",
    "\n",
    "            # mean gradient computation\n",
    "            x_new_buffer = torch.cat(all_train_data[i*2+n:i*2+n+1])\n",
    "            y_new_buffer = torch.cat(all_train_label[i*2+n:i*2+n+1])\n",
    "            x_new_buffer = torch.Tensor(x_new_buffer).to(device, dtype=torch.float32)\n",
    "            y_new_buffer = torch.Tensor(y_new_buffer).to(device, dtype=torch.int64)\n",
    "            \n",
    "            new_buffer_ds = TensorDataset(x_new_buffer, y_new_buffer)\n",
    "            new_buffer_loader = DataLoader(dataset=new_buffer_ds, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            for batch_idx, batch_data in enumerate(new_buffer_loader):\n",
    "                model.zero_grad()\n",
    "                if batch_idx == 0:\n",
    "                    x_batch, y_batch = batch_data\n",
    "\n",
    "                    out, emb = model(x_batch)\n",
    "\n",
    "                    init_out_buffer = torch.cat((init_out_buffer, out), dim=0)\n",
    "                    init_emb_buffer = torch.cat((init_emb_buffer, emb), dim=0)\n",
    "                    init_y_buffer = torch.cat((init_y_buffer, y_batch.view(-1, 1)), dim=0)\n",
    "\n",
    "                    loss = loss_sample(out, y_batch).sum()\n",
    "                    batch_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        if dataset in ['MNIST', 'FMNIST']:\n",
    "                            batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 256, dim=1)\n",
    "                        else:\n",
    "                            batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 512, dim=1)\n",
    "                        batch_l1_grads = batch_l0_expand * emb.repeat(1, n_class)\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    x_batch, y_batch = batch_data\n",
    "\n",
    "                    out, emb = model(x_batch)\n",
    "\n",
    "                    init_out_buffer = torch.cat((init_out_buffer, out), dim=0)\n",
    "                    init_emb_buffer = torch.cat((init_emb_buffer, emb), dim=0)\n",
    "                    init_y_buffer = torch.cat((init_y_buffer, y_batch.view(-1, 1)), dim=0)\n",
    "\n",
    "                    loss = loss_sample(out, y_batch).sum()\n",
    "                    next_batch_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        if dataset in ['MNIST', 'FMNIST']:\n",
    "                            next_batch_l0_expand = torch.repeat_interleave(next_batch_l0_grads, 256, dim=1)\n",
    "                        else:\n",
    "                            next_batch_l0_expand = torch.repeat_interleave(next_batch_l0_grads, 512, dim=1)\n",
    "                        next_batch_l1_grads = next_batch_l0_expand * emb.repeat(1, n_class)\n",
    "\n",
    "                        batch_l0_grads = torch.cat((batch_l0_grads, next_batch_l0_grads), dim=0)\n",
    "                        batch_l1_grads = torch.cat((batch_l1_grads, next_batch_l1_grads), dim=0)\n",
    "\n",
    "            # individual gradients\n",
    "            ind_l0_grads = batch_l0_grads.clone()\n",
    "            ind_l1_grads = batch_l1_grads.clone()\n",
    "            \n",
    "            new_l0_grads = torch.cat((new_l0_grads, ind_l0_grads), dim=0)\n",
    "            new_l1_grads = torch.cat((new_l1_grads, ind_l1_grads), dim=0)\n",
    "                    \n",
    "            # mean gradients\n",
    "            batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)\n",
    "            batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)\n",
    "\n",
    "            buffer_l0_grads = torch.cat((buffer_l0_grads, batch_l0_grads), dim=0)\n",
    "            buffer_l1_grads = torch.cat((buffer_l1_grads, batch_l1_grads), dim=0)\n",
    "\n",
    "            # mean loss computation\n",
    "            buffer_ds = TensorDataset(x_new_buffer, y_new_buffer)\n",
    "            buffer_loader = DataLoader(dataset=buffer_ds, batch_size=batch_size, shuffle=True)\n",
    "            buffer_output, buffer_loss = clf.evaluate(buffer_loader)\n",
    "            buffer_losses.append(buffer_loss)\n",
    "            \n",
    "#                     print('memory4:', torch.cuda.memory_allocated(device=device)/1024/1024/1024)\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        buffer_grads = torch.cat((buffer_l0_grads, buffer_l1_grads), dim=1)\n",
    "        buffer_grads = f.normalize(buffer_grads, p=2, dim=1)\n",
    "\n",
    "        ######################################\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            new_grads = torch.cat((new_l0_grads, new_l1_grads), dim=1)\n",
    "\n",
    "            new_grads_origin = new_grads.clone()\n",
    "            new_grads = f.normalize(new_grads, p=2, dim=1)\n",
    "#                         print('new grads:', new_grads.shape)\n",
    "#                         print('new grads norm:', torch.norm(new_grads, dim=1))\n",
    "#                         print('new grads shape:', new_grads, new_grads.shape)\n",
    "\n",
    "            buffer_losses = torch.tensor(buffer_losses).view(1,-1)\n",
    "#             print('initial loss:', buffer_losses)\n",
    "#             print('initial mean, std:', buffer_losses.mean(dim=1).item(), buffer_losses.std(dim=1).item())\n",
    "\n",
    "            loss_matrix = buffer_losses.repeat(len(new_grads), 1).to(device)\n",
    "            loss_matrix_origin = loss_matrix.clone()\n",
    "            forget_matrix = torch.matmul(new_grads, torch.transpose(buffer_grads, 0, 1)).to(device)\n",
    "\n",
    "#                     print('init forget matrix shape:', forget_matrix.shape)\n",
    "\n",
    "#                     print('init memory:', torch.cuda.memory_allocated(device=device)/1024/1024/1024)\n",
    "\n",
    "        accumulate_select_indexes = []\n",
    "        accumulate_mean = []\n",
    "        accumulate_std = []\n",
    "        accumulate_sum = []\n",
    "\n",
    "        select_indexes = []\n",
    "        non_select_indexes = list(range(len(x_train_batch)))\n",
    "\n",
    "        num_class1 = 0\n",
    "        num_class2 = 0\n",
    "\n",
    "        # current data selection\n",
    "        for b in range(len(x_train_batch)):\n",
    "            torch.cuda.empty_cache()\n",
    "            loss_matrix = loss_matrix - alpha * forget_matrix\n",
    "            loss_mean = torch.mean(loss_matrix, dim=1, keepdim=True)\n",
    "            loss_std = torch.std(loss_matrix, dim=1, keepdim=True)\n",
    "\n",
    "            # select_ind = torch.argmin(loss_mean, dim=0)\n",
    "            # select_ind = torch.argmin(loss_std, dim=0)\n",
    "            select_ind = torch.argmin(loss_mean + loss_std, dim=0)\n",
    "\n",
    "            accumulate_mean.append(copy.deepcopy(loss_mean[select_ind].item()))\n",
    "            accumulate_std.append(copy.deepcopy(loss_std[select_ind].item()))\n",
    "            accumulate_sum.append(copy.deepcopy(loss_mean[select_ind].item() + loss_std[select_ind].item()))\n",
    "\n",
    "            if non_select_indexes[select_ind.item()] < len(x_train_batch)/2:\n",
    "                num_class1 += 1\n",
    "            else:\n",
    "                num_class2 += 1\n",
    "\n",
    "            # metrics인듯?\n",
    "            select_indexes.append(non_select_indexes[select_ind.item()])\n",
    "            accumulate_select_indexes.append(copy.deepcopy(select_indexes))\n",
    "            del non_select_indexes[select_ind.item()]\n",
    "\n",
    "            best_buffer_losses = loss_matrix[select_ind].view(1,-1)\n",
    "            loss_matrix = best_buffer_losses.repeat(len(new_grads)-1, 1).to(device)\n",
    "            new_grads = torch.cat((new_grads[:select_ind.item()], new_grads[select_ind.item()+1:]))\n",
    "            forget_matrix = torch.cat((forget_matrix[:select_ind.item()], forget_matrix[select_ind.item()+1:]))\n",
    "        best_ind = np.argmin(np.array(accumulate_sum))\n",
    "        select_curr_indexes = accumulate_select_indexes[best_ind]\n",
    "\n",
    "        # best_ind=11999\n",
    "        # len(select_curr_indexes)=12000\n",
    "        # len(accumulate_select_indexes)=12000\n",
    "\n",
    "\n",
    "        # best_ind=7337\n",
    "        # len(select_curr_indexes)=7338\n",
    "        # len(accumulate_select_indexes)=12000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # buffer data selection\n",
    "        select_buffer_indexes = []\n",
    "\n",
    "        class1_indexes = list(range(0, int(len(x_train_batch)/2)))\n",
    "        class2_indexes = list(range(int(len(x_train_batch)/2), len(x_train_batch)))\n",
    "\n",
    "        class1_grad_mean = new_grads_origin[class1_indexes].mean(dim=0).view(1, -1)\n",
    "        class2_grad_mean = new_grads_origin[class2_indexes].mean(dim=0).view(1, -1)\n",
    "\n",
    "        candidate_class1_indexes = list(range(0, int(len(x_train_batch)/2)))\n",
    "        class1_buffer_indexes = []\n",
    "        for m in range(buffer_size):\n",
    "            buffer_ind = torch.argmin(\n",
    "                torch.norm(class1_grad_mean.repeat(int(len(new_grads_origin)/2)-m, 1) \\\n",
    "                        - (torch.sum(new_grads_origin[class1_buffer_indexes], dim=0)\n",
    "                              .repeat(int(len(new_grads_origin)/2)-m,1) \\\n",
    "                        + new_grads_origin[candidate_class1_indexes])/(m+1), dim=1), dim=0)\n",
    "            class1_buffer_indexes.append(copy.deepcopy(candidate_class1_indexes[buffer_ind]))\n",
    "            del candidate_class1_indexes[buffer_ind]\n",
    "\n",
    "        candidate_class2_indexes = list(range(int(len(x_train_batch)/2), len(x_train_batch)))\n",
    "        class2_buffer_indexes = []\n",
    "        for m in range(buffer_size):\n",
    "            buffer_ind = torch.argmin(torch.norm(class2_grad_mean.repeat(int(len(new_grads_origin)/2)-m, 1) \\\n",
    "                            - (torch.sum(new_grads_origin[class2_buffer_indexes], dim=0)\n",
    "                               .repeat(int(len(new_grads_origin)/2)-m,1) \\\n",
    "                            + new_grads_origin[candidate_class2_indexes])/(m+1), dim=1), dim=0)\n",
    "            class2_buffer_indexes.append(copy.deepcopy(candidate_class2_indexes[buffer_ind]))\n",
    "            del candidate_class2_indexes[buffer_ind]\n",
    "\n",
    "        for ind in class1_buffer_indexes:\n",
    "            select_buffer_indexes.append(ind)\n",
    "\n",
    "        for ind in class2_buffer_indexes:\n",
    "            select_buffer_indexes.append(ind)\n",
    "\n",
    "        select_curr_indexes = list(set(select_curr_indexes))\n",
    "\n",
    "        # current data selection\n",
    "        x_train_batch = x_train_batch[select_curr_indexes]\n",
    "        y_train_batch = y_train_batch[select_curr_indexes]\n",
    "\n",
    "    x_train_batch = torch.Tensor(x_train_batch).to(device, dtype=torch.float32)\n",
    "    y_train_batch = torch.Tensor(y_train_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "    train_ds = TensorDataset(x_train_batch, y_train_batch)\n",
    "    train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer_config = {\"lr\": 0.001}\n",
    "    \n",
    "    model.zero_grad()\n",
    "\n",
    "    # Model training using current data and buffer data\n",
    "    if i == 0:\n",
    "        clf = NNClassifier(model, nn.CrossEntropyLoss(reduction='mean'), optim.Adam, optimizer_config)\n",
    "        clf.fit({\"train\": train_loader, \"val\": train_loader}, epochs=1, earlystop_path=f'./ckpt/joint.pt')\n",
    "    elif i > 0:\n",
    "#                     print('len prev buffer:', len(x_prev_buffer))\n",
    "        clf = NNClassifier_CL(model, nn.CrossEntropyLoss(reduction='mean'), optim.Adam, optimizer_config)\n",
    "        clf.fit({\"train\": train_loader, \"val\": train_loader, \"buffer\": (x_prev_buffer, y_prev_buffer)}, \n",
    "                epochs=1, sample_size=64, lamb=lamb, device=device, earlystop_path=f'./ckpt/joint.pt', seed=s)\n",
    "\n",
    "    # Model evaluation\n",
    "    all_test_acc = []\n",
    "    all_test_loss = []\n",
    "\n",
    "    # evaluation using sci-kit learn tool\n",
    "    for j in range(n_class):\n",
    "\n",
    "        if j < i*2+2:\n",
    "\n",
    "            x_test_batch = all_test_data[j]\n",
    "            y_test_batch = all_test_label[j]\n",
    "\n",
    "            x_test_batch = torch.Tensor(x_test_batch).to(device, dtype=torch.float32)\n",
    "            y_test_batch = torch.Tensor(y_test_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "            test_ds = TensorDataset(x_test_batch, y_test_batch)\n",
    "            test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            test_output, test_loss = clf.evaluate(test_loader)\n",
    "            test_acc = accuracy_score(test_output['true_y'], test_output['output'])\n",
    "\n",
    "            cf_li = list(confusion_matrix(test_output['true_y'], test_output['output'], labels=range(i*2+2))[j])\n",
    "\n",
    "            cf_matrix = {}\n",
    "            for k in range(len(cf_li)):\n",
    "                cf_matrix[label_li[k]] = cf_li[k]\n",
    "\n",
    "            all_test_acc.append(test_acc)\n",
    "            all_test_loss.append(test_loss)\n",
    "            seq_acc[j].append(test_acc)\n",
    "\n",
    "        else:\n",
    "            seq_acc[j].append(0)\n",
    "\n",
    "#                 print('overall: acc = %.3f, fair = %.3f' %(np.mean(all_test_acc), np.std(all_test_acc)))\n",
    "    overall_acc.append(np.mean(all_test_acc))\n",
    "    overall_fair.append(np.std(all_test_acc))\n",
    "\n",
    "\n",
    "all_overall_acc.append(np.mean(overall_acc))\n",
    "all_overall_fair.append(np.mean(overall_fair))\n",
    "\n",
    "alpha_dict[alpha].append([np.mean(all_overall_acc), np.mean(all_overall_fair)])\n",
    "print(alpha)\n",
    "\n",
    "print('alpha:', alpha, alpha_dict[alpha])\n",
    "print('avg:', np.mean([e[0] for e in alpha_dict[alpha]]), np.mean([e[1] for e in alpha_dict[alpha]]))\n",
    "print('\\n')\n",
    "\n",
    "print('-------------------------------------------------------------------------------------------------')\n",
    "        \n",
    "for k, v in alpha_dict.items():\n",
    "    print(k, v)\n",
    "    print('avg:', np.mean([e[0] for e in v]), np.mean([e[1] for e in v]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b0074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
