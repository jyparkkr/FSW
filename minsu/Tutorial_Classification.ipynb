{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c3b92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./FMNIST_DATASET/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:21<00:00, 1244771.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./FMNIST_DATASET/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./FMNIST_DATASET/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./FMNIST_DATASET/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 133119.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./FMNIST_DATASET/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./FMNIST_DATASET/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./FMNIST_DATASET/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:05<00:00, 858235.95it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./FMNIST_DATASET/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./FMNIST_DATASET/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./FMNIST_DATASET/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 8343229.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./FMNIST_DATASET/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./FMNIST_DATASET/FashionMNIST/raw\n",
      "\n",
      "[60000, 28, 28]\n",
      "tensor(0.2860) tensor(0.3530)\n",
      "[10000, 28, 28]\n",
      "tensor(0.2868) tensor(0.3524)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as f\n",
    "\n",
    "# dataset = 'MNIST'\n",
    "dataset = 'FMNIST'\n",
    "# dataset = 'CIFAR10'\n",
    "\n",
    "if dataset == 'MNIST':\n",
    "    label_li = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    base_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    train_set = torchvision.datasets.MNIST(root=f'./{dataset}_DATASET', train=True, download=False, transform=base_transform)\n",
    "    print(list(train_set.data.size()))\n",
    "    train_mean = train_set.data.float().mean()/255\n",
    "    train_std = train_set.data.float().std()/255\n",
    "    print(train_mean, train_std)\n",
    "\n",
    "    base_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    test_set = torchvision.datasets.MNIST(root=f'./{dataset}_DATASET', train=False, download=False, transform=base_transform)\n",
    "    print(list(test_set.data.size()))\n",
    "    test_mean = test_set.data.float().mean()/255\n",
    "    test_std = test_set.data.float().std()/255\n",
    "    print(test_mean, test_std)\n",
    "\n",
    "elif dataset == 'FMNIST':\n",
    "    label_li = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', \n",
    "                  'Ankel boot']\n",
    "    base_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    train_set = torchvision.datasets.FashionMNIST(root=f'./{dataset}_DATASET', train=True, download=True, transform=base_transform)\n",
    "    print(list(train_set.data.size()))\n",
    "    train_mean = train_set.data.float().mean()/255\n",
    "    train_std = train_set.data.float().std()/255\n",
    "    print(train_mean, train_std)\n",
    "\n",
    "    base_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    test_set = torchvision.datasets.FashionMNIST(root=f'./{dataset}_DATASET', train=False, download=False, transform=base_transform)\n",
    "    print(list(test_set.data.size()))\n",
    "    test_mean = test_set.data.float().mean()/255\n",
    "    test_std = test_set.data.float().std()/255\n",
    "    print(test_mean, test_std)\n",
    "    \n",
    "elif dataset == 'CIFAR10':\n",
    "    label_li = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    base_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    train_set = torchvision.datasets.CIFAR10(root=f'./{dataset}_DATASET', train=True, download=False, transform=base_transform)\n",
    "    train_set.data = torch.from_numpy(train_set.data)\n",
    "    print(list(train_set.data.size()))\n",
    "    train_mean = train_set.data.float().mean()/255\n",
    "    train_std = train_set.data.float().std()/255\n",
    "    print(train_mean, train_std)\n",
    "\n",
    "    base_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    test_set = torchvision.datasets.CIFAR10(root=f'./{dataset}_DATASET', train=False, download=False, transform=base_transform)\n",
    "    test_set.data = torch.from_numpy(test_set.data)\n",
    "    print(list(test_set.data.size()))\n",
    "    test_mean = test_set.data.float().mean()/255\n",
    "    test_std = test_set.data.float().std()/255\n",
    "    print(test_mean, test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ac8d570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((train_mean,), (train_std,))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((test_mean,), (test_std,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a271c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
    "\n",
    "if dataset == 'MNIST':\n",
    "    download_root = f'./{dataset}_DATASET'\n",
    "    train_dataset = MNIST(download_root, transform=train_transform, train=True, download=False)\n",
    "    test_dataset = MNIST(download_root, transform=test_transform, train=False, download=False)\n",
    "\n",
    "elif dataset == 'FMNIST':\n",
    "    download_root = f'./{dataset}_DATASET'\n",
    "    train_dataset = FashionMNIST(download_root, transform=train_transform, train=True, download=False)\n",
    "    test_dataset = FashionMNIST(download_root, transform=test_transform, train=False, download=False)\n",
    "\n",
    "elif dataset == 'CIFAR10':\n",
    "    download_root = f'./{dataset}_DATASET'\n",
    "    train_dataset = CIFAR10(download_root, transform=train_transform, train=True, download=False)\n",
    "    test_dataset = CIFAR10(download_root, transform=test_transform, train=False, download=False)\n",
    "    train_dataset.data = torch.from_numpy(train_dataset.data).permute(0, 3, 1, 2)\n",
    "    train_dataset.targets = torch.from_numpy(np.array(train_dataset.targets))\n",
    "    test_dataset.data = torch.from_numpy(test_dataset.data).permute(0, 3, 1, 2)\n",
    "    test_dataset.targets = torch.from_numpy(np.array(test_dataset.targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f59b61ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9adce03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n",
      "tensor([9, 0, 0,  ..., 3, 0, 5])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.data.size())\n",
    "print(train_dataset.targets.size())\n",
    "print(train_dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad7cc636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 28, 28])\n",
      "torch.Size([10000])\n",
      "tensor([9, 2, 1,  ..., 8, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.data.size())\n",
    "print(test_dataset.targets.size())\n",
    "print(test_dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab0db1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_dataset.data\n",
    "y_train = train_dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "261c793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_0 = x_train[y_train == 0]\n",
    "y_train_0 = y_train[y_train == 0]\n",
    "\n",
    "x_train_1 = x_train[y_train == 1]\n",
    "y_train_1 = y_train[y_train == 1]\n",
    "\n",
    "x_train_2 = x_train[y_train == 2]\n",
    "y_train_2 = y_train[y_train == 2]\n",
    "\n",
    "x_train_3 = x_train[y_train == 3]\n",
    "y_train_3 = y_train[y_train == 3]\n",
    "\n",
    "x_train_4 = x_train[y_train == 4]\n",
    "y_train_4 = y_train[y_train == 4]\n",
    "\n",
    "x_train_5 = x_train[y_train == 5]\n",
    "y_train_5 = y_train[y_train == 5]\n",
    "\n",
    "x_train_6 = x_train[y_train == 6]\n",
    "y_train_6 = y_train[y_train == 6]\n",
    "\n",
    "x_train_7 = x_train[y_train == 7]\n",
    "y_train_7 = y_train[y_train == 7]\n",
    "\n",
    "x_train_8 = x_train[y_train == 8]\n",
    "y_train_8 = y_train[y_train == 8]\n",
    "\n",
    "x_train_9 = x_train[y_train == 9]\n",
    "y_train_9 = y_train[y_train == 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ed03e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'MNIST':\n",
    "    x_train_0 = x_train_0[:5000]\n",
    "    y_train_0 = y_train_0[:5000]\n",
    "    \n",
    "    x_train_1 = x_train_1[:5000]\n",
    "    y_train_1 = y_train_1[:5000]\n",
    "    \n",
    "    x_train_2 = x_train_2[:5000]\n",
    "    y_train_2 = y_train_2[:5000]\n",
    "    \n",
    "    x_train_3 = x_train_3[:5000]\n",
    "    y_train_3 = y_train_3[:5000]\n",
    "    \n",
    "    x_train_4 = x_train_4[:5000]\n",
    "    y_train_4 = y_train_4[:5000]\n",
    "    \n",
    "    x_train_5 = x_train_5[:5000]\n",
    "    y_train_5 = y_train_5[:5000]\n",
    "    \n",
    "    x_train_6 = x_train_6[:5000]\n",
    "    y_train_6 = y_train_6[:5000]\n",
    "    \n",
    "    x_train_7 = x_train_7[:5000]\n",
    "    y_train_7 = y_train_7[:5000]\n",
    "    \n",
    "    x_train_8 = x_train_8[:5000]\n",
    "    y_train_8 = y_train_8[:5000]\n",
    "    \n",
    "    x_train_9 = x_train_9[:5000]\n",
    "    y_train_9 = y_train_9[:5000]\n",
    "    \n",
    "if dataset == 'CIFAR10':\n",
    "    x_train_0 = x_train_0[:1000]\n",
    "    y_train_0 = y_train_0[:1000]\n",
    "    \n",
    "    x_train_1 = x_train_1[:1000]\n",
    "    y_train_1 = y_train_1[:1000]\n",
    "    \n",
    "    x_train_2 = x_train_2[:1000]\n",
    "    y_train_2 = y_train_2[:1000]\n",
    "    \n",
    "    x_train_3 = x_train_3[:1000]\n",
    "    y_train_3 = y_train_3[:1000]\n",
    "    \n",
    "    x_train_4 = x_train_4[:1000]\n",
    "    y_train_4 = y_train_4[:1000]\n",
    "    \n",
    "    x_train_5 = x_train_5[:1000]\n",
    "    y_train_5 = y_train_5[:1000]\n",
    "    \n",
    "    x_train_6 = x_train_6[:1000]\n",
    "    y_train_6 = y_train_6[:1000]\n",
    "    \n",
    "    x_train_7 = x_train_7[:1000]\n",
    "    y_train_7 = y_train_7[:1000]\n",
    "    \n",
    "    x_train_8 = x_train_8[:1000]\n",
    "    y_train_8 = y_train_8[:1000]\n",
    "    \n",
    "    x_train_9 = x_train_9[:1000]\n",
    "    y_train_9 = y_train_9[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "346abfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "6000\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "6000\n",
      "6000\n",
      "tensor([1, 1, 1,  ..., 1, 1, 1])\n",
      "6000\n",
      "6000\n",
      "tensor([2, 2, 2,  ..., 2, 2, 2])\n",
      "6000\n",
      "6000\n",
      "tensor([3, 3, 3,  ..., 3, 3, 3])\n",
      "6000\n",
      "6000\n",
      "tensor([4, 4, 4,  ..., 4, 4, 4])\n",
      "6000\n",
      "6000\n",
      "tensor([5, 5, 5,  ..., 5, 5, 5])\n",
      "6000\n",
      "6000\n",
      "tensor([6, 6, 6,  ..., 6, 6, 6])\n",
      "6000\n",
      "6000\n",
      "tensor([7, 7, 7,  ..., 7, 7, 7])\n",
      "6000\n",
      "6000\n",
      "tensor([8, 8, 8,  ..., 8, 8, 8])\n",
      "6000\n",
      "6000\n",
      "tensor([9, 9, 9,  ..., 9, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train_0))\n",
    "print(len(y_train_0))\n",
    "print(y_train_0)\n",
    "\n",
    "print(len(x_train_1))\n",
    "print(len(y_train_1))\n",
    "print(y_train_1)\n",
    "\n",
    "print(len(x_train_2))\n",
    "print(len(y_train_2))\n",
    "print(y_train_2)\n",
    "\n",
    "print(len(x_train_3))\n",
    "print(len(y_train_3))\n",
    "print(y_train_3)\n",
    "\n",
    "print(len(x_train_4))\n",
    "print(len(y_train_4))\n",
    "print(y_train_4)\n",
    "\n",
    "print(len(x_train_5))\n",
    "print(len(y_train_5))\n",
    "print(y_train_5)\n",
    "\n",
    "print(len(x_train_6))\n",
    "print(len(y_train_6))\n",
    "print(y_train_6)\n",
    "\n",
    "print(len(x_train_7))\n",
    "print(len(y_train_7))\n",
    "print(y_train_7)\n",
    "\n",
    "print(len(x_train_8))\n",
    "print(len(y_train_8))\n",
    "print(y_train_8)\n",
    "\n",
    "print(len(x_train_9))\n",
    "print(len(y_train_9))\n",
    "print(y_train_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f5de8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_dataset.data\n",
    "y_test = test_dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15dcdc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_0 = x_test[y_test == 0]\n",
    "y_test_0 = y_test[y_test == 0]\n",
    "\n",
    "x_test_1 = x_test[y_test == 1]\n",
    "y_test_1 = y_test[y_test == 1]\n",
    "\n",
    "x_test_2 = x_test[y_test == 2]\n",
    "y_test_2 = y_test[y_test == 2]\n",
    "\n",
    "x_test_3 = x_test[y_test == 3]\n",
    "y_test_3 = y_test[y_test == 3]\n",
    "\n",
    "x_test_4 = x_test[y_test == 4]\n",
    "y_test_4 = y_test[y_test == 4]\n",
    "\n",
    "x_test_5 = x_test[y_test == 5]\n",
    "y_test_5 = y_test[y_test == 5]\n",
    "\n",
    "x_test_6 = x_test[y_test == 6]\n",
    "y_test_6 = y_test[y_test == 6]\n",
    "\n",
    "x_test_7 = x_test[y_test == 7]\n",
    "y_test_7 = y_test[y_test == 7]\n",
    "\n",
    "x_test_8 = x_test[y_test == 8]\n",
    "y_test_8 = y_test[y_test == 8]\n",
    "\n",
    "x_test_9 = x_test[y_test == 9]\n",
    "y_test_9 = y_test[y_test == 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9379d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "1000\n",
      "1000\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "1000\n",
      "1000\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "1000\n",
      "1000\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "1000\n",
      "1000\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "1000\n",
      "1000\n",
      "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n",
      "1000\n",
      "1000\n",
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])\n",
      "1000\n",
      "1000\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7])\n",
      "1000\n",
      "1000\n",
      "tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
      "1000\n",
      "1000\n",
      "tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "print(len(x_test_0))\n",
    "print(len(y_test_0))\n",
    "print(y_test_0)\n",
    "\n",
    "print(len(x_test_1))\n",
    "print(len(y_test_1))\n",
    "print(y_test_1)\n",
    "\n",
    "print(len(x_test_2))\n",
    "print(len(y_test_2))\n",
    "print(y_test_2)\n",
    "\n",
    "print(len(x_test_3))\n",
    "print(len(y_test_3))\n",
    "print(y_test_3)\n",
    "\n",
    "print(len(x_test_4))\n",
    "print(len(y_test_4))\n",
    "print(y_test_4)\n",
    "\n",
    "print(len(x_test_5))\n",
    "print(len(y_test_5))\n",
    "print(y_test_5)\n",
    "\n",
    "print(len(x_test_6))\n",
    "print(len(y_test_6))\n",
    "print(y_test_6)\n",
    "\n",
    "print(len(x_test_7))\n",
    "print(len(y_test_7))\n",
    "print(y_test_7)\n",
    "\n",
    "print(len(x_test_8))\n",
    "print(len(y_test_8))\n",
    "print(y_test_8)\n",
    "\n",
    "print(len(x_test_9))\n",
    "print(len(y_test_9))\n",
    "print(y_test_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baf4f625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x1000 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4, 10))\n",
    "\n",
    "plt.subplot(5,2,1)\n",
    "if dataset in ['MNIST', 'FMNIST']:\n",
    "    plt.imshow(x_train_0[0], cmap=plt.get_cmap('gray'))\n",
    "else:\n",
    "    plt.imshow(x_train_0[0].permute(1, 2, 0), cmap=plt.get_cmap('gray'))\n",
    "\n",
    "    \n",
    "plt.subplot(5,2,2)\n",
    "if dataset in ['MNIST', 'FMNIST']:\n",
    "    plt.imshow(x_train_1[0], cmap=plt.get_cmap('gray'))\n",
    "else:\n",
    "    plt.imshow(x_train_1[0].permute(1, 2, 0), cmap=plt.get_cmap('gray'))\n",
    "\n",
    "    \n",
    "plt.subplot(5,2,3)\n",
    "if dataset in ['MNIST', 'FMNIST']:\n",
    "    plt.imshow(x_train_2[0], cmap=plt.get_cmap('gray'))\n",
    "else:\n",
    "    plt.imshow(x_train_2[0].permute(1, 2, 0), cmap=plt.get_cmap('gray'))\n",
    "\n",
    "    \n",
    "plt.subplot(5,2,4)\n",
    "if dataset in ['MNIST', 'FMNIST']:\n",
    "    plt.imshow(x_train_3[0], cmap=plt.get_cmap('gray'))\n",
    "else:\n",
    "    plt.imshow(x_train_3[0].permute(1, 2, 0), cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "plt.subplot(5,2,5)\n",
    "if dataset in ['MNIST', 'FMNIST']:\n",
    "    plt.imshow(x_train_4[0], cmap=plt.get_cmap('gray'))\n",
    "else:\n",
    "    plt.imshow(x_train_4[0].permute(1, 2, 0), cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "plt.subplot(5,2,6)\n",
    "if dataset in ['MNIST', 'FMNIST']:\n",
    "    plt.imshow(x_train_5[0], cmap=plt.get_cmap('gray'))\n",
    "else:\n",
    "    plt.imshow(x_train_5[0].permute(1, 2, 0), cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "plt.subplot(5,2,7)\n",
    "if dataset in ['MNIST', 'FMNIST']:\n",
    "    plt.imshow(x_train_6[0], cmap=plt.get_cmap('gray'))\n",
    "else:\n",
    "    plt.imshow(x_train_6[0].permute(1, 2, 0), cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "plt.subplot(5,2,8)\n",
    "if dataset in ['MNIST', 'FMNIST']:\n",
    "    plt.imshow(x_train_7[0], cmap=plt.get_cmap('gray'))\n",
    "else:\n",
    "    plt.imshow(x_train_7[0].permute(1, 2, 0), cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "plt.subplot(5,2,9)\n",
    "if dataset in ['MNIST', 'FMNIST']:\n",
    "    plt.imshow(x_train_8[0], cmap=plt.get_cmap('gray'))\n",
    "else:\n",
    "    plt.imshow(x_train_8[0].permute(1, 2, 0), cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "plt.subplot(5,2,10)\n",
    "if dataset in ['MNIST', 'FMNIST']:\n",
    "    plt.imshow(x_train_9[0], cmap=plt.get_cmap('gray'))\n",
    "else:\n",
    "    plt.imshow(x_train_9[0].permute(1, 2, 0), cmap=plt.get_cmap('gray'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba0216d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "if cuda:\n",
    "    device = 'cuda:6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f37c4768",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_data = [x_train_0, x_train_1, x_train_2, x_train_3, x_train_4, x_train_5, x_train_6, x_train_7, x_train_8, x_train_9]\n",
    "all_train_label = [y_train_0, y_train_1, y_train_2, y_train_3, y_train_4, y_train_5, y_train_6, y_train_7, y_train_8, y_train_9]\n",
    "\n",
    "all_test_data = [x_test_0, x_test_1, x_test_2, x_test_3, x_test_4, x_test_5, x_test_6, x_test_7, x_test_8, x_test_9]\n",
    "all_test_label = [y_test_0, y_test_1, y_test_2, y_test_3, y_test_4, y_test_5, y_test_6, y_test_7, y_test_8, y_test_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4256f313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import EarlyStopping\n",
    "from model import NormalNN, NNClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from torchvision import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torchinfo import summary\n",
    "# from resnet import ResNet18\n",
    "import random\n",
    "from model import NormalNN, NNClassifier_CL\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee7be3c",
   "metadata": {},
   "source": [
    "## Joint Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1e37be2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import EarlyStopping\n",
    "# from model import NormalNN, NNClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "# from torchvision import models\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch import optim\n",
    "# from torchinfo import summary\n",
    "# from resnet import ResNet18\n",
    "# import random\n",
    "\n",
    "# if dataset in ['MNIST', 'FMNIST']:\n",
    "#     n_feature = 28*28\n",
    "# elif dataset in ['CIFAR10']:\n",
    "#     n_feature = 32*32*3\n",
    "    \n",
    "# batch_size = 64\n",
    "# n_class = 10\n",
    "\n",
    "# # all_upper_bound = []\n",
    "# all_overall_acc = []\n",
    "# all_overall_fair = []\n",
    "\n",
    "# for s in range(5):\n",
    "    \n",
    "#     random.seed(s)\n",
    "#     np.random.seed(s)\n",
    "#     torch.manual_seed(s)\n",
    "#     torch.cuda.manual_seed_all(s)\n",
    "    \n",
    "#     upper_bound = []\n",
    "\n",
    "#     seq_acc = {k: [] for k in range(n_class)}\n",
    "#     overall_acc = []\n",
    "#     overall_fair = []\n",
    "\n",
    "#     for i in range(5):\n",
    "        \n",
    "#         upper_acc = []\n",
    "\n",
    "#         print('Task: ', i)\n",
    "\n",
    "#         x_train_batch = torch.cat(all_train_data[:i*2+2])\n",
    "#         y_train_batch = torch.cat(all_train_label[:i*2+2])\n",
    "\n",
    "#         x_train_batch = torch.Tensor(x_train_batch).to(device, dtype=torch.float32)\n",
    "#         y_train_batch = torch.Tensor(y_train_batch).to(device, dtype=torch.int64)\n",
    "        \n",
    "#         for c in range(i*2+2):\n",
    "#             print(c, y_train_batch.cpu().tolist().count(c))\n",
    "\n",
    "#         train_ds = TensorDataset(x_train_batch, y_train_batch)\n",
    "#         train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#         if dataset in ['MNIST', 'FMNIST']:\n",
    "#             model = NormalNN(input_features=n_feature, n_class=n_class, seed=s)\n",
    "#         else:\n",
    "#             model = ResNet18(num_classes=n_class, seed=s)\n",
    "#         model = model.to(device)\n",
    "        \n",
    "#         optimizer_config = {\"lr\": 0.001}\n",
    "#         clf = NNClassifier(model, nn.CrossEntropyLoss(reduction='mean'), optim.Adam, optimizer_config)\n",
    "\n",
    "#         clf.fit({\"train\": train_loader, \"val\": train_loader}, epochs=1, earlystop_path=f'./ckpt/joint.pt')\n",
    "\n",
    "#         all_test_acc = []\n",
    "\n",
    "#         for j in range(n_class):\n",
    "#             if j < i*2+2:\n",
    "\n",
    "#                 x_test_batch = all_test_data[j]\n",
    "#                 y_test_batch = all_test_label[j]\n",
    "\n",
    "#                 x_test_batch = torch.Tensor(x_test_batch).to(device, dtype=torch.float32)\n",
    "#                 y_test_batch = torch.Tensor(y_test_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "#                 test_ds = TensorDataset(x_test_batch, y_test_batch)\n",
    "#                 test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#                 test_output, test_loss = clf.evaluate(test_loader)\n",
    "#                 test_acc = accuracy_score(test_output['true_y'], test_output['output'])\n",
    "\n",
    "#                 cf_li = list(confusion_matrix(test_output['true_y'], test_output['output'], labels=range(i*2+2))[j])\n",
    "\n",
    "#                 cf_matrix = {}\n",
    "#                 for k in range(len(cf_li)):\n",
    "#                     cf_matrix[label_li[k]] = cf_li[k]\n",
    "\n",
    "#                 print('test %d: acc= %.3f' %(j, test_acc), cf_matrix)\n",
    "                \n",
    "#                 upper_acc.append(test_acc)\n",
    "#                 all_test_acc.append(test_acc)\n",
    "#                 seq_acc[j].append(test_acc)\n",
    "\n",
    "#             else:\n",
    "#                 seq_acc[j].append(0)\n",
    "\n",
    "        \n",
    "#         upper_bound.append(upper_acc)\n",
    "        \n",
    "#         print('overall: acc = %.3f, fair = %.3f' %(np.mean(all_test_acc), np.std(all_test_acc)))\n",
    "#         overall_acc.append(np.mean(all_test_acc))\n",
    "#         overall_fair.append(np.std(all_test_acc))\n",
    "#         print('\\n')\n",
    "\n",
    "#     print('sequential acc: ', seq_acc)\n",
    "\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     for i in range(5):\n",
    "#         plt.subplot(2,3,i+1)\n",
    "#         plt.plot(seq_acc[i*2], label='Class '+str(i*2))\n",
    "#         plt.plot(seq_acc[i*2+1], label='Class '+str(i*2+1))\n",
    "#         plt.xticks(range(5))\n",
    "#         plt.legend(loc='best')\n",
    "#         plt.xlabel(\"Task\")\n",
    "#         plt.ylabel(\"Accuracy\")\n",
    "#     plt.show()\n",
    "\n",
    "#     print('overall acc: ', overall_acc, 'avg:', np.mean(overall_acc))\n",
    "#     print('overall fair:', overall_fair, 'avg:', np.mean(overall_fair))\n",
    "    \n",
    "#     all_overall_acc.append(np.mean(overall_acc))\n",
    "#     all_overall_fair.append(np.mean(overall_fair))\n",
    "\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     plt.subplot(1,2,1)\n",
    "#     plt.plot(overall_acc)\n",
    "#     plt.xticks(range(5))\n",
    "#     plt.xlabel(\"Task\")\n",
    "#     plt.ylabel(\"Accuracy\")\n",
    "    \n",
    "#     plt.subplot(1,2,2)\n",
    "#     plt.plot(overall_fair)\n",
    "#     plt.xticks(range(5))\n",
    "#     plt.xlabel(\"Task\")\n",
    "#     plt.ylabel(\"Fairness\")\n",
    "#     plt.show()\n",
    "    \n",
    "#     print('-------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "# #     print('upper bound:', upper_bound)\n",
    "# #     all_upper_bound.append(upper_bound)\n",
    "# #     print('\\n')\n",
    "    \n",
    "# # print('all upper bound:', all_upper_bound)\n",
    "# print('all overall acc:', np.mean(all_overall_acc), np.std(all_overall_acc))\n",
    "# print('all overall fair:', np.mean(all_overall_fair), np.std(all_overall_fair))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ffc7bb",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d258aff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "# n_class = 10\n",
    "\n",
    "# if dataset in ['MNIST', 'FMNIST']:\n",
    "#     n_feature = 28*28\n",
    "# elif dataset in ['CIFAR10']:\n",
    "#     n_feature = 32*32*3\n",
    "    \n",
    "# all_overall_acc = []\n",
    "# all_overall_fair = []\n",
    "\n",
    "# for s in range(5):\n",
    "    \n",
    "#     random.seed(s)\n",
    "#     np.random.seed(s)\n",
    "#     torch.manual_seed(s)\n",
    "#     torch.cuda.manual_seed_all(s)\n",
    "\n",
    "#     if dataset in ['MNIST', 'FMNIST']:\n",
    "#         model = NormalNN(input_features=n_feature, n_class=n_class, seed=s)\n",
    "#     else:\n",
    "#         model = ResNet18(num_classes=n_class, seed=s)\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     seq_acc = {k: [] for k in range(n_class)}\n",
    "#     overall_acc = []\n",
    "#     overall_fair = []\n",
    "\n",
    "#     for i in range(5):\n",
    "\n",
    "#         print('Task: ', i)\n",
    "\n",
    "#         x_train_batch = torch.cat(all_train_data[i*2:i*2+2])\n",
    "#         y_train_batch = torch.cat(all_train_label[i*2:i*2+2])\n",
    "\n",
    "#         x_train_batch = torch.Tensor(x_train_batch).to(device, dtype=torch.float32)\n",
    "#         y_train_batch = torch.Tensor(y_train_batch).to(device, dtype=torch.int64)\n",
    "        \n",
    "#         for c in range(i*2+2):\n",
    "#             print(c, y_train_batch.cpu().tolist().count(c))\n",
    "\n",
    "#         train_ds = TensorDataset(x_train_batch, y_train_batch)\n",
    "#         train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#         optimizer_config = {\"lr\": 0.001}\n",
    "#         clf = NNClassifier(model, nn.CrossEntropyLoss(reduction='mean'), optim.Adam, optimizer_config)\n",
    "\n",
    "#         clf.fit({\"train\": train_loader, \"val\": train_loader}, epochs=1, earlystop_path=f'./ckpt/joint.pt')\n",
    "\n",
    "#         all_test_acc = []\n",
    "\n",
    "#         for j in range(n_class):\n",
    "\n",
    "#             if j < i*2+2:\n",
    "\n",
    "#                 x_test_batch = all_test_data[j]\n",
    "#                 y_test_batch = all_test_label[j]\n",
    "\n",
    "#                 x_test_batch = torch.Tensor(x_test_batch).to(device, dtype=torch.float32)\n",
    "#                 y_test_batch = torch.Tensor(y_test_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "#                 test_ds = TensorDataset(x_test_batch, y_test_batch)\n",
    "#                 test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#                 test_output, test_loss = clf.evaluate(test_loader)\n",
    "#                 test_acc = accuracy_score(test_output['true_y'], test_output['output'])\n",
    "\n",
    "#                 cf_li = list(confusion_matrix(test_output['true_y'], test_output['output'], labels=range(i*2+2))[j])\n",
    "\n",
    "#                 cf_matrix = {}\n",
    "#                 for k in range(len(cf_li)):\n",
    "#                     cf_matrix[label_li[k]] = cf_li[k]\n",
    "\n",
    "#                 print('test %d: acc= %.3f' %(j, test_acc), cf_matrix)\n",
    "\n",
    "#                 all_test_acc.append(test_acc)\n",
    "#                 seq_acc[j].append(test_acc)\n",
    "\n",
    "#             else:\n",
    "#                 seq_acc[j].append(0)    \n",
    "            \n",
    "#         print('overall: acc = %.3f, fair = %.3f' %(np.mean(all_test_acc), np.std(all_test_acc)))\n",
    "#         overall_acc.append(np.mean(all_test_acc))\n",
    "#         overall_fair.append(np.std(all_test_acc))\n",
    "#         print('\\n')\n",
    "\n",
    "#     print('sequential acc: ', seq_acc)\n",
    "\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     for i in range(5):\n",
    "#         plt.subplot(2,3,i+1)\n",
    "#         plt.plot(seq_acc[i*2], label='Class '+str(i*2))\n",
    "#         plt.plot(seq_acc[i*2+1], label='Class '+str(i*2+1))\n",
    "#         plt.xticks(range(5))\n",
    "#         plt.legend(loc='best')\n",
    "#         plt.xlabel(\"Task\")\n",
    "#         plt.ylabel(\"Accuracy\")\n",
    "#     plt.show()\n",
    "\n",
    "#     print('overall acc: ', overall_acc, 'avg:', np.mean(overall_acc))\n",
    "#     print('overall fair:', overall_fair, 'avg:', np.mean(overall_fair))\n",
    "    \n",
    "#     all_overall_acc.append(np.mean(overall_acc))\n",
    "#     all_overall_fair.append(np.mean(overall_fair))\n",
    "\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     plt.subplot(1,2,1)\n",
    "#     plt.plot(overall_acc)\n",
    "#     plt.xticks(range(5))\n",
    "#     plt.xlabel(\"Task\")\n",
    "#     plt.ylabel(\"Accuracy\")\n",
    "    \n",
    "#     plt.subplot(1,2,2)\n",
    "#     plt.plot(overall_fair)\n",
    "#     plt.xticks(range(5))\n",
    "#     plt.xlabel(\"Task\")\n",
    "#     plt.ylabel(\"Fairness\")\n",
    "#     plt.show()\n",
    "    \n",
    "#     print('-------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "# #     upper_bound = all_upper_bound[s]\n",
    "    \n",
    "# #     print('upper bound:', upper_bound)\n",
    "# #     print('lower bound:', lower_bound)\n",
    "    \n",
    "# #     forget_result = []\n",
    "    \n",
    "# #     for ind in range(len(upper_bound)):\n",
    "# #         list1 = upper_bound[ind]\n",
    "# #         list2 = lower_bound[ind]\n",
    "# #         result = list(map(lambda a, b: round(a - b, 3), list1, list2))\n",
    "# #         forget_result.append(result)\n",
    "        \n",
    "# #     print('forget result:', forget_result)\n",
    "    \n",
    "# #     forget_summary = []\n",
    "    \n",
    "# #     for ind in range(len(forget_result)):\n",
    "# #         avg_std = []\n",
    "# #         avg_std.append(round(np.mean(forget_result[ind]), 3))\n",
    "# #         avg_std.append(round(np.std(forget_result[ind]), 3))\n",
    "# #         forget_summary.append(avg_std)\n",
    "        \n",
    "# #     print('forget summary:', forget_summary)\n",
    "# #     print('metric1:', round(np.mean(np.array(forget_summary)[:,0]), 3), 'metric2:', round(np.mean(np.array(forget_summary)[:,1]), 3))\n",
    "# #     print('\\n')\n",
    "    \n",
    "# #     print('\\n')\n",
    "# print('all overall acc:', np.mean(all_overall_acc), np.std(all_overall_acc))\n",
    "# print('all overall fair:', np.mean(all_overall_fair), np.std(all_overall_fair))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770fe89f",
   "metadata": {},
   "source": [
    "## Random Buffer (curr = all, buffer size/class = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b989d30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "# buffer_size = 32\n",
    "# n_class = 10\n",
    "\n",
    "# if dataset in ['MNIST', 'FMNIST']:\n",
    "#     n_feature = 28*28\n",
    "# elif dataset in ['CIFAR10']:\n",
    "#     n_feature = 32*32*3\n",
    "    \n",
    "# all_overall_acc = []\n",
    "# all_overall_fair = []\n",
    "\n",
    "# for s in range(5):\n",
    "    \n",
    "#     random.seed(s)\n",
    "#     np.random.seed(s)\n",
    "#     torch.manual_seed(s)\n",
    "#     torch.cuda.manual_seed_all(s)\n",
    "    \n",
    "#     if dataset in ['MNIST', 'FMNIST']:\n",
    "#         model = NormalNN(input_features=n_feature, n_class=n_class, seed=s)\n",
    "#     else:\n",
    "#         model = ResNet18(num_classes=n_class, seed=s)\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     seq_acc = {k: [] for k in range(n_class)}\n",
    "#     overall_acc = []\n",
    "#     overall_fair = []\n",
    "\n",
    "#     for i in range(5):\n",
    "\n",
    "#         print('Task: ', i)\n",
    "\n",
    "#         if i == 0:\n",
    "#             x_train_batch = torch.cat(all_train_data[i*2:i*2+2])\n",
    "#             y_train_batch = torch.cat(all_train_label[i*2:i*2+2])\n",
    "\n",
    "#         elif i > 0:\n",
    "#             x_train_batch = torch.cat(all_train_data[i*2:i*2+2])\n",
    "#             y_train_batch = torch.cat(all_train_label[i*2:i*2+2])\n",
    "#             if i == 1:\n",
    "#                 x_prev_buffer1 = torch.cat(all_train_data[i*2-2:i*2-1])\n",
    "#                 y_prev_buffer1 = torch.cat(all_train_label[i*2-2:i*2-1])\n",
    "#                 indices1 = torch.randperm(len(x_prev_buffer1))[:buffer_size]\n",
    "#                 x_prev_buffer1 = x_prev_buffer1[indices1]\n",
    "#                 y_prev_buffer1 = y_prev_buffer1[indices1]\n",
    "\n",
    "#                 x_prev_buffer2 = torch.cat(all_train_data[i*2-1:i*2])\n",
    "#                 y_prev_buffer2 = torch.cat(all_train_label[i*2-1:i*2])\n",
    "#                 indices2 = torch.randperm(len(x_prev_buffer2))[:buffer_size]\n",
    "#                 x_prev_buffer2 = x_prev_buffer2[indices2]\n",
    "#                 y_prev_buffer2 = y_prev_buffer2[indices2]\n",
    "\n",
    "#                 x_prev_buffer = torch.cat([x_prev_buffer1, x_prev_buffer2])\n",
    "#                 y_prev_buffer = torch.cat([y_prev_buffer1, y_prev_buffer2])\n",
    "\n",
    "#             else:\n",
    "#                 x_new_buffer1 = torch.cat(all_train_data[i*2-2:i*2-1])\n",
    "#                 y_new_buffer1 = torch.cat(all_train_label[i*2-2:i*2-1])\n",
    "#                 indices1 = torch.randperm(len(x_new_buffer1))[:buffer_size]\n",
    "#                 x_new_buffer1 = x_new_buffer1[indices1]\n",
    "#                 y_new_buffer1 = y_new_buffer1[indices1]\n",
    "\n",
    "#                 x_new_buffer2 = torch.cat(all_train_data[i*2-1:i*2])\n",
    "#                 y_new_buffer2 = torch.cat(all_train_label[i*2-1:i*2])\n",
    "#                 indices2 = torch.randperm(len(x_new_buffer2))[:buffer_size]\n",
    "#                 x_new_buffer2 = x_new_buffer2[indices2]\n",
    "#                 y_new_buffer2 = y_new_buffer2[indices2]\n",
    "\n",
    "#                 x_new_buffer = torch.cat([x_new_buffer1, x_new_buffer2])\n",
    "#                 y_new_buffer = torch.cat([y_new_buffer1, y_new_buffer2])\n",
    "\n",
    "#                 x_prev_buffer = torch.cat([x_prev_buffer, x_new_buffer])\n",
    "#                 y_prev_buffer = torch.cat([y_prev_buffer, y_new_buffer])\n",
    "\n",
    "#             x_train_batch = torch.cat([x_prev_buffer, x_train_batch])\n",
    "#             y_train_batch = torch.cat([y_prev_buffer, y_train_batch])\n",
    "\n",
    "#         x_train_batch = torch.Tensor(x_train_batch).to(device, dtype=torch.float32)\n",
    "#         y_train_batch = torch.Tensor(y_train_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "#         for c in range(i*2+2):\n",
    "#             print(c, y_train_batch.cpu().tolist().count(c))\n",
    "\n",
    "#         train_ds = TensorDataset(x_train_batch, y_train_batch)\n",
    "#         train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#         optimizer_config = {\"lr\": 0.001}\n",
    "#         clf = NNClassifier(model, nn.CrossEntropyLoss(reduction='mean'), optim.Adam, optimizer_config)\n",
    "\n",
    "#         clf.fit({\"train\": train_loader, \"val\": train_loader}, epochs=1, earlystop_path=f'./ckpt/joint.pt')\n",
    "\n",
    "#         all_test_acc = []\n",
    "\n",
    "#         for j in range(n_class):\n",
    "\n",
    "#             if j < i*2+2:\n",
    "\n",
    "#                 x_test_batch = all_test_data[j]\n",
    "#                 y_test_batch = all_test_label[j]\n",
    "\n",
    "#                 x_test_batch = torch.Tensor(x_test_batch).to(device, dtype=torch.float32)\n",
    "#                 y_test_batch = torch.Tensor(y_test_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "#                 test_ds = TensorDataset(x_test_batch, y_test_batch)\n",
    "#                 test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#                 test_output, test_loss = clf.evaluate(test_loader)\n",
    "#                 test_acc = accuracy_score(test_output['true_y'], test_output['output'])\n",
    "\n",
    "#                 cf_li = list(confusion_matrix(test_output['true_y'], test_output['output'], labels=range(i*2+2))[j])\n",
    "\n",
    "#                 cf_matrix = {}\n",
    "#                 for k in range(len(cf_li)):\n",
    "#                     cf_matrix[label_li[k]] = cf_li[k]\n",
    "\n",
    "#                 print('test %d: acc= %.3f' %(j, test_acc), cf_matrix)\n",
    "#                 all_test_acc.append(test_acc)\n",
    "#                 seq_acc[j].append(test_acc)\n",
    "\n",
    "#             else:\n",
    "#                 seq_acc[j].append(0)\n",
    "\n",
    "#         print('overall: acc = %.3f, fair = %.3f' %(np.mean(all_test_acc), np.std(all_test_acc)))\n",
    "#         overall_acc.append(np.mean(all_test_acc))\n",
    "#         overall_fair.append(np.std(all_test_acc))\n",
    "#         print('\\n')\n",
    "\n",
    "#     print('sequential acc: ', seq_acc)\n",
    "\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     for i in range(5):\n",
    "#         plt.subplot(2,3,i+1)\n",
    "#         plt.plot(seq_acc[i*2], label='Class '+str(i*2))\n",
    "#         plt.plot(seq_acc[i*2+1], label='Class '+str(i*2+1))\n",
    "#         plt.xticks(range(5))\n",
    "#         plt.legend(loc='best')\n",
    "#         plt.xlabel(\"Task\")\n",
    "#         plt.ylabel(\"Accuracy\")\n",
    "#     plt.show()\n",
    "\n",
    "#     print('overall acc: ', overall_acc, 'avg:', np.mean(overall_acc))\n",
    "#     print('overall fair:', overall_fair, 'avg:', np.mean(overall_fair))\n",
    "    \n",
    "#     all_overall_acc.append(np.mean(overall_acc))\n",
    "#     all_overall_fair.append(np.mean(overall_fair))\n",
    "\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     plt.subplot(1,2,1)\n",
    "#     plt.plot(overall_acc)\n",
    "#     plt.xticks(range(5))\n",
    "#     plt.xlabel(\"Task\")\n",
    "#     plt.ylabel(\"Accuracy\")\n",
    "\n",
    "#     plt.subplot(1,2,2)\n",
    "#     plt.plot(overall_fair)\n",
    "#     plt.xticks(range(5))\n",
    "#     plt.xlabel(\"Task\")\n",
    "#     plt.ylabel(\"Fairness\")\n",
    "#     plt.show()\n",
    "    \n",
    "#     print('-------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "# print('all overall acc:', np.mean(all_overall_acc), np.std(all_overall_acc))\n",
    "# print('all overall fair:', np.mean(all_overall_fair), np.std(all_overall_fair))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df65f0c",
   "metadata": {},
   "source": [
    "## Random Buffer (curr/class = 32, buffer size/class = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "952b2d14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "# buffer_size = 32\n",
    "# n_class = 10\n",
    "\n",
    "# if dataset in ['MNIST', 'FMNIST']:\n",
    "#     n_feature = 28*28\n",
    "# elif dataset in ['CIFAR10']:\n",
    "#     n_feature = 32*32*3\n",
    "    \n",
    "# all_overall_acc = []\n",
    "# all_overall_fair = []\n",
    "    \n",
    "# for s in range(5):\n",
    "    \n",
    "#     random.seed(s)\n",
    "#     np.random.seed(s)\n",
    "#     torch.manual_seed(s)\n",
    "#     torch.cuda.manual_seed_all(s)\n",
    "\n",
    "#     if dataset in ['MNIST', 'FMNIST']:\n",
    "#         model = NormalNN(input_features=n_feature, n_class=n_class, seed=s)\n",
    "#     else:\n",
    "#         model = ResNet18(num_classes=n_class, seed=s)\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     seq_acc = {k: [] for k in range(n_class)}\n",
    "#     overall_acc = []\n",
    "#     overall_fair = []\n",
    "\n",
    "#     for i in range(5):\n",
    "\n",
    "#         print('batch ind: ', i)\n",
    "\n",
    "#         if i == 0:\n",
    "#             x_train_batch = torch.cat(all_train_data[i*2:i*2+2])\n",
    "#             y_train_batch = torch.cat(all_train_label[i*2:i*2+2])\n",
    "\n",
    "#         elif i > 0:\n",
    "#             x_train_batch1 = torch.cat(all_train_data[i*2:i*2+1])\n",
    "#             y_train_batch1 = torch.cat(all_train_label[i*2:i*2+1])\n",
    "#             indices1 = torch.randperm(len(x_train_batch1))[:buffer_size]\n",
    "\n",
    "#             x_train_batch2 = torch.cat(all_train_data[i*2+1:i*2+2])\n",
    "#             y_train_batch2 = torch.cat(all_train_label[i*2+1:i*2+2])\n",
    "#             indices2 = torch.randperm(len(x_train_batch2))[:buffer_size]\n",
    "\n",
    "#             x_train_batch = torch.cat([x_train_batch1[indices1], x_train_batch2[indices2]])\n",
    "#             y_train_batch = torch.cat([y_train_batch1[indices1], y_train_batch2[indices2]])\n",
    "\n",
    "#             if i == 1:\n",
    "#                 x_prev_buffer1 = torch.cat(all_train_data[i*2-2:i*2-1])\n",
    "#                 y_prev_buffer1 = torch.cat(all_train_label[i*2-2:i*2-1])\n",
    "#                 indices1 = torch.randperm(len(x_prev_buffer1))[:buffer_size]\n",
    "#                 x_prev_buffer1 = x_prev_buffer1[indices1]\n",
    "#                 y_prev_buffer1 = y_prev_buffer1[indices1]\n",
    "\n",
    "#                 x_prev_buffer2 = torch.cat(all_train_data[i*2-1:i*2])\n",
    "#                 y_prev_buffer2 = torch.cat(all_train_label[i*2-1:i*2])\n",
    "#                 indices2 = torch.randperm(len(x_prev_buffer2))[:buffer_size]\n",
    "#                 x_prev_buffer2 = x_prev_buffer2[indices2]\n",
    "#                 y_prev_buffer2 = y_prev_buffer2[indices2]\n",
    "\n",
    "#                 x_prev_buffer = torch.cat([x_prev_buffer1, x_prev_buffer2])\n",
    "#                 y_prev_buffer = torch.cat([y_prev_buffer1, y_prev_buffer2])\n",
    "\n",
    "#             else:\n",
    "#                 x_new_buffer1 = torch.cat(all_train_data[i*2-2:i*2-1])\n",
    "#                 y_new_buffer1 = torch.cat(all_train_label[i*2-2:i*2-1])\n",
    "#                 indices1 = torch.randperm(len(x_new_buffer1))[:buffer_size]\n",
    "#                 x_new_buffer1 = x_new_buffer1[indices1]\n",
    "#                 y_new_buffer1 = y_new_buffer1[indices1]\n",
    "\n",
    "#                 x_new_buffer2 = torch.cat(all_train_data[i*2-1:i*2])\n",
    "#                 y_new_buffer2 = torch.cat(all_train_label[i*2-1:i*2])\n",
    "#                 indices2 = torch.randperm(len(x_new_buffer2))[:buffer_size]\n",
    "#                 x_new_buffer2 = x_new_buffer2[indices2]\n",
    "#                 y_new_buffer2 = y_new_buffer2[indices2]\n",
    "\n",
    "#                 x_new_buffer = torch.cat([x_new_buffer1, x_new_buffer2])\n",
    "#                 y_new_buffer = torch.cat([y_new_buffer1, y_new_buffer2])\n",
    "\n",
    "#                 x_prev_buffer = torch.cat([x_prev_buffer, x_new_buffer])\n",
    "#                 y_prev_buffer = torch.cat([y_prev_buffer, y_new_buffer])\n",
    "\n",
    "#             x_train_batch = torch.cat([x_prev_buffer, x_train_batch])\n",
    "#             y_train_batch = torch.cat([y_prev_buffer, y_train_batch])\n",
    "\n",
    "#         x_train_batch = torch.Tensor(x_train_batch).to(device, dtype=torch.float32)\n",
    "#         y_train_batch = torch.Tensor(y_train_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "#         for c in range(i*2+2):\n",
    "#             print(c, y_train_batch.cpu().tolist().count(c))\n",
    "\n",
    "#         train_ds = TensorDataset(x_train_batch, y_train_batch)\n",
    "#         train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#         optimizer_config = {\"lr\": 0.001}\n",
    "#         clf = NNClassifier(model, nn.CrossEntropyLoss(reduction='mean'), optim.Adam, optimizer_config)\n",
    "\n",
    "#         clf.fit({\"train\": train_loader, \"val\": train_loader}, epochs=1, earlystop_path=f'./ckpt/joint.pt')\n",
    "\n",
    "#         all_test_acc = []\n",
    "\n",
    "#         for j in range(n_class):\n",
    "\n",
    "#             if j < i*2+2:\n",
    "\n",
    "#                 x_test_batch = all_test_data[j]\n",
    "#                 y_test_batch = all_test_label[j]\n",
    "\n",
    "#                 x_test_batch = torch.Tensor(x_test_batch).to(device, dtype=torch.float32)\n",
    "#                 y_test_batch = torch.Tensor(y_test_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "#                 test_ds = TensorDataset(x_test_batch, y_test_batch)\n",
    "#                 test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#                 test_output, test_loss = clf.evaluate(test_loader)\n",
    "#                 test_acc = accuracy_score(test_output['true_y'], test_output['output'])\n",
    "\n",
    "#                 cf_li = list(confusion_matrix(test_output['true_y'], test_output['output'], labels=range(i*2+2))[j])\n",
    "\n",
    "#                 cf_matrix = {}\n",
    "#                 for k in range(len(cf_li)):\n",
    "#                     cf_matrix[label_li[k]] = cf_li[k]\n",
    "\n",
    "#                 print('test %d: acc= %.3f' %(j, test_acc), cf_matrix)\n",
    "#                 all_test_acc.append(test_acc)\n",
    "#                 seq_acc[j].append(test_acc)\n",
    "\n",
    "#             else:\n",
    "#                 seq_acc[j].append(0)\n",
    "\n",
    "#         print('overall: acc = %.3f, fair = %.3f' %(np.mean(all_test_acc), np.std(all_test_acc)))\n",
    "#         overall_acc.append(np.mean(all_test_acc))\n",
    "#         overall_fair.append(np.std(all_test_acc))\n",
    "#         print('\\n')\n",
    "\n",
    "#     print('sequential: ', seq_acc)\n",
    "\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     for i in range(5):\n",
    "#         plt.subplot(2,3,i+1)\n",
    "#         plt.plot(seq_acc[i*2], label='Class '+str(i*2))\n",
    "#         plt.plot(seq_acc[i*2+1], label='Class '+str(i*2+1))\n",
    "#         plt.xticks(range(5))\n",
    "#         plt.legend(loc='best')\n",
    "#         plt.xlabel(\"Task\")\n",
    "#         plt.ylabel(\"Accuracy\")\n",
    "#     plt.show()\n",
    "\n",
    "#     print('overall acc: ', overall_acc, 'avg:', np.mean(overall_acc))\n",
    "#     print('overall fair:', overall_fair, 'avg:', np.mean(overall_fair))\n",
    "    \n",
    "#     all_overall_acc.append(np.mean(overall_acc))\n",
    "#     all_overall_fair.append(np.mean(overall_fair))\n",
    "\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     plt.subplot(1,2,1)\n",
    "#     plt.plot(overall_acc)\n",
    "#     plt.xticks(range(5))\n",
    "#     plt.xlabel(\"Task\")\n",
    "#     plt.ylabel(\"Accuracy\")\n",
    "\n",
    "#     plt.subplot(1,2,2)\n",
    "#     plt.plot(overall_fair)\n",
    "#     plt.xticks(range(5))\n",
    "#     plt.xlabel(\"Task\")\n",
    "#     plt.ylabel(\"Fairness\")\n",
    "#     plt.show()\n",
    "    \n",
    "#     print('-------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "# print('all overall acc:', np.mean(all_overall_acc), np.std(all_overall_acc))\n",
    "# print('all overall fair:', np.mean(all_overall_fair), np.std(all_overall_fair))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37a9244",
   "metadata": {},
   "source": [
    "## Random Buffer (curr = all, buffer size/class = 32) - CL training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "247a1cf5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from model import NormalNN, NNClassifier_CL\n",
    "\n",
    "# batch_size = 64\n",
    "# buffer_size = 32\n",
    "# n_class = 10\n",
    "\n",
    "# if dataset in ['MNIST', 'FMNIST']:\n",
    "#     n_feature = 28*28\n",
    "# elif dataset in ['CIFAR10']:\n",
    "#     n_feature = 32*32*3\n",
    "\n",
    "# lamb_li = [0.01, 0.05, 0.1, 1, 10, 50, 100]\n",
    "\n",
    "# lamb_dict = {l: [] for l in lamb_li}\n",
    "\n",
    "# for lamb in lamb_li:\n",
    "    \n",
    "#     all_overall_acc = []\n",
    "#     all_overall_fair = []\n",
    "\n",
    "#     for s in range(5):\n",
    "\n",
    "#         random.seed(s)\n",
    "#         np.random.seed(s)\n",
    "#         torch.manual_seed(s)\n",
    "#         torch.cuda.manual_seed_all(s)\n",
    "\n",
    "#         if dataset in ['MNIST', 'FMNIST']:\n",
    "#             model = NormalNN(input_features=n_feature, n_class=n_class, seed=s)\n",
    "#         else:\n",
    "#             model = ResNet18(num_classes=n_class, seed=s)\n",
    "#         model = model.to(device)\n",
    "\n",
    "#         seq_acc = {k: [] for k in range(n_class)}\n",
    "#         overall_acc = []\n",
    "#         overall_fair = []\n",
    "\n",
    "#         for i in range(5):\n",
    "\n",
    "# #             print('Task: ', i)\n",
    "\n",
    "#             if i == 0:\n",
    "#                 x_train_batch = torch.cat(all_train_data[i*2:i*2+2])\n",
    "#                 y_train_batch = torch.cat(all_train_label[i*2:i*2+2])\n",
    "\n",
    "#             elif i > 0:\n",
    "#                 x_train_batch = torch.cat(all_train_data[i*2:i*2+2])\n",
    "#                 y_train_batch = torch.cat(all_train_label[i*2:i*2+2])\n",
    "\n",
    "#                 if i == 1:\n",
    "#                     x_prev_buffer1 = torch.cat(all_train_data[i*2-2:i*2-1])\n",
    "#                     y_prev_buffer1 = torch.cat(all_train_label[i*2-2:i*2-1])\n",
    "#                     indices1 = torch.randperm(len(x_prev_buffer1))[:buffer_size]\n",
    "#                     x_prev_buffer1 = x_prev_buffer1[indices1]\n",
    "#                     y_prev_buffer1 = y_prev_buffer1[indices1]\n",
    "\n",
    "#                     x_prev_buffer2 = torch.cat(all_train_data[i*2-1:i*2])\n",
    "#                     y_prev_buffer2 = torch.cat(all_train_label[i*2-1:i*2])\n",
    "#                     indices2 = torch.randperm(len(x_prev_buffer2))[:buffer_size]\n",
    "#                     x_prev_buffer2 = x_prev_buffer2[indices2]\n",
    "#                     y_prev_buffer2 = y_prev_buffer2[indices2]\n",
    "\n",
    "#                     x_prev_buffer = torch.cat([x_prev_buffer1, x_prev_buffer2])\n",
    "#                     y_prev_buffer = torch.cat([y_prev_buffer1, y_prev_buffer2])\n",
    "\n",
    "#                 else:\n",
    "#                     x_new_buffer1 = torch.cat(all_train_data[i*2-2:i*2-1])\n",
    "#                     y_new_buffer1 = torch.cat(all_train_label[i*2-2:i*2-1])\n",
    "#                     indices1 = torch.randperm(len(x_new_buffer1))[:buffer_size]\n",
    "#                     x_new_buffer1 = x_new_buffer1[indices1]\n",
    "#                     y_new_buffer1 = y_new_buffer1[indices1]\n",
    "\n",
    "#                     x_new_buffer2 = torch.cat(all_train_data[i*2-1:i*2])\n",
    "#                     y_new_buffer2 = torch.cat(all_train_label[i*2-1:i*2])\n",
    "#                     indices2 = torch.randperm(len(x_new_buffer2))[:buffer_size]\n",
    "#                     x_new_buffer2 = x_new_buffer2[indices2]\n",
    "#                     y_new_buffer2 = y_new_buffer2[indices2]\n",
    "\n",
    "#                     x_new_buffer = torch.cat([x_new_buffer1, x_new_buffer2])\n",
    "#                     y_new_buffer = torch.cat([y_new_buffer1, y_new_buffer2])\n",
    "\n",
    "#                     x_prev_buffer = torch.cat([x_prev_buffer, x_new_buffer])\n",
    "#                     y_prev_buffer = torch.cat([y_prev_buffer, y_new_buffer])\n",
    "\n",
    "#             x_train_batch = torch.Tensor(x_train_batch).to(device, dtype=torch.float32)\n",
    "#             y_train_batch = torch.Tensor(y_train_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "# #             for c in range(i*2+2):\n",
    "# #                 print(c, y_train_batch.cpu().tolist().count(c))\n",
    "\n",
    "#             train_ds = TensorDataset(x_train_batch, y_train_batch)\n",
    "#             train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#             optimizer_config = {\"lr\": 0.001}\n",
    "\n",
    "#             if i == 0:\n",
    "#                 clf = NNClassifier(model, nn.CrossEntropyLoss(reduction='mean'), optim.Adam, optimizer_config)\n",
    "#                 clf.fit({\"train\": train_loader, \"val\": train_loader}, epochs=1, earlystop_path=f'./ckpt/joint.pt')\n",
    "#             elif i > 0:\n",
    "# #                 print('len prev buffer:', len(x_prev_buffer))\n",
    "#                 clf = NNClassifier_CL(model, nn.CrossEntropyLoss(reduction='mean'), optim.Adam, optimizer_config)\n",
    "#                 clf.fit({\"train\": train_loader, \"val\": train_loader, \"buffer\": (x_prev_buffer, y_prev_buffer)}, \n",
    "#                         epochs=1, sample_size=64, lamb=lamb, device=device, earlystop_path=f'./ckpt/joint.pt', seed=s)\n",
    "\n",
    "#             all_test_acc = []\n",
    "\n",
    "#             for j in range(n_class):\n",
    "\n",
    "#                 if j < i*2+2:\n",
    "\n",
    "#                     x_test_batch = all_test_data[j]\n",
    "#                     y_test_batch = all_test_label[j]\n",
    "\n",
    "#                     x_test_batch = torch.Tensor(x_test_batch).to(device, dtype=torch.float32)\n",
    "#                     y_test_batch = torch.Tensor(y_test_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "#                     test_ds = TensorDataset(x_test_batch, y_test_batch)\n",
    "#                     test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#                     test_output, test_loss = clf.evaluate(test_loader)\n",
    "#                     test_acc = accuracy_score(test_output['true_y'], test_output['output'])\n",
    "\n",
    "#                     cf_li = list(confusion_matrix(test_output['true_y'], test_output['output'], labels=range(i*2+2))[j])\n",
    "\n",
    "#                     cf_matrix = {}\n",
    "#                     for k in range(len(cf_li)):\n",
    "#                         cf_matrix[label_li[k]] = cf_li[k]\n",
    "\n",
    "# #                     print('test %d: acc= %.3f' %(j, test_acc), cf_matrix)\n",
    "#                     all_test_acc.append(test_acc)\n",
    "#                     seq_acc[j].append(test_acc)\n",
    "\n",
    "#                 else:\n",
    "#                     seq_acc[j].append(0)\n",
    "\n",
    "# #             print('overall: acc = %.3f, fair = %.3f' %(np.mean(all_test_acc), np.std(all_test_acc)))\n",
    "#             overall_acc.append(np.mean(all_test_acc))\n",
    "#             overall_fair.append(np.std(all_test_acc))\n",
    "# #             print('\\n')\n",
    "\n",
    "# #         print('sequential acc: ', seq_acc)\n",
    "\n",
    "# #         plt.figure(figsize=(12, 8))\n",
    "# #         for i in range(5):\n",
    "# #             plt.subplot(2,3,i+1)\n",
    "# #             plt.plot(seq_acc[i*2], label='Class '+str(i*2))\n",
    "# #             plt.plot(seq_acc[i*2+1], label='Class '+str(i*2+1))\n",
    "# #             plt.xticks(range(5))\n",
    "# #             plt.legend(loc='best')\n",
    "# #             plt.xlabel(\"Task\")\n",
    "# #             plt.ylabel(\"Accuracy\")\n",
    "# #         plt.show()\n",
    "\n",
    "# #         print('overall acc: ', overall_acc, 'avg:', np.mean(overall_acc))\n",
    "# #         print('overall fair:', overall_fair, 'avg:', np.mean(overall_fair))\n",
    "\n",
    "#         all_overall_acc.append(np.mean(overall_acc))\n",
    "#         all_overall_fair.append(np.mean(overall_fair))\n",
    "\n",
    "# #         plt.figure(figsize=(10, 4))\n",
    "# #         plt.subplot(1,2,1)\n",
    "# #         plt.plot(overall_acc)\n",
    "# #         plt.xticks(range(5))\n",
    "# #         plt.xlabel(\"Task\")\n",
    "# #         plt.ylabel(\"Accuracy\")\n",
    "\n",
    "# #         plt.subplot(1,2,2)\n",
    "# #         plt.plot(overall_fair)\n",
    "# #         plt.xticks(range(5))\n",
    "# #         plt.xlabel(\"Task\")\n",
    "# #         plt.ylabel(\"Fairness\")\n",
    "# #         plt.show()\n",
    "\n",
    "# #         print('-------------------------------------------------------------------------------------------------')\n",
    "\n",
    "# #     print('all overall acc:', np.mean(all_overall_acc), np.std(all_overall_acc))\n",
    "# #     print('all overall fair:', np.mean(all_overall_fair), np.std(all_overall_fair))\n",
    "    \n",
    "#     lamb_dict[lamb].append([np.mean(all_overall_acc), np.mean(all_overall_fair)])\n",
    "    \n",
    "#     print(lamb, lamb_dict[lamb])\n",
    "    \n",
    "# print('-------------------------------------------------------------------------------------------------')\n",
    "        \n",
    "# for k, v in lamb_dict.items():\n",
    "#     print(k, v)\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea77e7d",
   "metadata": {},
   "source": [
    "## Gradient-based data selection (curr/class = all, buffer size/class = 32) - CL training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d54e66a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer_l0_grads.shape=torch.Size([32, 10])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "batch_l0_grads.shape=torch.Size([1, 10])\n",
      "batch_l1_grads.shape=torch.Size([1, 2560])\n",
      "buffer_l0_grads.shape=torch.Size([2, 10])\n",
      "buffer_l1_grads.shape=torch.Size([2, 2560])\n",
      "out of loop\n",
      "cur_buffer_l0_grads.shape=torch.Size([2, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([2, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([3, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([3, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([3, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([3, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([4, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([4, 2560])\n",
      "buffer_grads.shape=torch.Size([4, 2570])\n",
      "new_l0_grads.shape=torch.Size([12000, 10])\n",
      "new_l1_grads.shape=torch.Size([12000, 2560])\n",
      "buffer_grads.shape=torch.Size([4, 2570])\n",
      "current end\n",
      "new_grads.shape=torch.Size([12000, 2570])\n",
      "buffer_losses.shape=torch.Size([1, 4])\n",
      "forget_matrix.shape=torch.Size([12000, 4])\n",
      "best_ind=11999\n",
      "len(select_curr_indexes)=12000\n",
      "len(accumulate_select_indexes)=12000\n",
      "class1_grad_mean.shape=torch.Size([1, 2570])\n",
      "class2_grad_mean.shape=torch.Size([1, 2570])\n",
      "buffer_l0_grads.shape=torch.Size([32, 10])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "batch_l0_grads.shape=torch.Size([1, 10])\n",
      "batch_l1_grads.shape=torch.Size([1, 2560])\n",
      "buffer_l0_grads.shape=torch.Size([4, 10])\n",
      "buffer_l1_grads.shape=torch.Size([4, 2560])\n",
      "out of loop\n",
      "cur_buffer_l0_grads.shape=torch.Size([4, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([4, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([5, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([5, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([5, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([5, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([6, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([6, 2560])\n",
      "buffer_grads.shape=torch.Size([6, 2570])\n",
      "new_l0_grads.shape=torch.Size([12000, 10])\n",
      "new_l1_grads.shape=torch.Size([12000, 2560])\n",
      "buffer_grads.shape=torch.Size([6, 2570])\n",
      "current end\n",
      "new_grads.shape=torch.Size([12000, 2570])\n",
      "buffer_losses.shape=torch.Size([1, 6])\n",
      "forget_matrix.shape=torch.Size([12000, 6])\n",
      "best_ind=7337\n",
      "len(select_curr_indexes)=7338\n",
      "len(accumulate_select_indexes)=12000\n",
      "class1_grad_mean.shape=torch.Size([1, 2570])\n",
      "class2_grad_mean.shape=torch.Size([1, 2570])\n",
      "buffer_l0_grads.shape=torch.Size([32, 10])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "batch_l0_grads.shape=torch.Size([1, 10])\n",
      "batch_l1_grads.shape=torch.Size([1, 2560])\n",
      "buffer_l0_grads.shape=torch.Size([6, 10])\n",
      "buffer_l1_grads.shape=torch.Size([6, 2560])\n",
      "out of loop\n",
      "cur_buffer_l0_grads.shape=torch.Size([6, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([6, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([7, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([7, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([7, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([7, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([8, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([8, 2560])\n",
      "buffer_grads.shape=torch.Size([8, 2570])\n",
      "new_l0_grads.shape=torch.Size([12000, 10])\n",
      "new_l1_grads.shape=torch.Size([12000, 2560])\n",
      "buffer_grads.shape=torch.Size([8, 2570])\n",
      "current end\n",
      "new_grads.shape=torch.Size([12000, 2570])\n",
      "buffer_losses.shape=torch.Size([1, 8])\n",
      "forget_matrix.shape=torch.Size([12000, 8])\n",
      "best_ind=10430\n",
      "len(select_curr_indexes)=10431\n",
      "len(accumulate_select_indexes)=12000\n",
      "class1_grad_mean.shape=torch.Size([1, 2570])\n",
      "class2_grad_mean.shape=torch.Size([1, 2570])\n",
      "buffer_l0_grads.shape=torch.Size([32, 10])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "batch_l0_grads.shape=torch.Size([1, 10])\n",
      "batch_l1_grads.shape=torch.Size([1, 2560])\n",
      "buffer_l0_grads.shape=torch.Size([8, 10])\n",
      "buffer_l1_grads.shape=torch.Size([8, 2560])\n",
      "out of loop\n",
      "cur_buffer_l0_grads.shape=torch.Size([8, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([8, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([9, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([9, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([9, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([9, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([10, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([10, 2560])\n",
      "buffer_grads.shape=torch.Size([10, 2570])\n",
      "new_l0_grads.shape=torch.Size([12000, 10])\n",
      "new_l1_grads.shape=torch.Size([12000, 2560])\n",
      "buffer_grads.shape=torch.Size([10, 2570])\n",
      "current end\n",
      "new_grads.shape=torch.Size([12000, 2570])\n",
      "buffer_losses.shape=torch.Size([1, 10])\n",
      "forget_matrix.shape=torch.Size([12000, 10])\n",
      "best_ind=6961\n",
      "len(select_curr_indexes)=6962\n",
      "len(accumulate_select_indexes)=12000\n",
      "class1_grad_mean.shape=torch.Size([1, 2570])\n",
      "class2_grad_mean.shape=torch.Size([1, 2570])\n",
      "buffer_l0_grads.shape=torch.Size([32, 10])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "batch_l0_grads.shape=torch.Size([1, 10])\n",
      "batch_l1_grads.shape=torch.Size([1, 2560])\n",
      "buffer_l0_grads.shape=torch.Size([2, 10])\n",
      "buffer_l1_grads.shape=torch.Size([2, 2560])\n",
      "out of loop\n",
      "cur_buffer_l0_grads.shape=torch.Size([2, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([2, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([3, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([3, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([3, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([3, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([4, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([4, 2560])\n",
      "buffer_grads.shape=torch.Size([4, 2570])\n",
      "new_l0_grads.shape=torch.Size([12000, 10])\n",
      "new_l1_grads.shape=torch.Size([12000, 2560])\n",
      "buffer_grads.shape=torch.Size([4, 2570])\n",
      "current end\n",
      "new_grads.shape=torch.Size([12000, 2570])\n",
      "buffer_losses.shape=torch.Size([1, 4])\n",
      "forget_matrix.shape=torch.Size([12000, 4])\n",
      "best_ind=11999\n",
      "len(select_curr_indexes)=12000\n",
      "len(accumulate_select_indexes)=12000\n",
      "class1_grad_mean.shape=torch.Size([1, 2570])\n",
      "class2_grad_mean.shape=torch.Size([1, 2570])\n",
      "buffer_l0_grads.shape=torch.Size([32, 10])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "batch_l0_grads.shape=torch.Size([1, 10])\n",
      "batch_l1_grads.shape=torch.Size([1, 2560])\n",
      "buffer_l0_grads.shape=torch.Size([4, 10])\n",
      "buffer_l1_grads.shape=torch.Size([4, 2560])\n",
      "out of loop\n",
      "cur_buffer_l0_grads.shape=torch.Size([4, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([4, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([5, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([5, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([5, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([5, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([6, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([6, 2560])\n",
      "buffer_grads.shape=torch.Size([6, 2570])\n",
      "new_l0_grads.shape=torch.Size([12000, 10])\n",
      "new_l1_grads.shape=torch.Size([12000, 2560])\n",
      "buffer_grads.shape=torch.Size([6, 2570])\n",
      "current end\n",
      "new_grads.shape=torch.Size([12000, 2570])\n",
      "buffer_losses.shape=torch.Size([1, 6])\n",
      "forget_matrix.shape=torch.Size([12000, 6])\n",
      "best_ind=5634\n",
      "len(select_curr_indexes)=5635\n",
      "len(accumulate_select_indexes)=12000\n",
      "class1_grad_mean.shape=torch.Size([1, 2570])\n",
      "class2_grad_mean.shape=torch.Size([1, 2570])\n",
      "buffer_l0_grads.shape=torch.Size([32, 10])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "batch_l0_grads.shape=torch.Size([1, 10])\n",
      "batch_l1_grads.shape=torch.Size([1, 2560])\n",
      "buffer_l0_grads.shape=torch.Size([6, 10])\n",
      "buffer_l1_grads.shape=torch.Size([6, 2560])\n",
      "out of loop\n",
      "cur_buffer_l0_grads.shape=torch.Size([6, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([6, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([7, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([7, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([7, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([7, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([8, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([8, 2560])\n",
      "buffer_grads.shape=torch.Size([8, 2570])\n",
      "new_l0_grads.shape=torch.Size([12000, 10])\n",
      "new_l1_grads.shape=torch.Size([12000, 2560])\n",
      "buffer_grads.shape=torch.Size([8, 2570])\n",
      "current end\n",
      "new_grads.shape=torch.Size([12000, 2570])\n",
      "buffer_losses.shape=torch.Size([1, 8])\n",
      "forget_matrix.shape=torch.Size([12000, 8])\n",
      "best_ind=9009\n",
      "len(select_curr_indexes)=9010\n",
      "len(accumulate_select_indexes)=12000\n",
      "class1_grad_mean.shape=torch.Size([1, 2570])\n",
      "class2_grad_mean.shape=torch.Size([1, 2570])\n",
      "buffer_l0_grads.shape=torch.Size([32, 10])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "batch_l0_grads.shape=torch.Size([1, 10])\n",
      "batch_l1_grads.shape=torch.Size([1, 2560])\n",
      "buffer_l0_grads.shape=torch.Size([8, 10])\n",
      "buffer_l1_grads.shape=torch.Size([8, 2560])\n",
      "out of loop\n",
      "cur_buffer_l0_grads.shape=torch.Size([8, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([8, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([9, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([9, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([9, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([9, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([10, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([10, 2560])\n",
      "buffer_grads.shape=torch.Size([10, 2570])\n",
      "new_l0_grads.shape=torch.Size([12000, 10])\n",
      "new_l1_grads.shape=torch.Size([12000, 2560])\n",
      "buffer_grads.shape=torch.Size([10, 2570])\n",
      "current end\n",
      "new_grads.shape=torch.Size([12000, 2570])\n",
      "buffer_losses.shape=torch.Size([1, 10])\n",
      "forget_matrix.shape=torch.Size([12000, 10])\n",
      "best_ind=4616\n",
      "len(select_curr_indexes)=4617\n",
      "len(accumulate_select_indexes)=12000\n",
      "class1_grad_mean.shape=torch.Size([1, 2570])\n",
      "class2_grad_mean.shape=torch.Size([1, 2570])\n",
      "buffer_l0_grads.shape=torch.Size([32, 10])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "batch_l0_grads.shape=torch.Size([1, 10])\n",
      "batch_l1_grads.shape=torch.Size([1, 2560])\n",
      "buffer_l0_grads.shape=torch.Size([2, 10])\n",
      "buffer_l1_grads.shape=torch.Size([2, 2560])\n",
      "out of loop\n",
      "cur_buffer_l0_grads.shape=torch.Size([2, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([2, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([3, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([3, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([3, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([3, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([4, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([4, 2560])\n",
      "buffer_grads.shape=torch.Size([4, 2570])\n",
      "new_l0_grads.shape=torch.Size([12000, 10])\n",
      "new_l1_grads.shape=torch.Size([12000, 2560])\n",
      "buffer_grads.shape=torch.Size([4, 2570])\n",
      "current end\n",
      "new_grads.shape=torch.Size([12000, 2570])\n",
      "buffer_losses.shape=torch.Size([1, 4])\n",
      "forget_matrix.shape=torch.Size([12000, 4])\n",
      "best_ind=11243\n",
      "len(select_curr_indexes)=11244\n",
      "len(accumulate_select_indexes)=12000\n",
      "class1_grad_mean.shape=torch.Size([1, 2570])\n",
      "class2_grad_mean.shape=torch.Size([1, 2570])\n",
      "buffer_l0_grads.shape=torch.Size([32, 10])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "batch_l0_grads.shape=torch.Size([1, 10])\n",
      "batch_l1_grads.shape=torch.Size([1, 2560])\n",
      "buffer_l0_grads.shape=torch.Size([4, 10])\n",
      "buffer_l1_grads.shape=torch.Size([4, 2560])\n",
      "out of loop\n",
      "cur_buffer_l0_grads.shape=torch.Size([4, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([4, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([5, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([5, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([5, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([5, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([6, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([6, 2560])\n",
      "buffer_grads.shape=torch.Size([6, 2570])\n",
      "new_l0_grads.shape=torch.Size([12000, 10])\n",
      "new_l1_grads.shape=torch.Size([12000, 2560])\n",
      "buffer_grads.shape=torch.Size([6, 2570])\n",
      "current end\n",
      "new_grads.shape=torch.Size([12000, 2570])\n",
      "buffer_losses.shape=torch.Size([1, 6])\n",
      "forget_matrix.shape=torch.Size([12000, 6])\n",
      "best_ind=8435\n",
      "len(select_curr_indexes)=8436\n",
      "len(accumulate_select_indexes)=12000\n",
      "class1_grad_mean.shape=torch.Size([1, 2570])\n",
      "class2_grad_mean.shape=torch.Size([1, 2570])\n",
      "buffer_l0_grads.shape=torch.Size([32, 10])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "batch_l0_grads.shape=torch.Size([1, 10])\n",
      "batch_l1_grads.shape=torch.Size([1, 2560])\n",
      "buffer_l0_grads.shape=torch.Size([6, 10])\n",
      "buffer_l1_grads.shape=torch.Size([6, 2560])\n",
      "out of loop\n",
      "cur_buffer_l0_grads.shape=torch.Size([6, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([6, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([7, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([7, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([7, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([7, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([8, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([8, 2560])\n",
      "buffer_grads.shape=torch.Size([8, 2570])\n",
      "new_l0_grads.shape=torch.Size([12000, 10])\n",
      "new_l1_grads.shape=torch.Size([12000, 2560])\n",
      "buffer_grads.shape=torch.Size([8, 2570])\n",
      "current end\n",
      "new_grads.shape=torch.Size([12000, 2570])\n",
      "buffer_losses.shape=torch.Size([1, 8])\n",
      "forget_matrix.shape=torch.Size([12000, 8])\n",
      "best_ind=7577\n",
      "len(select_curr_indexes)=7578\n",
      "len(accumulate_select_indexes)=12000\n",
      "class1_grad_mean.shape=torch.Size([1, 2570])\n",
      "class2_grad_mean.shape=torch.Size([1, 2570])\n",
      "buffer_l0_grads.shape=torch.Size([32, 10])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "batch_l0_grads.shape=torch.Size([1, 10])\n",
      "batch_l1_grads.shape=torch.Size([1, 2560])\n",
      "buffer_l0_grads.shape=torch.Size([8, 10])\n",
      "buffer_l1_grads.shape=torch.Size([8, 2560])\n",
      "out of loop\n",
      "cur_buffer_l0_grads.shape=torch.Size([8, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([8, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([9, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([9, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([9, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([9, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([10, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([10, 2560])\n",
      "buffer_grads.shape=torch.Size([10, 2570])\n",
      "new_l0_grads.shape=torch.Size([12000, 10])\n",
      "new_l1_grads.shape=torch.Size([12000, 2560])\n",
      "buffer_grads.shape=torch.Size([10, 2570])\n",
      "current end\n",
      "new_grads.shape=torch.Size([12000, 2570])\n",
      "buffer_losses.shape=torch.Size([1, 10])\n",
      "forget_matrix.shape=torch.Size([12000, 10])\n",
      "best_ind=5683\n",
      "len(select_curr_indexes)=5684\n",
      "len(accumulate_select_indexes)=12000\n",
      "class1_grad_mean.shape=torch.Size([1, 2570])\n",
      "class2_grad_mean.shape=torch.Size([1, 2570])\n",
      "buffer_l0_grads.shape=torch.Size([32, 10])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "batch_l0_grads.shape=torch.Size([1, 10])\n",
      "batch_l1_grads.shape=torch.Size([1, 2560])\n",
      "buffer_l0_grads.shape=torch.Size([2, 10])\n",
      "buffer_l1_grads.shape=torch.Size([2, 2560])\n",
      "out of loop\n",
      "cur_buffer_l0_grads.shape=torch.Size([2, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([2, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([3, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([3, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([3, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([3, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([4, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([4, 2560])\n",
      "buffer_grads.shape=torch.Size([4, 2570])\n",
      "new_l0_grads.shape=torch.Size([12000, 10])\n",
      "new_l1_grads.shape=torch.Size([12000, 2560])\n",
      "buffer_grads.shape=torch.Size([4, 2570])\n",
      "current end\n",
      "new_grads.shape=torch.Size([12000, 2570])\n",
      "buffer_losses.shape=torch.Size([1, 4])\n",
      "forget_matrix.shape=torch.Size([12000, 4])\n",
      "best_ind=11999\n",
      "len(select_curr_indexes)=12000\n",
      "len(accumulate_select_indexes)=12000\n",
      "class1_grad_mean.shape=torch.Size([1, 2570])\n",
      "class2_grad_mean.shape=torch.Size([1, 2570])\n",
      "buffer_l0_grads.shape=torch.Size([32, 10])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "batch_l0_grads.shape=torch.Size([1, 10])\n",
      "batch_l1_grads.shape=torch.Size([1, 2560])\n",
      "buffer_l0_grads.shape=torch.Size([4, 10])\n",
      "buffer_l1_grads.shape=torch.Size([4, 2560])\n",
      "out of loop\n",
      "cur_buffer_l0_grads.shape=torch.Size([4, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([4, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([5, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([5, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([5, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([5, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([6, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([6, 2560])\n",
      "buffer_grads.shape=torch.Size([6, 2570])\n",
      "new_l0_grads.shape=torch.Size([12000, 10])\n",
      "new_l1_grads.shape=torch.Size([12000, 2560])\n",
      "buffer_grads.shape=torch.Size([6, 2570])\n",
      "current end\n",
      "new_grads.shape=torch.Size([12000, 2570])\n",
      "buffer_losses.shape=torch.Size([1, 6])\n",
      "forget_matrix.shape=torch.Size([12000, 6])\n",
      "best_ind=9004\n",
      "len(select_curr_indexes)=9005\n",
      "len(accumulate_select_indexes)=12000\n",
      "class1_grad_mean.shape=torch.Size([1, 2570])\n",
      "class2_grad_mean.shape=torch.Size([1, 2570])\n",
      "buffer_l0_grads.shape=torch.Size([32, 10])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "batch_l0_grads.shape=torch.Size([1, 10])\n",
      "batch_l1_grads.shape=torch.Size([1, 2560])\n",
      "buffer_l0_grads.shape=torch.Size([6, 10])\n",
      "buffer_l1_grads.shape=torch.Size([6, 2560])\n",
      "out of loop\n",
      "cur_buffer_l0_grads.shape=torch.Size([6, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([6, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([7, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([7, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([7, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([7, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([8, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([8, 2560])\n",
      "buffer_grads.shape=torch.Size([8, 2570])\n",
      "new_l0_grads.shape=torch.Size([12000, 10])\n",
      "new_l1_grads.shape=torch.Size([12000, 2560])\n",
      "buffer_grads.shape=torch.Size([8, 2570])\n",
      "current end\n",
      "new_grads.shape=torch.Size([12000, 2570])\n",
      "buffer_losses.shape=torch.Size([1, 8])\n",
      "forget_matrix.shape=torch.Size([12000, 8])\n",
      "best_ind=11397\n",
      "len(select_curr_indexes)=11398\n",
      "len(accumulate_select_indexes)=12000\n",
      "class1_grad_mean.shape=torch.Size([1, 2570])\n",
      "class2_grad_mean.shape=torch.Size([1, 2570])\n",
      "buffer_l0_grads.shape=torch.Size([32, 10])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "batch_l0_grads.shape=torch.Size([1, 10])\n",
      "batch_l1_grads.shape=torch.Size([1, 2560])\n",
      "buffer_l0_grads.shape=torch.Size([8, 10])\n",
      "buffer_l1_grads.shape=torch.Size([8, 2560])\n",
      "out of loop\n",
      "cur_buffer_l0_grads.shape=torch.Size([8, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([8, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([9, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([9, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([9, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([9, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([10, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([10, 2560])\n",
      "buffer_grads.shape=torch.Size([10, 2570])\n",
      "new_l0_grads.shape=torch.Size([12000, 10])\n",
      "new_l1_grads.shape=torch.Size([12000, 2560])\n",
      "buffer_grads.shape=torch.Size([10, 2570])\n",
      "current end\n",
      "new_grads.shape=torch.Size([12000, 2570])\n",
      "buffer_losses.shape=torch.Size([1, 10])\n",
      "forget_matrix.shape=torch.Size([12000, 10])\n",
      "best_ind=9933\n",
      "len(select_curr_indexes)=9934\n",
      "len(accumulate_select_indexes)=12000\n",
      "class1_grad_mean.shape=torch.Size([1, 2570])\n",
      "class2_grad_mean.shape=torch.Size([1, 2570])\n",
      "buffer_l0_grads.shape=torch.Size([32, 10])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "batch_l0_grads.shape=torch.Size([1, 10])\n",
      "batch_l1_grads.shape=torch.Size([1, 2560])\n",
      "buffer_l0_grads.shape=torch.Size([2, 10])\n",
      "buffer_l1_grads.shape=torch.Size([2, 2560])\n",
      "out of loop\n",
      "cur_buffer_l0_grads.shape=torch.Size([2, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([2, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([3, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([3, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([3, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([3, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([4, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([4, 2560])\n",
      "buffer_grads.shape=torch.Size([4, 2570])\n",
      "new_l0_grads.shape=torch.Size([12000, 10])\n",
      "new_l1_grads.shape=torch.Size([12000, 2560])\n",
      "buffer_grads.shape=torch.Size([4, 2570])\n",
      "current end\n",
      "new_grads.shape=torch.Size([12000, 2570])\n",
      "buffer_losses.shape=torch.Size([1, 4])\n",
      "forget_matrix.shape=torch.Size([12000, 4])\n",
      "best_ind=11999\n",
      "len(select_curr_indexes)=12000\n",
      "len(accumulate_select_indexes)=12000\n",
      "class1_grad_mean.shape=torch.Size([1, 2570])\n",
      "class2_grad_mean.shape=torch.Size([1, 2570])\n",
      "buffer_l0_grads.shape=torch.Size([32, 10])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "batch_l0_grads.shape=torch.Size([1, 10])\n",
      "batch_l1_grads.shape=torch.Size([1, 2560])\n",
      "buffer_l0_grads.shape=torch.Size([4, 10])\n",
      "buffer_l1_grads.shape=torch.Size([4, 2560])\n",
      "out of loop\n",
      "cur_buffer_l0_grads.shape=torch.Size([4, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([4, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([5, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([5, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([5, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([5, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([6, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([6, 2560])\n",
      "buffer_grads.shape=torch.Size([6, 2570])\n",
      "new_l0_grads.shape=torch.Size([12000, 10])\n",
      "new_l1_grads.shape=torch.Size([12000, 2560])\n",
      "buffer_grads.shape=torch.Size([6, 2570])\n",
      "current end\n",
      "new_grads.shape=torch.Size([12000, 2570])\n",
      "buffer_losses.shape=torch.Size([1, 6])\n",
      "forget_matrix.shape=torch.Size([12000, 6])\n",
      "best_ind=6209\n",
      "len(select_curr_indexes)=6210\n",
      "len(accumulate_select_indexes)=12000\n",
      "class1_grad_mean.shape=torch.Size([1, 2570])\n",
      "class2_grad_mean.shape=torch.Size([1, 2570])\n",
      "buffer_l0_grads.shape=torch.Size([32, 10])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "batch_l0_grads.shape=torch.Size([1, 10])\n",
      "batch_l1_grads.shape=torch.Size([1, 2560])\n",
      "buffer_l0_grads.shape=torch.Size([6, 10])\n",
      "buffer_l1_grads.shape=torch.Size([6, 2560])\n",
      "out of loop\n",
      "cur_buffer_l0_grads.shape=torch.Size([6, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([6, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([7, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([7, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([7, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([7, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([8, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([8, 2560])\n",
      "buffer_grads.shape=torch.Size([8, 2570])\n",
      "new_l0_grads.shape=torch.Size([12000, 10])\n",
      "new_l1_grads.shape=torch.Size([12000, 2560])\n",
      "buffer_grads.shape=torch.Size([8, 2570])\n",
      "current end\n",
      "new_grads.shape=torch.Size([12000, 2570])\n",
      "buffer_losses.shape=torch.Size([1, 8])\n",
      "forget_matrix.shape=torch.Size([12000, 8])\n",
      "best_ind=7515\n",
      "len(select_curr_indexes)=7516\n",
      "len(accumulate_select_indexes)=12000\n",
      "class1_grad_mean.shape=torch.Size([1, 2570])\n",
      "class2_grad_mean.shape=torch.Size([1, 2570])\n",
      "buffer_l0_grads.shape=torch.Size([32, 10])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "buffer_l0_expand.shape=torch.Size([32, 2560])\n",
      "batch_l0_grads.shape=torch.Size([1, 10])\n",
      "batch_l1_grads.shape=torch.Size([1, 2560])\n",
      "buffer_l0_grads.shape=torch.Size([8, 10])\n",
      "buffer_l1_grads.shape=torch.Size([8, 2560])\n",
      "out of loop\n",
      "cur_buffer_l0_grads.shape=torch.Size([8, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([8, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([9, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([9, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([9, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([9, 2560])\n",
      "cur_buffer_l0_grads.shape=torch.Size([10, 10])\n",
      "cur_buffer_l1_grads.shape=torch.Size([10, 2560])\n",
      "buffer_grads.shape=torch.Size([10, 2570])\n",
      "new_l0_grads.shape=torch.Size([12000, 10])\n",
      "new_l1_grads.shape=torch.Size([12000, 2560])\n",
      "buffer_grads.shape=torch.Size([10, 2570])\n",
      "current end\n",
      "new_grads.shape=torch.Size([12000, 2570])\n",
      "buffer_losses.shape=torch.Size([1, 10])\n",
      "forget_matrix.shape=torch.Size([12000, 10])\n",
      "best_ind=4866\n",
      "len(select_curr_indexes)=4867\n",
      "len(accumulate_select_indexes)=12000\n",
      "class1_grad_mean.shape=torch.Size([1, 2570])\n",
      "class2_grad_mean.shape=torch.Size([1, 2570])\n",
      "0.05\n",
      "alpha: 0.05 [[0.820881, 0.11402019608737719]]\n",
      "avg: 0.820881 0.11402019608737719\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------\n",
      "0.05 [[0.820881, 0.11402019608737719]]\n",
      "avg: 0.820881 0.11402019608737719\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "batch_size = 64\n",
    "buffer_size = 32\n",
    "n_class = 10\n",
    "loss_sample = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "if dataset in ['MNIST', 'FMNIST']:\n",
    "    n_feature = 28*28\n",
    "elif dataset in ['CIFAR10']:\n",
    "    n_feature = 32*32*3\n",
    "    \n",
    "if dataset == 'MNIST':\n",
    "    alpha_li = [0.05]\n",
    "    lamb_li = [50]\n",
    "\n",
    "elif dataset == 'FMNIST':\n",
    "    alpha_li = [0.05]\n",
    "    lamb_li = [10]\n",
    "\n",
    "# alpha_li = [0.05]\n",
    "# lamb_li = [50]\n",
    "\n",
    "# alpha_li = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]\n",
    "# lamb_li = [0.01, 0.05, 0.1, 1, 10, 50, 100]\n",
    "\n",
    "alpha_dict = {a: [] for a in alpha_li}\n",
    "\n",
    "# print(alpha_dict)\n",
    "\n",
    "for alpha in alpha_li:\n",
    "    \n",
    "    for lamb in lamb_li:\n",
    "\n",
    "        all_overall_acc = []\n",
    "        all_overall_fair = []\n",
    "\n",
    "        for s in range(5):\n",
    "            \n",
    "            random.seed(s)\n",
    "            np.random.seed(s)\n",
    "            torch.manual_seed(s)\n",
    "            torch.cuda.manual_seed_all(s)\n",
    "\n",
    "            if dataset in ['MNIST', 'FMNIST']:\n",
    "                model = NormalNN(input_features=n_feature, n_class=n_class, seed=s)\n",
    "            else:\n",
    "                model = ResNet18(num_classes=n_class, seed=s)\n",
    "            model = model.to(device)\n",
    "\n",
    "            seq_acc = {k: [] for k in range(n_class)}\n",
    "            overall_acc = []\n",
    "            overall_fair = []\n",
    "\n",
    "            for i in range(5):\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "#                 print('Task: ', i)\n",
    "                \n",
    "#                 print('memory1:', torch.cuda.memory_allocated(device=device)/1024/1024/1024)\n",
    "\n",
    "                if i == 0:\n",
    "                    x_train_batch = torch.cat(all_train_data[i*2:i*2+2])\n",
    "                    y_train_batch = torch.cat(all_train_label[i*2:i*2+2])\n",
    "\n",
    "                elif i > 0:\n",
    "                    x_train_batch = torch.cat(all_train_data[i*2:i*2+2])\n",
    "                    y_train_batch = torch.cat(all_train_label[i*2:i*2+2])\n",
    "                    if i == 1:\n",
    "                        x_prev_buffer1 = torch.cat(all_train_data[i*2-2:i*2-1])\n",
    "                        y_prev_buffer1 = torch.cat(all_train_label[i*2-2:i*2-1])\n",
    "                        indices1 = torch.randperm(len(x_prev_buffer1))[:buffer_size]\n",
    "                        x_prev_buffer1 = x_prev_buffer1[indices1]\n",
    "                        y_prev_buffer1 = y_prev_buffer1[indices1]\n",
    "\n",
    "                        x_prev_buffer2 = torch.cat(all_train_data[i*2-1:i*2])\n",
    "                        y_prev_buffer2 = torch.cat(all_train_label[i*2-1:i*2])\n",
    "                        indices2 = torch.randperm(len(x_prev_buffer2))[:buffer_size]\n",
    "                        x_prev_buffer2 = x_prev_buffer2[indices2]\n",
    "                        y_prev_buffer2 = y_prev_buffer2[indices2]\n",
    "\n",
    "                        x_prev_buffer = torch.cat([x_prev_buffer1, x_prev_buffer2])\n",
    "                        y_prev_buffer = torch.cat([y_prev_buffer1, y_prev_buffer2])\n",
    "\n",
    "                    else:\n",
    "        #                 # select random buffer\n",
    "        #                 x_new_buffer1 = torch.cat(all_train_data[i*2-2:i*2-1])\n",
    "        #                 y_new_buffer1 = torch.cat(all_train_label[i*2-2:i*2-1])\n",
    "        #                 indices1 = torch.randperm(len(x_new_buffer1))[:buffer_size]\n",
    "        #                 x_new_buffer1 = x_new_buffer1[indices1]\n",
    "        #                 y_new_buffer1 = y_new_buffer1[indices1]\n",
    "\n",
    "        #                 x_new_buffer2 = torch.cat(all_train_data[i*2-1:i*2])\n",
    "        #                 y_new_buffer2 = torch.cat(all_train_label[i*2-1:i*2])\n",
    "        #                 indices2 = torch.randperm(len(x_new_buffer2))[:buffer_size]\n",
    "        #                 x_new_buffer2 = x_new_buffer2[indices2]\n",
    "        #                 y_new_buffer2 = y_new_buffer2[indices2]\n",
    "\n",
    "        #                 x_new_buffer = torch.cat([x_new_buffer1, x_new_buffer2])\n",
    "        #                 y_new_buffer = torch.cat([y_new_buffer1, y_new_buffer2])\n",
    "\n",
    "                        # select gradient-based herding buffer\n",
    "                        x_new_buffer = torch.cat(all_train_data[i*2-2:i*2])\n",
    "                        y_new_buffer = torch.cat(all_train_label[i*2-2:i*2])\n",
    "\n",
    "                        x_new_buffer = x_new_buffer[select_buffer_indexes]\n",
    "                        y_new_buffer = y_new_buffer[select_buffer_indexes]\n",
    "\n",
    "                        x_prev_buffer = torch.cat([x_prev_buffer, x_new_buffer])\n",
    "                        y_prev_buffer = torch.cat([y_prev_buffer, y_new_buffer])\n",
    "                        \n",
    "#                     print('memory2:', torch.cuda.memory_allocated(device=device)/1024/1024/1024)\n",
    "\n",
    "                    buffer_losses = []\n",
    "\n",
    "                    # computation of mean gradients and losses for buffers \n",
    "                    for n in range(i*2):\n",
    "                        buffer_ind = [m for m in range(len(y_prev_buffer)) if y_prev_buffer[m] == n]\n",
    "                        x_buffer = x_prev_buffer[buffer_ind]\n",
    "                        y_buffer = y_prev_buffer[buffer_ind]\n",
    "                        x_buffer = torch.Tensor(x_buffer).to(device, dtype=torch.float32)\n",
    "                        y_buffer = torch.Tensor(y_buffer).to(device, dtype=torch.int64)\n",
    "\n",
    "                        if n == 0:\n",
    "                            model.zero_grad()\n",
    "\n",
    "                            # mean gradient computation\n",
    "                            out, emb = model(x_buffer)\n",
    "\n",
    "                            init_out_buffer = out\n",
    "                            init_emb_buffer = emb\n",
    "                            init_y_buffer = y_buffer.view(-1, 1)\n",
    "\n",
    "                            loss = loss_sample(out, y_buffer).sum()\n",
    "                            buffer_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "                            print(f\"{buffer_l0_grads.shape=}\")\n",
    "                            if dataset in ['MNIST', 'FMNIST']:\n",
    "                                buffer_l0_expand = torch.repeat_interleave(buffer_l0_grads, 256, dim=1)\n",
    "                            else:\n",
    "                                buffer_l0_expand = torch.repeat_interleave(buffer_l0_grads, 512, dim=1)\n",
    "                            print(f\"{buffer_l0_expand.shape=}\")\n",
    "                            buffer_l1_grads = buffer_l0_expand * emb.repeat(1, n_class)\n",
    "                            print(f\"{buffer_l0_expand.shape=}\")\n",
    "\n",
    "                            buffer_l0_grads = buffer_l0_grads.mean(dim=0).view(1, -1)\n",
    "                            buffer_l1_grads = buffer_l1_grads.mean(dim=0).view(1, -1)\n",
    "\n",
    "\n",
    "                            # mean loss computation\n",
    "                            buffer_ds = TensorDataset(x_buffer, y_buffer)\n",
    "                            buffer_loader = DataLoader(dataset=buffer_ds, batch_size=batch_size, shuffle=True)\n",
    "                            buffer_output, buffer_loss = clf.evaluate(buffer_loader)\n",
    "                            buffer_losses.append(buffer_loss)\n",
    "\n",
    "                        else:\n",
    "                            model.zero_grad()\n",
    "\n",
    "                            # mean gradient computation\n",
    "                            out, emb = model(x_buffer)\n",
    "\n",
    "                            init_out_buffer = torch.cat((init_out_buffer, out), dim=0)\n",
    "                            init_emb_buffer = torch.cat((init_emb_buffer, emb), dim=0)\n",
    "                            init_y_buffer = torch.cat((init_y_buffer, y_buffer.view(-1, 1)), dim=0)\n",
    "\n",
    "                            loss = loss_sample(out, y_buffer).sum()\n",
    "                            batch_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "                            if dataset in ['MNIST', 'FMNIST']:\n",
    "                                batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 256, dim=1)\n",
    "                            else:\n",
    "                                batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 512, dim=1)\n",
    "\n",
    "                            batch_l1_grads = batch_l0_expand * emb.repeat(1, n_class)\n",
    "\n",
    "                            batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)\n",
    "                            batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)\n",
    "\n",
    "                            buffer_l0_grads = torch.cat((buffer_l0_grads, batch_l0_grads), dim=0)\n",
    "                            buffer_l1_grads = torch.cat((buffer_l1_grads, batch_l1_grads), dim=0)\n",
    "\n",
    "                            # mean loss computation\n",
    "                            buffer_ds = TensorDataset(x_buffer, y_buffer)\n",
    "                            buffer_loader = DataLoader(dataset=buffer_ds, batch_size=batch_size, shuffle=True)\n",
    "                            buffer_output, buffer_loss = clf.evaluate(buffer_loader)\n",
    "                            buffer_losses.append(buffer_loss)\n",
    "                            \n",
    "#                     print('memory3:', torch.cuda.memory_allocated(device=device)/1024/1024/1024)\n",
    "            \n",
    "                    print(f\"{batch_l0_grads.shape=}\")\n",
    "                    print(f\"{batch_l1_grads.shape=}\")\n",
    "                    print(f\"{buffer_l0_grads.shape=}\")\n",
    "                    print(f\"{buffer_l1_grads.shape=}\")\n",
    "                    print(\"out of loop\")\n",
    "\n",
    "\n",
    "                    # initialize individual sample gradients of new data\n",
    "                    new_l0_grads = torch.empty((0, n_class), device=device, dtype=torch.float32)\n",
    "                    if dataset in ['MNIST', 'FMNIST']:\n",
    "                        new_l1_grads = torch.empty((0, n_class*256), device=device, dtype=torch.float32)\n",
    "                    else:\n",
    "                        new_l1_grads = torch.empty((0, n_class*512), device=device, dtype=torch.float32)\n",
    "\n",
    "                    # computation of mean and individual gradients and mean losses for new data \n",
    "                    for n in range(2):\n",
    "                        model.zero_grad()\n",
    "\n",
    "                        # mean gradient computation\n",
    "                        x_new_buffer = torch.cat(all_train_data[i*2+n:i*2+n+1])\n",
    "                        y_new_buffer = torch.cat(all_train_label[i*2+n:i*2+n+1])\n",
    "                        x_new_buffer = torch.Tensor(x_new_buffer).to(device, dtype=torch.float32)\n",
    "                        y_new_buffer = torch.Tensor(y_new_buffer).to(device, dtype=torch.int64)\n",
    "                        \n",
    "                        new_buffer_ds = TensorDataset(x_new_buffer, y_new_buffer)\n",
    "                        new_buffer_loader = DataLoader(dataset=new_buffer_ds, batch_size=batch_size, shuffle=False)\n",
    "                        \n",
    "                        for batch_idx, batch_data in enumerate(new_buffer_loader):\n",
    "                            model.zero_grad()\n",
    "                            if batch_idx == 0:\n",
    "                                x_batch, y_batch = batch_data\n",
    "\n",
    "                                out, emb = model(x_batch)\n",
    "\n",
    "                                init_out_buffer = torch.cat((init_out_buffer, out), dim=0)\n",
    "                                init_emb_buffer = torch.cat((init_emb_buffer, emb), dim=0)\n",
    "                                init_y_buffer = torch.cat((init_y_buffer, y_batch.view(-1, 1)), dim=0)\n",
    "\n",
    "                                loss = loss_sample(out, y_batch).sum()\n",
    "                                batch_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "                                \n",
    "                                with torch.no_grad():\n",
    "                                    if dataset in ['MNIST', 'FMNIST']:\n",
    "                                        batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 256, dim=1)\n",
    "                                    else:\n",
    "                                        batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 512, dim=1)\n",
    "                                    batch_l1_grads = batch_l0_expand * emb.repeat(1, n_class)\n",
    "                                \n",
    "                            else:\n",
    "                                \n",
    "                                x_batch, y_batch = batch_data\n",
    "\n",
    "                                out, emb = model(x_batch)\n",
    "\n",
    "                                init_out_buffer = torch.cat((init_out_buffer, out), dim=0)\n",
    "                                init_emb_buffer = torch.cat((init_emb_buffer, emb), dim=0)\n",
    "                                init_y_buffer = torch.cat((init_y_buffer, y_batch.view(-1, 1)), dim=0)\n",
    "\n",
    "                                loss = loss_sample(out, y_batch).sum()\n",
    "                                next_batch_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "                                \n",
    "                                with torch.no_grad():\n",
    "                                    if dataset in ['MNIST', 'FMNIST']:\n",
    "                                        next_batch_l0_expand = torch.repeat_interleave(next_batch_l0_grads, 256, dim=1)\n",
    "                                    else:\n",
    "                                        next_batch_l0_expand = torch.repeat_interleave(next_batch_l0_grads, 512, dim=1)\n",
    "                                    next_batch_l1_grads = next_batch_l0_expand * emb.repeat(1, n_class)\n",
    "\n",
    "                                    batch_l0_grads = torch.cat((batch_l0_grads, next_batch_l0_grads), dim=0)\n",
    "                                    batch_l1_grads = torch.cat((batch_l1_grads, next_batch_l1_grads), dim=0)\n",
    "\n",
    "                        # individual gradients\n",
    "                        ind_l0_grads = batch_l0_grads.clone()\n",
    "                        ind_l1_grads = batch_l1_grads.clone()\n",
    "                        \n",
    "                        new_l0_grads = torch.cat((new_l0_grads, ind_l0_grads), dim=0)\n",
    "                        new_l1_grads = torch.cat((new_l1_grads, ind_l1_grads), dim=0)\n",
    "                                \n",
    "                        # mean gradients\n",
    "                        batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)\n",
    "                        batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)\n",
    "                        print(f\"cur_{buffer_l0_grads.shape=}\")\n",
    "                        print(f\"cur_{buffer_l1_grads.shape=}\")\n",
    "\n",
    "\n",
    "                        buffer_l0_grads = torch.cat((buffer_l0_grads, batch_l0_grads), dim=0)\n",
    "                        buffer_l1_grads = torch.cat((buffer_l1_grads, batch_l1_grads), dim=0)\n",
    "                        print(f\"cur_{buffer_l0_grads.shape=}\")\n",
    "                        print(f\"cur_{buffer_l1_grads.shape=}\")\n",
    "\n",
    "                        # mean loss computation\n",
    "                        buffer_ds = TensorDataset(x_new_buffer, y_new_buffer)\n",
    "                        buffer_loader = DataLoader(dataset=buffer_ds, batch_size=batch_size, shuffle=True)\n",
    "                        buffer_output, buffer_loss = clf.evaluate(buffer_loader)\n",
    "                        buffer_losses.append(buffer_loss)\n",
    "                        \n",
    "#                     print('memory4:', torch.cuda.memory_allocated(device=device)/1024/1024/1024)\n",
    "                        \n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                    buffer_grads = torch.cat((buffer_l0_grads, buffer_l1_grads), dim=1)\n",
    "                    print(f\"{buffer_grads.shape=}\")\n",
    "                    print(f\"{new_l0_grads.shape=}\")\n",
    "                    print(f\"{new_l1_grads.shape=}\")\n",
    "                    \n",
    "\n",
    "                    buffer_grads = f.normalize(buffer_grads, p=2, dim=1)\n",
    "#                     print('buffer grads:', buffer_grads.shape)\n",
    "#                     print('buffer grads norm:', torch.norm(buffer_grads, dim=1))\n",
    "\n",
    "                    print(f\"{buffer_grads.shape=}\")\n",
    "                    print(\"current end\")\n",
    "\n",
    "#                     # initialize individual sample gradients of new data\n",
    "#                     new_l0_grads = torch.empty((0, n_class), device=device, dtype=torch.float32)\n",
    "#                     if dataset in ['MNIST', 'FMNIST']:\n",
    "#                         new_l1_grads = torch.empty((0, n_class*256), device=device, dtype=torch.float32)\n",
    "#                     else:\n",
    "#                         new_l1_grads = torch.empty((0, n_class*512), device=device, dtype=torch.float32)\n",
    "\n",
    "#                     # compute individual sample gradients of new data\n",
    "#                     for n in range(2):\n",
    "#                         model.zero_grad()\n",
    "\n",
    "#                         # individual sample gradient computation\n",
    "#                         x_new = all_train_data[i*2+n:i*2+n+1][0]\n",
    "#                         y_new = all_train_label[i*2+n:i*2+n+1][0]\n",
    "#                         x_new = torch.Tensor(x_new).to(device, dtype=torch.float32)\n",
    "#                         y_new = torch.Tensor(y_new).to(device, dtype=torch.int64)\n",
    "                        \n",
    "#                         print('len new:', len(x_new))\n",
    "\n",
    "#                         out, emb = model(x_new)\n",
    "#                         loss = loss_sample(out, y_new).sum()\n",
    "#                         batch_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "#                         if dataset in ['MNIST', 'FMNIST']:\n",
    "#                             batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 256, dim=1)\n",
    "#                         else:\n",
    "#                             batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 512, dim=1)\n",
    "#                         batch_l1_grads = batch_l0_expand * emb.repeat(1, n_class)\n",
    "\n",
    "#                         new_l0_grads = torch.cat((new_l0_grads, batch_l0_grads), dim=0)\n",
    "#                         new_l1_grads = torch.cat((new_l1_grads, batch_l1_grads), dim=0)\n",
    "                    \n",
    "                        \n",
    "#                     print('memory5:', torch.cuda.memory_allocated(device=device)/1024/1024/1024)\n",
    "                        \n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "\n",
    "                        new_grads = torch.cat((new_l0_grads, new_l1_grads), dim=1)\n",
    "                        print(f\"{new_grads.shape=}\")\n",
    "                        new_grads_origin = new_grads.clone()\n",
    "                        new_grads = f.normalize(new_grads, p=2, dim=1)\n",
    "#                         print('new grads:', new_grads.shape)\n",
    "#                         print('new grads norm:', torch.norm(new_grads, dim=1))\n",
    "#                         print('new grads shape:', new_grads, new_grads.shape)\n",
    "\n",
    "                        buffer_losses = torch.tensor(buffer_losses).view(1,-1)\n",
    "                        print(f\"{buffer_losses.shape=}\")\n",
    "            #             print('initial loss:', buffer_losses)\n",
    "            #             print('initial mean, std:', buffer_losses.mean(dim=1).item(), buffer_losses.std(dim=1).item())\n",
    "\n",
    "                        loss_matrix = buffer_losses.repeat(len(new_grads), 1).to(device)\n",
    "                        loss_matrix_origin = loss_matrix.clone()\n",
    "                        forget_matrix = torch.matmul(new_grads, torch.transpose(buffer_grads, 0, 1)).to(device)\n",
    "\n",
    "                    print(f\"{forget_matrix.shape=}\")\n",
    "                    \n",
    "#                     print('init forget matrix shape:', forget_matrix.shape)\n",
    "                    \n",
    "#                     print('init memory:', torch.cuda.memory_allocated(device=device)/1024/1024/1024)\n",
    "\n",
    "                    accumulate_select_indexes = []\n",
    "                    accumulate_mean = []\n",
    "                    accumulate_std = []\n",
    "                    accumulate_sum = []\n",
    "\n",
    "                    select_indexes = []\n",
    "                    non_select_indexes = list(range(len(x_train_batch)))\n",
    "\n",
    "                    num_class1 = 0\n",
    "                    num_class2 = 0\n",
    "\n",
    "                    # current data selection\n",
    "                    for b in range(len(x_train_batch)):\n",
    "                        \n",
    "#                         with torch.no_grad():\n",
    "                            \n",
    "                        torch.cuda.empty_cache()\n",
    "        \n",
    "#                         print('memoryb:', torch.cuda.memory_allocated(device=device)/1024/1024/1024)\n",
    "\n",
    "                        loss_matrix = loss_matrix - alpha * forget_matrix\n",
    "                        loss_mean = torch.mean(loss_matrix, dim=1, keepdim=True)\n",
    "                        loss_std = torch.std(loss_matrix, dim=1, keepdim=True)\n",
    "\n",
    "        #                 select_ind = torch.argmin(loss_mean, dim=0)\n",
    "        #                 select_ind = torch.argmin(loss_std, dim=0)\n",
    "                        select_ind = torch.argmin(loss_mean + loss_std, dim=0)\n",
    "\n",
    "                        accumulate_mean.append(copy.deepcopy(loss_mean[select_ind].item()))\n",
    "                        accumulate_std.append(copy.deepcopy(loss_std[select_ind].item()))\n",
    "                        accumulate_sum.append(copy.deepcopy(loss_mean[select_ind].item() + loss_std[select_ind].item()))\n",
    "\n",
    "                        if non_select_indexes[select_ind.item()] < len(x_train_batch)/2:\n",
    "                            num_class1 += 1\n",
    "                        else:\n",
    "                            num_class2 += 1\n",
    "\n",
    "                        select_indexes.append(non_select_indexes[select_ind.item()])\n",
    "                        accumulate_select_indexes.append(copy.deepcopy(select_indexes))\n",
    "                        del non_select_indexes[select_ind.item()]\n",
    "\n",
    "                        best_buffer_losses = loss_matrix[select_ind].view(1,-1)\n",
    "                        loss_matrix = best_buffer_losses.repeat(len(new_grads)-1, 1).to(device)\n",
    "\n",
    "#                         # update buffer grads and corresponding forget matrix\n",
    "#                         if b == 0:\n",
    "#                             grads_curr = new_grads[select_ind].clone().view(1, -1)\n",
    "#                         else:\n",
    "#                             grads_curr += new_grads[select_ind].clone().view(1,-1)\n",
    "#         #                     grads_curr += new_grads[select_ind].clone().sum(dim=0)\n",
    "\n",
    "                        new_grads = torch.cat((new_grads[:select_ind.item()], new_grads[select_ind.item()+1:]))\n",
    "\n",
    "#                         model.zero_grad()\n",
    "\n",
    "#                         embDim = 256\n",
    "\n",
    "#                         out_vec = init_out_buffer - (alpha * grads_curr[0][0:n_class].view(1, -1).expand(init_out_buffer.shape[0], -1))\n",
    "\n",
    "#                         out_vec = out_vec - (alpha * torch.matmul(init_emb_buffer, grads_curr[0][n_class:].view(n_class, -1).transpose(0, 1)))\n",
    "\n",
    "#                         loss = loss_sample(out_vec, init_y_buffer.view(-1)).sum()\n",
    "#                         l0_grads = torch.autograd.grad(loss, out_vec)[0]\n",
    "#                         l0_expand = torch.repeat_interleave(l0_grads, embDim, dim=1)\n",
    "#                         l1_grads = l0_expand * init_emb_buffer.repeat(1, n_class)\n",
    "\n",
    "#                         split_ind = []\n",
    "#                         for n in range(i*2):\n",
    "#                             split_ind.append((n+1)*buffer_size)\n",
    "#                         split_ind.append((i*2)*buffer_size+int(len(x_train_batch)/2))\n",
    "\n",
    "#                         l0_grads = torch.tensor_split(l0_grads, split_ind, dim=0)\n",
    "#                         mean_l0_grads = []\n",
    "#                         for n in range(len(l0_grads)):\n",
    "#                             mean_l0_grads.append(torch.mean(l0_grads[n], dim=0).view(1, -1))\n",
    "#                         l0_grads = torch.cat(mean_l0_grads, dim=0)\n",
    "\n",
    "#                         l1_grads = torch.tensor_split(l1_grads, split_ind, dim=0)\n",
    "#                         mean_l1_grads = []\n",
    "#                         for n in range(len(l1_grads)):\n",
    "#                             mean_l1_grads.append(torch.mean(l1_grads[n], dim=0).view(1, -1))\n",
    "#                         l1_grads = torch.cat(mean_l1_grads, dim=0)\n",
    "\n",
    "#                         buffer_grads = torch.cat((l0_grads, l1_grads), dim=1)\n",
    "\n",
    "#                         with torch.no_grad():\n",
    "#                             forget_matrix = torch.matmul(new_grads, torch.transpose(buffer_grads, 0, 1)).to(device)\n",
    "                        \n",
    "#                         print('forget matrix shape:', forget_matrix.shape)\n",
    "\n",
    "                        forget_matrix = torch.cat((forget_matrix[:select_ind.item()], forget_matrix[select_ind.item()+1:]))\n",
    "\n",
    "        #                 if num_class1 == int(len(x_train_batch)/2):\n",
    "        #                     new_grads = new_grads[[n for n in range(len(forget_matrix)) if non_select_indexes[n] >= len(x_train_batch)/2]]\n",
    "        #                     forget_matrix = forget_matrix[[n for n in range(len(forget_matrix)) if non_select_indexes[n] >= len(x_train_batch)/2]]\n",
    "        #                     non_select_indexes = [n for n in non_select_indexes if n >= len(x_train_batch)/2]\n",
    "        #                     loss_matrix = best_buffer_losses.repeat(len(non_select_indexes), 1).to(device)\n",
    "\n",
    "        #                 elif num_class2 == int(len(x_train_batch)/2):\n",
    "        #                     new_grads = new_grads[[n for n in range(len(forget_matrix)) if non_select_indexes[n] < len(x_train_batch)/2]]\n",
    "        #                     forget_matrix = forget_matrix[[n for n in range(len(forget_matrix)) if non_select_indexes[n] < len(x_train_batch)/2]]\n",
    "        #                     non_select_indexes = [n for n in non_select_indexes if n < len(x_train_batch)/2]\n",
    "        #                     loss_matrix = best_buffer_losses.repeat(len(non_select_indexes), 1).to(device)\n",
    "\n",
    "        #             best_ind = np.argmin(np.array(accumulate_mean))\n",
    "        #             best_ind = np.argmin(np.array(accumulate_std))\n",
    "                    best_ind = np.argmin(np.array(accumulate_sum))\n",
    "                    print(f\"{best_ind=}\")\n",
    "                    select_curr_indexes = accumulate_select_indexes[best_ind]\n",
    "                    print(f\"{len(select_curr_indexes)=}\")\n",
    "                    print(f\"{len(accumulate_select_indexes)=}\")\n",
    "                    \n",
    "\n",
    "\n",
    "                    # buffer data selection\n",
    "                    select_buffer_indexes = []\n",
    "\n",
    "                    class1_indexes = list(range(0, int(len(x_train_batch)/2)))\n",
    "                    class2_indexes = list(range(int(len(x_train_batch)/2), len(x_train_batch)))\n",
    "\n",
    "        #             class1_indexes = [ind for ind in select_curr_indexes if ind < len(x_train_batch)/2]\n",
    "        #             class2_indexes = [ind for ind in select_curr_indexes if ind >= len(x_train_batch)/2]\n",
    "\n",
    "                    class1_grad_mean = new_grads_origin[class1_indexes].mean(dim=0).view(1, -1)\n",
    "                    class2_grad_mean = new_grads_origin[class2_indexes].mean(dim=0).view(1, -1)\n",
    "\n",
    "                    print(f\"{class1_grad_mean.shape=}\")\n",
    "                    print(f\"{class2_grad_mean.shape=}\")\n",
    "\n",
    "                    candidate_class1_indexes = list(range(0, int(len(x_train_batch)/2)))\n",
    "                    class1_buffer_indexes = []\n",
    "                    for m in range(buffer_size):\n",
    "                        buffer_ind = torch.argmin(torch.norm(class1_grad_mean.repeat(int(len(new_grads_origin)/2)-m, 1) - (torch.sum(new_grads_origin[class1_buffer_indexes], dim=0).repeat(int(len(new_grads_origin)/2)-m,1) + new_grads_origin[candidate_class1_indexes])/(m+1), dim=1), dim=0)\n",
    "                        class1_buffer_indexes.append(copy.deepcopy(candidate_class1_indexes[buffer_ind]))\n",
    "                        del candidate_class1_indexes[buffer_ind]\n",
    "\n",
    "        #             print('class1 buffer indexes:', class1_buffer_indexes, len(class1_buffer_indexes))\n",
    "\n",
    "                    candidate_class2_indexes = list(range(int(len(x_train_batch)/2), len(x_train_batch)))\n",
    "                    class2_buffer_indexes = []\n",
    "                    for m in range(buffer_size):\n",
    "                        buffer_ind = torch.argmin(torch.norm(class2_grad_mean.repeat(int(len(new_grads_origin)/2)-m, 1) - (torch.sum(new_grads_origin[class2_buffer_indexes], dim=0).repeat(int(len(new_grads_origin)/2)-m,1) + new_grads_origin[candidate_class2_indexes])/(m+1), dim=1), dim=0)\n",
    "                        class2_buffer_indexes.append(copy.deepcopy(candidate_class2_indexes[buffer_ind]))\n",
    "                        del candidate_class2_indexes[buffer_ind]\n",
    "\n",
    "        #             print('class2 buffer indexes:', class2_buffer_indexes, len(class2_buffer_indexes))\n",
    "\n",
    "                    for ind in class1_buffer_indexes:\n",
    "                        select_buffer_indexes.append(ind)\n",
    "\n",
    "                    for ind in class2_buffer_indexes:\n",
    "                        select_buffer_indexes.append(ind)\n",
    "\n",
    "#                     print('buffer data:')\n",
    "#                     for c in range(i*2+2):\n",
    "#                         print(c, y_prev_buffer.cpu().tolist().count(c))\n",
    "\n",
    "                    select_curr_indexes = list(set(select_curr_indexes))\n",
    "\n",
    "                    # current data selection\n",
    "                    x_train_batch = x_train_batch[select_curr_indexes]\n",
    "                    y_train_batch = y_train_batch[select_curr_indexes]\n",
    "\n",
    "                x_train_batch = torch.Tensor(x_train_batch).to(device, dtype=torch.float32)\n",
    "                y_train_batch = torch.Tensor(y_train_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "#                 print('current data:')\n",
    "#                 for c in range(i*2+2):\n",
    "#                     print(c, y_train_batch.cpu().tolist().count(c))\n",
    "\n",
    "                train_ds = TensorDataset(x_train_batch, y_train_batch)\n",
    "                train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "                optimizer_config = {\"lr\": 0.001}\n",
    "                \n",
    "                model.zero_grad()\n",
    "\n",
    "                # Model training using current data and buffer data\n",
    "                if i == 0:\n",
    "                    clf = NNClassifier(model, nn.CrossEntropyLoss(reduction='mean'), optim.Adam, optimizer_config)\n",
    "                    clf.fit({\"train\": train_loader, \"val\": train_loader}, epochs=1, earlystop_path=f'./ckpt/joint.pt')\n",
    "                elif i > 0:\n",
    "#                     print('len prev buffer:', len(x_prev_buffer))\n",
    "                    clf = NNClassifier_CL(model, nn.CrossEntropyLoss(reduction='mean'), optim.Adam, optimizer_config)\n",
    "                    clf.fit({\"train\": train_loader, \"val\": train_loader, \"buffer\": (x_prev_buffer, y_prev_buffer)}, \n",
    "                            epochs=1, sample_size=64, lamb=lamb, device=device, earlystop_path=f'./ckpt/joint.pt', seed=s)\n",
    "\n",
    "                # Model evaluation\n",
    "                all_test_acc = []\n",
    "                all_test_loss = []\n",
    "\n",
    "                for j in range(n_class):\n",
    "\n",
    "                    if j < i*2+2:\n",
    "\n",
    "                        x_test_batch = all_test_data[j]\n",
    "                        y_test_batch = all_test_label[j]\n",
    "\n",
    "                        x_test_batch = torch.Tensor(x_test_batch).to(device, dtype=torch.float32)\n",
    "                        y_test_batch = torch.Tensor(y_test_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "                        test_ds = TensorDataset(x_test_batch, y_test_batch)\n",
    "                        test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "                        test_output, test_loss = clf.evaluate(test_loader)\n",
    "                        test_acc = accuracy_score(test_output['true_y'], test_output['output'])\n",
    "\n",
    "                        cf_li = list(confusion_matrix(test_output['true_y'], test_output['output'], labels=range(i*2+2))[j])\n",
    "\n",
    "                        cf_matrix = {}\n",
    "                        for k in range(len(cf_li)):\n",
    "                            cf_matrix[label_li[k]] = cf_li[k]\n",
    "\n",
    "#                         print('test %d: acc= %.3f' %(j, test_acc), cf_matrix)\n",
    "\n",
    "                        all_test_acc.append(test_acc)\n",
    "                        all_test_loss.append(test_loss)\n",
    "                        seq_acc[j].append(test_acc)\n",
    "\n",
    "                    else:\n",
    "                        seq_acc[j].append(0)\n",
    "\n",
    "#                 print('overall: acc = %.3f, fair = %.3f' %(np.mean(all_test_acc), np.std(all_test_acc)))\n",
    "                overall_acc.append(np.mean(all_test_acc))\n",
    "                overall_fair.append(np.std(all_test_acc))\n",
    "\n",
    "#                 print('\\n')\n",
    "\n",
    "#             print('sequential acc: ', seq_acc)\n",
    "\n",
    "#             plt.figure(figsize=(12, 8))\n",
    "#             for i in range(5):\n",
    "#                 plt.subplot(2,3,i+1)\n",
    "#                 plt.plot(seq_acc[i*2], label='Class '+str(i*2))\n",
    "#                 plt.plot(seq_acc[i*2+1], label='Class '+str(i*2+1))\n",
    "#                 plt.xticks(range(5))\n",
    "#                 plt.legend(loc='best')\n",
    "#                 plt.xlabel(\"Task\")\n",
    "#                 plt.ylabel(\"Accuracy\")\n",
    "#             plt.show()\n",
    "\n",
    "#             print('overall acc: ', overall_acc, 'avg:', np.mean(overall_acc))\n",
    "#             print('overall fair:', overall_fair, 'avg:', np.mean(overall_fair))\n",
    "\n",
    "            all_overall_acc.append(np.mean(overall_acc))\n",
    "            all_overall_fair.append(np.mean(overall_fair))\n",
    "\n",
    "#             plt.figure(figsize=(10, 4))\n",
    "#             plt.subplot(1,2,1)\n",
    "#             plt.plot(overall_acc)\n",
    "#             plt.xticks(range(5))\n",
    "#             plt.xlabel(\"Task\")\n",
    "#             plt.ylabel(\"Accuracy\")\n",
    "\n",
    "#             plt.subplot(1,2,2)\n",
    "#             plt.plot(overall_fair)\n",
    "#             plt.xticks(range(5))\n",
    "#             plt.xlabel(\"Task\")\n",
    "#             plt.ylabel(\"Fairness\")\n",
    "#             plt.show()\n",
    "\n",
    "#             print('-------------------------------------------------------------------------------------------------')\n",
    "\n",
    "#         print('all overall acc:', np.mean(all_overall_acc), np.std(all_overall_acc))\n",
    "#         print('all overall fair:', np.mean(all_overall_fair), np.std(all_overall_fair))\n",
    "        \n",
    "        alpha_dict[alpha].append([np.mean(all_overall_acc), np.mean(all_overall_fair)])\n",
    "        print(alpha)\n",
    "        \n",
    "    print('alpha:', alpha, alpha_dict[alpha])\n",
    "    print('avg:', np.mean([e[0] for e in alpha_dict[alpha]]), np.mean([e[1] for e in alpha_dict[alpha]]))\n",
    "    print('\\n')\n",
    "\n",
    "print('-------------------------------------------------------------------------------------------------')\n",
    "        \n",
    "for k, v in alpha_dict.items():\n",
    "    print(k, v)\n",
    "    print('avg:', np.mean([e[0] for e in v]), np.mean([e[1] for e in v]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e92b19b",
   "metadata": {},
   "source": [
    "## A-GEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f1efaa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "# n_class = 10\n",
    "# s = 0\n",
    "# buffer_size = 100\n",
    "# curr_size = 4000\n",
    "# # curr_size = 500\n",
    "# perBatch_buffer = True\n",
    "# perBatch_curr = False\n",
    "\n",
    "# if dataset in ['MNIST', 'FMNIST']:\n",
    "#     n_feature = 28*28\n",
    "# elif dataset in ['CIFAR10']:\n",
    "#     n_feature = 32*32*3\n",
    "\n",
    "# for s in range(1):\n",
    "    \n",
    "#     method_bound = []\n",
    "\n",
    "#     model = NormalNN(input_features=n_feature, n_class=n_class, seed=s)\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     seq_acc = {k: [] for k in range(n_class)}\n",
    "#     all_overall_acc = []\n",
    "\n",
    "#     for i in range(5):\n",
    "        \n",
    "#         method_acc = []\n",
    "\n",
    "#         print('batch ind: ', i)\n",
    "\n",
    "#         if i == 0:\n",
    "# #             x_train_batch = torch.cat(all_train_data[i*2:i*2+2])\n",
    "# #             y_train_batch = torch.cat(all_train_label[i*2:i*2+2])\n",
    "\n",
    "#             x_train_batch1 = torch.cat(all_train_data[i*2:i*2+1])\n",
    "#             y_train_batch1 = torch.cat(all_train_label[i*2:i*2+1])\n",
    "#             indices1 = torch.randperm(len(x_train_batch1))[:curr_size]\n",
    "            \n",
    "#             x_train_batch2 = torch.cat(all_train_data[i*2+1:i*2+2])\n",
    "#             y_train_batch2 = torch.cat(all_train_label[i*2+1:i*2+2])\n",
    "#             indices2 = torch.randperm(len(x_train_batch2))[:curr_size]\n",
    "            \n",
    "#             x_train_batch = torch.cat([x_train_batch1[indices1], x_train_batch2[indices2]])\n",
    "#             y_train_batch = torch.cat([y_train_batch1[indices1], y_train_batch2[indices2]])\n",
    "\n",
    "#         elif i > 0:\n",
    "# #             x_train_batch = torch.cat(all_train_data[i*2:i*2+2])\n",
    "# #             y_train_batch = torch.cat(all_train_label[i*2:i*2+2])\n",
    "\n",
    "#             x_train_batch1 = torch.cat(all_train_data[i*2:i*2+1])\n",
    "#             y_train_batch1 = torch.cat(all_train_label[i*2:i*2+1])\n",
    "#             indices1 = torch.randperm(len(x_train_batch1))[:curr_size]\n",
    "            \n",
    "#             x_train_batch2 = torch.cat(all_train_data[i*2+1:i*2+2])\n",
    "#             y_train_batch2 = torch.cat(all_train_label[i*2+1:i*2+2])\n",
    "#             indices2 = torch.randperm(len(x_train_batch2))[:curr_size]\n",
    "            \n",
    "#             x_train_batch = torch.cat([x_train_batch1[indices1], x_train_batch2[indices2]])\n",
    "#             y_train_batch = torch.cat([y_train_batch1[indices1], y_train_batch2[indices2]])\n",
    "\n",
    "#             if i == 1:\n",
    "#                 x_prev_buffer1 = torch.cat(all_train_data[i*2-2:i*2-1])\n",
    "#                 y_prev_buffer1 = torch.cat(all_train_label[i*2-2:i*2-1])\n",
    "#                 indices1 = torch.randperm(len(x_prev_buffer1))[:buffer_size]\n",
    "#                 x_prev_buffer1 = x_prev_buffer1[indices1]\n",
    "#                 y_prev_buffer1 = y_prev_buffer1[indices1]\n",
    "\n",
    "#                 x_prev_buffer2 = torch.cat(all_train_data[i*2-1:i*2])\n",
    "#                 y_prev_buffer2 = torch.cat(all_train_label[i*2-1:i*2])\n",
    "#                 indices2 = torch.randperm(len(x_prev_buffer2))[:buffer_size]\n",
    "#                 x_prev_buffer2 = x_prev_buffer2[indices2]\n",
    "#                 y_prev_buffer2 = y_prev_buffer2[indices2]\n",
    "\n",
    "#                 x_prev_buffer = torch.cat([x_prev_buffer1, x_prev_buffer2])\n",
    "#                 y_prev_buffer = torch.cat([y_prev_buffer1, y_prev_buffer2])\n",
    "\n",
    "#             else:\n",
    "#                 x_new_buffer1 = torch.cat(all_train_data[i*2-2:i*2-1])\n",
    "#                 y_new_buffer1 = torch.cat(all_train_label[i*2-2:i*2-1])\n",
    "#                 indices1 = torch.randperm(len(x_new_buffer1))[:buffer_size]\n",
    "#                 x_new_buffer1 = x_new_buffer1[indices1]\n",
    "#                 y_new_buffer1 = y_new_buffer1[indices1]\n",
    "\n",
    "#                 x_new_buffer2 = torch.cat(all_train_data[i*2-1:i*2])\n",
    "#                 y_new_buffer2 = torch.cat(all_train_label[i*2-1:i*2])\n",
    "#                 indices2 = torch.randperm(len(x_new_buffer2))[:buffer_size]\n",
    "#                 x_new_buffer2 = x_new_buffer2[indices2]\n",
    "#                 y_new_buffer2 = y_new_buffer2[indices2]\n",
    "\n",
    "#                 x_new_buffer = torch.cat([x_new_buffer1, x_new_buffer2])\n",
    "#                 y_new_buffer = torch.cat([y_new_buffer1, y_new_buffer2])\n",
    "\n",
    "#                 x_prev_buffer = torch.cat([x_prev_buffer, x_new_buffer])\n",
    "#                 y_prev_buffer = torch.cat([y_prev_buffer, y_new_buffer])\n",
    "\n",
    "# #             # gradient computation\n",
    "# #             loss_sample = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# #             for n in range(i*2):\n",
    "# #                 x_buffer = x_prev_buffer[n*buffer_size:(n+1)*buffer_size]\n",
    "# #                 y_buffer = y_prev_buffer[n*buffer_size:(n+1)*buffer_size]\n",
    "# #                 x_buffer = torch.Tensor(x_buffer).to(device, dtype=torch.float32)\n",
    "# #                 y_buffer = torch.Tensor(y_buffer).to(device, dtype=torch.int64)\n",
    "\n",
    "# #                 if n == 0:    \n",
    "# #                     out, emb = model(x_buffer)\n",
    "# #                     loss = loss_sample(out, y_buffer).sum()\n",
    "# #                     buffer_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "# #                     buffer_l0_expand = torch.repeat_interleave(buffer_l0_grads, 256, dim=1)\n",
    "# #                     buffer_l1_grads = buffer_l0_expand * emb.repeat(1, n_class)\n",
    "                    \n",
    "# #                     buffer_l0_grads = buffer_l0_grads.mean(dim=0).view(1, -1)\n",
    "# #                     buffer_l1_grads = buffer_l1_grads.mean(dim=0).view(1, -1)\n",
    "\n",
    "# #                 else:   \n",
    "# #                     out, emb = model(x_buffer)\n",
    "# #                     loss = loss_sample(out, y_buffer).sum()\n",
    "# #                     batch_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "# #                     batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 256, dim=1)\n",
    "# #                     batch_l1_grads = batch_l0_expand * emb.repeat(1, n_class)\n",
    "                    \n",
    "# #                     batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)\n",
    "# #                     batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)\n",
    "\n",
    "# #                     buffer_l0_grads = torch.cat((buffer_l0_grads, batch_l0_grads), dim=0)\n",
    "# #                     buffer_l1_grads = torch.cat((buffer_l1_grads, batch_l1_grads), dim=0)\n",
    "\n",
    "# #             buffer_grads = torch.cat((buffer_l0_grads, buffer_l1_grads), dim=1)\n",
    "\n",
    "# #             new_l0_grads = torch.empty((0, n_class), device=device, dtype=torch.float32)\n",
    "# #             new_l1_grads = torch.empty((0, n_class*256), device=device, dtype=torch.float32)\n",
    "\n",
    "# #             for n in range(2):\n",
    "# #                 x_new = all_train_data[i*2+n:i*2+n+1][0]\n",
    "# #                 y_new = all_train_label[i*2+n:i*2+n+1][0]\n",
    "# #                 x_new = torch.Tensor(x_new).to(device, dtype=torch.float32)\n",
    "# #                 y_new = torch.Tensor(y_new).to(device, dtype=torch.int64)\n",
    "\n",
    "# #                 out, emb = model(x_new)\n",
    "# #                 loss = loss_sample(out, y_new).sum()\n",
    "# #                 batch_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "# #                 batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 256, dim=1)\n",
    "# #                 batch_l1_grads = batch_l0_expand * emb.repeat(1, n_class)\n",
    "                \n",
    "# #                 batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)\n",
    "# #                 batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)\n",
    "\n",
    "# #                 new_l0_grads = torch.cat((new_l0_grads, batch_l0_grads), dim=0)\n",
    "# #                 new_l1_grads = torch.cat((new_l1_grads, batch_l1_grads), dim=0)\n",
    "\n",
    "# #             new_grads = torch.cat((new_l0_grads, new_l1_grads), dim=1)\n",
    "            \n",
    "# #             # gradient normalization\n",
    "# #             for n in range(i*2):\n",
    "# #                 buffer_grads[n*buffer_size:(n+1)*buffer_size] = f.normalize(buffer_grads[n*buffer_size:(n+1)*buffer_size], p=2, dim=1)\n",
    "            \n",
    "# #             for n in range(2):\n",
    "# #                 new_grads[n*4800:(n+1)*4800] = f.normalize(new_grads[n*4800:(n+1)*4800], p=2, dim=1)\n",
    "                \n",
    "# #             avg_buffer_grads = torch.mean(buffer_grads, dim=0)\n",
    "\n",
    "# #             # embedding computation\n",
    "# #             buffer_emb = []\n",
    "# #             new_emb = []\n",
    "\n",
    "# #             with torch.no_grad():\n",
    "# #                 for n in range(i*2):\n",
    "# #                     x_buffer = x_prev_buffer[n*buffer_size:(n+1)*buffer_size]\n",
    "# #                     x_buffer = torch.Tensor(x_buffer).to(device, dtype=torch.float32)\n",
    "# #                     _, emb = model(x_buffer)\n",
    "# #                     emb_avg = torch.mean(emb, dim=0)\n",
    "# #                     buffer_emb.append(emb_avg)\n",
    "\n",
    "# #             with torch.no_grad():\n",
    "# #                 for n in range(2):\n",
    "# #     #                 x_new = all_train_data[i*2+n:i*2+n+1][0]\n",
    "# #                     x_new = x_train_batch[n*curr_size:(n+1)*curr_size]\n",
    "# #     #                 print(len(x_new[0]))\n",
    "# #                     x_new = torch.Tensor(x_new).to(device, dtype=torch.float32)\n",
    "# #                     _, emb = model(x_new)\n",
    "# #                     emb_avg = torch.mean(emb, dim=0)\n",
    "# #                     new_emb.append(emb_avg)\n",
    "\n",
    "# #     #         print('buffer emb:', len(buffer_emb))\n",
    "# #     #         print('new emb:', len(new_emb))\n",
    "    \n",
    "# #             all_cos_li = []\n",
    "\n",
    "# #             for grad1 in new_grads:\n",
    "# # #                 dist_li = []\n",
    "# #                 cos_li = []\n",
    "# #                 for grad2 in buffer_grads:\n",
    "# #                     cos = torch.dot(grad1, grad2)\n",
    "# #                     cos_li.append(cos.cpu().detach())\n",
    "# # #                     dist = torch.linalg.norm(emb1 - emb2, ord=2)\n",
    "# # #                     dist_li.append(dist.cpu())\n",
    "# #     #             print(sorted(range(len(dist_li)), key=lambda k: dist_li[k]))\n",
    "# # #                 all_dist_li.append(dist_li)\n",
    "# #                 all_cos_li.append(cos_li)\n",
    "\n",
    "# #             print('all cos li:', all_cos_li)\n",
    "\n",
    "# #             all_dist_li = []\n",
    "\n",
    "# #             for emb1 in new_emb:\n",
    "# #                 dist_li = []\n",
    "# #                 for emb2 in buffer_emb:\n",
    "# #                     dist = torch.linalg.norm(emb1 - emb2, ord=2)\n",
    "# #                     dist_li.append(dist.cpu())\n",
    "# #     #             print(sorted(range(len(dist_li)), key=lambda k: dist_li[k]))\n",
    "# #                 all_dist_li.append(dist_li)\n",
    "\n",
    "# #             print('all dist li:', all_dist_li)\n",
    "\n",
    "# # #             x_train_batch = torch.cat([x_prev_buffer, x_train_batch])\n",
    "# # #             y_train_batch = torch.cat([y_prev_buffer, y_train_batch])\n",
    "\n",
    "#         x_train_batch = torch.Tensor(x_train_batch).to(device, dtype=torch.float32)\n",
    "#         y_train_batch = torch.Tensor(y_train_batch).to(device, dtype=torch.int64)\n",
    "        \n",
    "#         for c in range(i*2+2):\n",
    "#             print(c, y_train_batch.cpu().tolist().count(c))\n",
    "\n",
    "#     #     x_valid_batch = torch.cat(all_valid_data[i*2:i*2+2])\n",
    "#     #     y_valid_batch = torch.cat(all_valid_label[i*2:i*2+2])\n",
    "\n",
    "#         x_valid_batch = torch.cat(all_valid_data[:i*2+2])\n",
    "#         y_valid_batch = torch.cat(all_valid_label[:i*2+2])\n",
    "\n",
    "#         x_valid_batch = torch.Tensor(x_valid_batch).to(device, dtype=torch.float32)\n",
    "#         y_valid_batch = torch.Tensor(y_valid_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "#         train_ds = TensorDataset(x_train_batch, y_train_batch)\n",
    "#         train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#         valid_ds = TensorDataset(x_valid_batch, y_valid_batch)\n",
    "#         valid_loader = DataLoader(dataset=valid_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#         optimizer_config = {\"lr\": 0.001}\n",
    "        \n",
    "#         if i == 0:\n",
    "#             clf = NNClassifier(model, nn.CrossEntropyLoss(reduction='mean'), optim.Adam, optimizer_config)\n",
    "#             clf.fit({\"train\": train_loader, \"val\": valid_loader}, epochs=1, earlystop_path=f'./ckpt/joint.pt')\n",
    "#         elif i > 0:\n",
    "#             clf = NNClassifier_CL(model, nn.CrossEntropyLoss(reduction='mean'), optim.Adam, optimizer_config)\n",
    "#             clf.fit({\"train\": train_loader, \"val\": valid_loader}, epochs=1, earlystop_path=f'./ckpt/joint.pt', x_prev_buffer=x_prev_buffer, y_prev_buffer=y_prev_buffer, buffer_size=buffer_size, i=i, device=device, n_class=n_class)\n",
    "\n",
    "#         all_test_acc = []\n",
    "\n",
    "#         for j in range(n_class):\n",
    "\n",
    "#             if j < i*2+2:\n",
    "\n",
    "#                 x_test_batch = all_test_data[j]\n",
    "#                 y_test_batch = all_test_label[j]\n",
    "\n",
    "#                 x_test_batch = torch.Tensor(x_test_batch).to(device, dtype=torch.float32)\n",
    "#                 y_test_batch = torch.Tensor(y_test_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "#                 test_ds = TensorDataset(x_test_batch, y_test_batch)\n",
    "#                 test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#                 test_output, test_loss = clf.evaluate(test_loader)\n",
    "#                 test_acc = accuracy_score(test_output['true_y'], test_output['output'])\n",
    "\n",
    "#                 cf_li = list(confusion_matrix(test_output['true_y'], test_output['output'], labels=range(i*2+2))[j])\n",
    "\n",
    "#                 cf_matrix = {}\n",
    "#                 for k in range(len(cf_li)):\n",
    "#                     cf_matrix[label_li[k]] = cf_li[k]\n",
    "\n",
    "#                 print('test %d: acc= %.3f' %(j, test_acc), cf_matrix)\n",
    "                \n",
    "#                 method_acc.append(test_acc)\n",
    "\n",
    "#                 all_test_acc.append(test_acc)\n",
    "\n",
    "#                 seq_acc[j].append(test_acc)\n",
    "\n",
    "#             else:\n",
    "#                 seq_acc[j].append(0)\n",
    "\n",
    "#         method_bound.append(method_acc)  \n",
    "                \n",
    "#         print('overall: avg acc = %.3f' %(np.mean(all_test_acc)))  \n",
    "#         all_overall_acc.append(np.mean(all_test_acc))\n",
    "#         print('\\n')\n",
    "\n",
    "#     print('sequential: ', seq_acc)\n",
    "\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     for i in range(5):\n",
    "#         plt.subplot(2,3,i+1)\n",
    "#         plt.plot(seq_acc[i*2], label=str(i*2))\n",
    "#         plt.plot(seq_acc[i*2+1], label=str(i*2+1))\n",
    "#         plt.xticks(range(5))\n",
    "#         plt.legend(loc='best')\n",
    "#     plt.show()\n",
    "\n",
    "#     print('overall result: ', all_overall_acc)\n",
    "#     print('overall avg: ', np.mean(all_overall_acc))\n",
    "\n",
    "#     plt.plot(all_overall_acc)\n",
    "#     plt.xticks(range(5))\n",
    "#     plt.show()\n",
    "    \n",
    "#     upper_bound = all_upper_bound[s]\n",
    "    \n",
    "#     print('upper bound:', upper_bound)\n",
    "#     print('method bound:', method_bound, '\\n')\n",
    "    \n",
    "#     forget_result = []\n",
    "    \n",
    "#     for ind in range(len(upper_bound)):\n",
    "#         list1 = upper_bound[ind]\n",
    "#         list2 = method_bound[ind]\n",
    "#         result = list(map(lambda a, b: round(a - b, 3), list1, list2))\n",
    "#         forget_result.append(result)\n",
    "        \n",
    "#     print('forget result:', forget_result)\n",
    "    \n",
    "#     forget_summary = []\n",
    "    \n",
    "#     for ind in range(len(forget_result)):\n",
    "#         avg_std = []\n",
    "#         avg_std.append(round(np.mean(forget_result[ind]), 3))\n",
    "#         avg_std.append(round(np.std(forget_result[ind]), 3))\n",
    "#         forget_summary.append(avg_std)\n",
    "        \n",
    "#     print('forget summary:', forget_summary)\n",
    "#     print('metric1:', round(np.mean(np.array(forget_summary)[:,0]), 3), 'metric2:', round(np.mean(np.array(forget_summary)[:,1]), 3))\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c02a40",
   "metadata": {},
   "source": [
    "## grad curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6aba0c90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "# n_class = 10\n",
    "# buffer_size = 100\n",
    "# curr_size = 100\n",
    "# perBatch_buffer = True\n",
    "# perBatch_curr = False\n",
    "\n",
    "# if dataset in ['MNIST', 'FMNIST']:\n",
    "#     n_feature = 28*28\n",
    "# elif dataset in ['CIFAR10']:\n",
    "#     n_feature = 32*32*3\n",
    "    \n",
    "# for s in range(1):\n",
    "    \n",
    "#     method_bound = []\n",
    "\n",
    "#     model = NormalNN(input_features=n_feature, n_class=n_class, seed=s)\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     seq_acc = {k: [] for k in range(n_class)}\n",
    "#     all_overall_acc = []\n",
    "\n",
    "#     for i in range(5):\n",
    "        \n",
    "#         method_acc = []\n",
    "\n",
    "#         print('batch ind: ', i)\n",
    "# #         print('past: ', label_li[:i*2])\n",
    "\n",
    "# #         for p in range(len(label_li[:i*2])):\n",
    "# #             plt.figure(figsize=(8, 40))\n",
    "# #             for r in range(5):\n",
    "# #                 plt.subplot(1,5,r+1)\n",
    "# #                 if dataset in ['MNIST', 'FMNIST']:\n",
    "# #                     plt.imshow(torch.cat(all_train_data[p:p+1])[r], cmap=plt.get_cmap('gray'))\n",
    "# #                 else:\n",
    "# #                     plt.imshow(torch.cat(all_train_data[p:p+1])[r].permute(1, 2, 0), cmap=plt.get_cmap('gray'))  \n",
    "# #             plt.show()\n",
    "\n",
    "# #         print('curr: ', label_li[i*2:i*2+2])\n",
    "\n",
    "# #         plt.figure(figsize=(8, 40))\n",
    "# #         for r in range(5):\n",
    "# #             plt.subplot(1,5,r+1)\n",
    "# #             if dataset in ['MNIST', 'FMNIST']:\n",
    "# #                 plt.imshow(torch.cat(all_train_data[i*2:i*2+1])[r], cmap=plt.get_cmap('gray'))\n",
    "# #             else:\n",
    "# #                 plt.imshow(torch.cat(all_train_data[i*2:i*2+1])[r].permute(1, 2, 0), cmap=plt.get_cmap('gray'))  \n",
    "# #         plt.show()\n",
    "\n",
    "# #         plt.figure(figsize=(8, 40))\n",
    "# #         for r in range(5):\n",
    "# #             plt.subplot(1,5,r+1)\n",
    "# #             if dataset in ['MNIST', 'FMNIST']:\n",
    "# #                 plt.imshow(torch.cat(all_train_data[i*2+1:i*2+2])[r], cmap=plt.get_cmap('gray'))\n",
    "# #             else:\n",
    "# #                 plt.imshow(torch.cat(all_train_data[i*2+1:i*2+2])[r].permute(1, 2, 0), cmap=plt.get_cmap('gray'))  \n",
    "# #         plt.show()\n",
    "\n",
    "#         if i == 0:\n",
    "#             x_train_batch = torch.cat(all_train_data[i*2:i*2+2])\n",
    "#             y_train_batch = torch.cat(all_train_label[i*2:i*2+2])\n",
    "\n",
    "#         elif i > 0:\n",
    "#             x_train_batch = torch.cat(all_train_data[i*2:i*2+2])\n",
    "#             y_train_batch = torch.cat(all_train_label[i*2:i*2+2])\n",
    "#             if i == 1:\n",
    "#                 x_prev_buffer1 = torch.cat(all_train_data[i*2-2:i*2-1])\n",
    "#                 y_prev_buffer1 = torch.cat(all_train_label[i*2-2:i*2-1])\n",
    "#                 indices1 = torch.randperm(len(x_prev_buffer1))[:buffer_size]\n",
    "#                 x_prev_buffer1 = x_prev_buffer1[indices1]\n",
    "#                 y_prev_buffer1 = y_prev_buffer1[indices1]\n",
    "\n",
    "#                 x_prev_buffer2 = torch.cat(all_train_data[i*2-1:i*2])\n",
    "#                 y_prev_buffer2 = torch.cat(all_train_label[i*2-1:i*2])\n",
    "#                 indices2 = torch.randperm(len(x_prev_buffer2))[:buffer_size]\n",
    "#                 x_prev_buffer2 = x_prev_buffer2[indices2]\n",
    "#                 y_prev_buffer2 = y_prev_buffer2[indices2]\n",
    "\n",
    "#                 x_prev_buffer = torch.cat([x_prev_buffer1, x_prev_buffer2])\n",
    "#                 y_prev_buffer = torch.cat([y_prev_buffer1, y_prev_buffer2])\n",
    "\n",
    "#             else:\n",
    "#                 x_new_buffer1 = torch.cat(all_train_data[i*2-2:i*2-1])\n",
    "#                 y_new_buffer1 = torch.cat(all_train_label[i*2-2:i*2-1])\n",
    "#                 indices1 = torch.randperm(len(x_new_buffer1))[:buffer_size]\n",
    "# #                 indices1 = bot_indices1.cpu()\n",
    "#                 x_new_buffer1 = x_new_buffer1[indices1]\n",
    "#                 y_new_buffer1 = y_new_buffer1[indices1]\n",
    "\n",
    "#                 x_new_buffer2 = torch.cat(all_train_data[i*2-1:i*2])\n",
    "#                 y_new_buffer2 = torch.cat(all_train_label[i*2-1:i*2])\n",
    "#                 indices2 = torch.randperm(len(x_new_buffer2))[:buffer_size]\n",
    "# #                 indices2 = torch.subtract(bot_indices2, 4800).cpu()\n",
    "#                 x_new_buffer2 = x_new_buffer2[indices2]\n",
    "#                 y_new_buffer2 = y_new_buffer2[indices2]\n",
    "\n",
    "#                 x_new_buffer = torch.cat([x_new_buffer1, x_new_buffer2])\n",
    "#                 y_new_buffer = torch.cat([y_new_buffer1, y_new_buffer2])\n",
    "\n",
    "#                 x_prev_buffer = torch.cat([x_prev_buffer, x_new_buffer])\n",
    "#                 y_prev_buffer = torch.cat([y_prev_buffer, y_new_buffer])\n",
    "\n",
    "#             # gradient computation\n",
    "#             loss_sample = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "#             for n in range(i*2):\n",
    "#                 x_buffer = x_prev_buffer[n*buffer_size:(n+1)*buffer_size]\n",
    "#                 y_buffer = y_prev_buffer[n*buffer_size:(n+1)*buffer_size]\n",
    "#                 x_buffer = torch.Tensor(x_buffer).to(device, dtype=torch.float32)\n",
    "#                 y_buffer = torch.Tensor(y_buffer).to(device, dtype=torch.int64)\n",
    "\n",
    "#                 if n == 0:    \n",
    "#                     out, emb = model(x_buffer)\n",
    "#                     loss = loss_sample(out, y_buffer).sum()\n",
    "#                     buffer_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "#                     buffer_l0_expand = torch.repeat_interleave(buffer_l0_grads, 256, dim=1)\n",
    "#                     buffer_l1_grads = buffer_l0_expand * emb.repeat(1, n_class)\n",
    "#                     if perBatch_buffer:\n",
    "#                         buffer_l0_grads = buffer_l0_grads.mean(dim=0).view(1, -1)\n",
    "#                         buffer_l1_grads = buffer_l1_grads.mean(dim=0).view(1, -1)\n",
    "\n",
    "#                 else:   \n",
    "#                     out, emb = model(x_buffer)\n",
    "#                     loss = loss_sample(out, y_buffer).sum()\n",
    "#                     batch_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "#                     batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 256, dim=1)\n",
    "#                     batch_l1_grads = batch_l0_expand * emb.repeat(1, n_class)\n",
    "#                     if perBatch_buffer:\n",
    "#                         batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)\n",
    "#                         batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)\n",
    "\n",
    "#                     buffer_l0_grads = torch.cat((buffer_l0_grads, batch_l0_grads), dim=0)\n",
    "#                     buffer_l1_grads = torch.cat((buffer_l1_grads, batch_l1_grads), dim=0)\n",
    "\n",
    "#             buffer_grads = torch.cat((buffer_l0_grads, buffer_l1_grads), dim=1)\n",
    "# #             buffer_grads_norm = f.normalize(buffer_grads, p=2, dim=1)\n",
    "\n",
    "#             new_l0_grads = torch.empty((0, n_class), device=device, dtype=torch.float32)\n",
    "#             new_l1_grads = torch.empty((0, n_class*256), device=device, dtype=torch.float32)\n",
    "\n",
    "#             for n in range(2):\n",
    "#                 x_new = all_train_data[i*2+n:i*2+n+1][0]\n",
    "#                 y_new = all_train_label[i*2+n:i*2+n+1][0]\n",
    "#                 x_new = torch.Tensor(x_new).to(device, dtype=torch.float32)\n",
    "#                 y_new = torch.Tensor(y_new).to(device, dtype=torch.int64)\n",
    "\n",
    "#                 out, emb = model(x_new)\n",
    "#                 loss = loss_sample(out, y_new).sum()\n",
    "#                 batch_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "#                 batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 256, dim=1)\n",
    "#                 batch_l1_grads = batch_l0_expand * emb.repeat(1, n_class)\n",
    "#                 if perBatch_curr:\n",
    "#                     batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)\n",
    "#                     batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)\n",
    "\n",
    "#                 new_l0_grads = torch.cat((new_l0_grads, batch_l0_grads), dim=0)\n",
    "#                 new_l1_grads = torch.cat((new_l1_grads, batch_l1_grads), dim=0)\n",
    "\n",
    "#             new_grads = torch.cat((new_l0_grads, new_l1_grads), dim=1)\n",
    "# #             new_grads_norm = f.normalize(new_grads, p=2, dim=1)\n",
    "# #             new_grads_norm = new_grads\n",
    "\n",
    "            \n",
    "#             # gradient normalization\n",
    "#             for n in range(i*2):\n",
    "#                 buffer_grads[n*buffer_size:(n+1)*buffer_size] = f.normalize(buffer_grads[n*buffer_size:(n+1)*buffer_size], p=2, dim=1)\n",
    "            \n",
    "#             for n in range(2):\n",
    "#                 new_grads[n*4800:(n+1)*4800] = f.normalize(new_grads[n*4800:(n+1)*4800], p=2, dim=1)\n",
    "            \n",
    "\n",
    "#             # coreset\n",
    "#             with torch.no_grad():\n",
    "#                 new_grads_avg1 = new_grads[:4800].mean(dim=0).view(1, -1)\n",
    "#                 new_grads_avg1_flip = new_grads_avg1.transpose(0,1)\n",
    "#                 core_score1 = torch.matmul(new_grads[:4800], new_grads_avg1_flip).view(-1)\n",
    "                \n",
    "#                 new_grads_avg2 = new_grads[4800:4800*2].mean(dim=0).view(1, -1)\n",
    "#                 new_grads_avg2_flip = new_grads_avg2.transpose(0,1)\n",
    "#                 core_score2 = torch.matmul(new_grads[4800:4800*2], new_grads_avg2_flip).view(-1)\n",
    "                \n",
    "#             # embedding computation\n",
    "#             buffer_emb = []\n",
    "#             new_emb = []\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 for n in range(i*2):\n",
    "#                     x_buffer = x_prev_buffer[n*buffer_size:(n+1)*buffer_size]\n",
    "#                     x_buffer = torch.Tensor(x_buffer).to(device, dtype=torch.float32)\n",
    "#                     _, emb = model(x_buffer)\n",
    "#                     if perBatch_buffer:\n",
    "#                         emb_avg = torch.mean(emb, dim=0)\n",
    "#                         buffer_emb.append(emb_avg)\n",
    "#                     else:\n",
    "#                         buffer_emb.append(emb)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 for n in range(2):\n",
    "#                     x_new = all_train_data[i*2+n:i*2+n+1][0]\n",
    "#                     x_new = torch.Tensor(x_new).to(device, dtype=torch.float32)\n",
    "#                     _, emb = model(x_new)\n",
    "# #                     if perBatch_curr:\n",
    "#                     emb_avg = torch.mean(emb, dim=0)\n",
    "#                     new_emb.append(emb_avg)\n",
    "# #                     else:\n",
    "# #                         new_emb.extend(emb)\n",
    "\n",
    "#             print('buffer emb:', len(buffer_emb))\n",
    "#             print('new emb:', len(new_emb))\n",
    "\n",
    "#             all_dist_li = []\n",
    "            \n",
    "# #             for emb1 in buffer_emb:\n",
    "# #                 dist_li = []\n",
    "# #                 for emb2 in buffer_emb:\n",
    "# #                     dist = torch.linalg.norm(emb1 - emb2, ord=2)\n",
    "# #                     dist_li.append(dist.cpu())\n",
    "# #                 all_dist_li.append(dist_li)\n",
    "\n",
    "#             for emb1 in new_emb:\n",
    "#                 dist_li = []\n",
    "#                 for emb2 in buffer_emb:\n",
    "#                     dist = torch.linalg.norm(emb1 - emb2, ord=2)\n",
    "#                     dist_li.append(dist.cpu())\n",
    "#                 all_dist_li.append(dist_li)\n",
    "\n",
    "#             print('all dist li:', all_dist_li)\n",
    "# #             print('first:', all_dist_li[0])\n",
    "# #             print('second:', all_dist_li[1])\n",
    "            \n",
    "# #             print(sum(all_dist_li[0]))\n",
    "            \n",
    "#             weight1 = torch.div(torch.Tensor([1]*len(all_dist_li[0])), torch.Tensor(all_dist_li[0]))\n",
    "#             weight1 = torch.div(weight1, sum(weight1)).view(-1, 1).to(device)\n",
    "#             weight2 = torch.div(torch.Tensor([1]*len(all_dist_li[1])), torch.Tensor(all_dist_li[1]))\n",
    "#             weight2 = torch.div(weight2, sum(weight2)).view(-1, 1).to(device)\n",
    "            \n",
    "#             print('weight1:', weight1, weight1.size())\n",
    "#             print('weight2:', weight2, weight2.size())\n",
    "            \n",
    "#             # grad angle\n",
    "#             with torch.no_grad():\n",
    "#                 buffer_grads_flip = buffer_grads.transpose(0,1)\n",
    "#                 new_grads_1 = new_grads[:4800].mean(dim=0).view(1, -1)\n",
    "#                 cos_1 = torch.matmul(new_grads_1, buffer_grads_flip)\n",
    "#                 new_grads_2 = new_grads[4800:4800*2].mean(dim=0).view(1, -1)\n",
    "#                 cos_2 = torch.matmul(new_grads_2, buffer_grads_flip)\n",
    "#                 print('cos 1:', cos_1)\n",
    "#                 print('cos 2:', cos_2)\n",
    "\n",
    "#             # interference\n",
    "#             with torch.no_grad():\n",
    "#                 buffer_grads_flip = buffer_grads.transpose(0,1)\n",
    "#                 inner_p1 = torch.matmul(new_grads[:4800], buffer_grads_flip)\n",
    "                \n",
    "#                 print('inner_p1:', inner_p1.size())\n",
    "\n",
    "#                 # top k\n",
    "#                 inters1 = torch.sum(inner_p1, dim=1)\n",
    "#                 final_score1 = core_score1\n",
    "# #                 final_score1 = core_score1\n",
    "#                 top_values1, top_indices1 = final_score1.topk(buffer_size, largest=False)\n",
    "\n",
    "# #                 plt.figure(figsize=(50, 50))\n",
    "# #                 for t in range(len(top_indices1)):\n",
    "# #                     plt.subplot(10,10,t+1)\n",
    "# #                     if dataset in ['MNIST', 'FMNIST']:\n",
    "# #                         plt.imshow(x_train_batch[top_indices1[t]], cmap=plt.get_cmap('gray'))\n",
    "# #                     else:\n",
    "# #                         plt.imshow(x_train_batch[top_indices1[t]].permute(1, 2, 0), cmap=plt.get_cmap('gray'))  \n",
    "# #                 plt.show()\n",
    "\n",
    "#                 # bot k\n",
    "# #                 inters1 = torch.sum(inner_p1, dim=1)\n",
    "# #                 inters1 = torch.sum(torch.abs(inner_p1), dim=1)\n",
    "#                 inters1 = torch.matmul(inner_p1, weight1).view(-1)\n",
    "# #                 inters1 = torch.matmul(torch.abs(inner_p1), weight1).view(-1)\n",
    "# #                 final_score1 = -(1/(i*2))*core_score1 + inters1\n",
    "#                 final_score1 = inters1\n",
    "#                 bot_values1, bot_indices1 = final_score1.topk(buffer_size, largest=True)\n",
    "\n",
    "# #                 plt.figure(figsize=(50, 50))\n",
    "# #                 for t in range(len(bot_indices1)):\n",
    "# #                     plt.subplot(10,10,t+1)\n",
    "# #                     if dataset in ['MNIST', 'FMNIST']:\n",
    "# #                         plt.imshow(x_train_batch[bot_indices1[t]], cmap=plt.get_cmap('gray'))\n",
    "# #                     else:\n",
    "# #                         plt.imshow(x_train_batch[bot_indices1[t]].permute(1, 2, 0), cmap=plt.get_cmap('gray'))  \n",
    "# #                 plt.show()\n",
    "\n",
    "\n",
    "#                 inner_p2 = torch.matmul(new_grads[4800:4800*2], buffer_grads_flip)\n",
    "\n",
    "#                 # top k\n",
    "#                 inters2 = torch.sum(inner_p2, dim=1)\n",
    "#                 final_score2 = core_score2\n",
    "# #                 final_score2 = core_score2\n",
    "#                 top_values2, top_indices2 = final_score2.topk(buffer_size, largest=False)\n",
    "#                 top_indices2 = torch.add(top_indices2, 4800)\n",
    "\n",
    "# #                 plt.figure(figsize=(50, 50))\n",
    "# #                 for t in range(len(top_indices2)):\n",
    "# #                     plt.subplot(10,10,t+1)\n",
    "# #                     if dataset in ['MNIST', 'FMNIST']:\n",
    "# #                         plt.imshow(x_train_batch[top_indices2[t]], cmap=plt.get_cmap('gray'))\n",
    "# #                     else:\n",
    "# #                         plt.imshow(x_train_batch[top_indices2[t]].permute(1, 2, 0), cmap=plt.get_cmap('gray'))  \n",
    "# #                 plt.show()\n",
    "\n",
    "#                 # bot k\n",
    "# #                 inters2 = torch.sum(inner_p2, dim=1)\n",
    "# #                 inters2 = torch.sum(torch.abs(inner_p2), dim=1)\n",
    "#                 inters2 = torch.matmul(inner_p2, weight2).view(-1)\n",
    "# #                 inters2 = torch.matmul(torch.abs(inner_p2), weight2).view(-1)\n",
    "# #                 final_score2 = -(1/(i*2))*core_score2 + inters2\n",
    "#                 final_score2 = inters2\n",
    "#                 bot_values2, bot_indices2 = final_score2.topk(buffer_size, largest=True)\n",
    "#                 bot_indices2 = torch.add(bot_indices2, 4800)\n",
    "\n",
    "# #                 plt.figure(figsize=(50, 50))\n",
    "# #                 for t in range(len(bot_indices2)):\n",
    "# #                     plt.subplot(10,10,t+1)\n",
    "# #                     if dataset in ['MNIST', 'FMNIST']:\n",
    "# #                         plt.imshow(x_train_batch[bot_indices2[t]], cmap=plt.get_cmap('gray'))\n",
    "# #                     else:\n",
    "# #                         plt.imshow(x_train_batch[bot_indices2[t]].permute(1, 2, 0), cmap=plt.get_cmap('gray'))  \n",
    "# #                 plt.show()\n",
    "\n",
    "# #             print('top indices1:', top_indices1)\n",
    "# #             print('top indices2:', top_indices2)\n",
    "# #             print('len:', len(x_train_batch))\n",
    "\n",
    "# #             x_train_batch = torch.cat((x_train_batch[top_indices1.cpu()], x_train_batch[top_indices2.cpu()]))\n",
    "# #             y_train_batch = torch.cat((y_train_batch[top_indices1.cpu()], y_train_batch[top_indices2.cpu()]))\n",
    "\n",
    "# #             bot_indices1 = torch.randperm(len(x_train_batch[:4800]))[:curr_size]\n",
    "# #             bot_indices2 = torch.randperm(len(x_train_batch[4800:4800*2]))[:curr_size]\n",
    "# #             bot_indices2 = torch.add(bot_indices2, 4800)\n",
    "\n",
    "#             x_train_batch = torch.cat((x_train_batch[bot_indices1.cpu()], x_train_batch[bot_indices2.cpu()]))\n",
    "#             y_train_batch = torch.cat((y_train_batch[bot_indices1.cpu()], y_train_batch[bot_indices2.cpu()]))\n",
    "            \n",
    "# #             for c in range(i*2+2):\n",
    "# #                 print(c, y_train_batch.cpu().tolist().count(c))\n",
    "\n",
    "#             x_train_batch = torch.cat([x_prev_buffer, x_train_batch])\n",
    "#             y_train_batch = torch.cat([y_prev_buffer, y_train_batch])\n",
    "\n",
    "#         x_train_batch = torch.Tensor(x_train_batch).to(device, dtype=torch.float32)\n",
    "#         y_train_batch = torch.Tensor(y_train_batch).to(device, dtype=torch.int64)\n",
    "        \n",
    "#         for c in range(i*2+2):\n",
    "#             print(c, y_train_batch.cpu().tolist().count(c))\n",
    "\n",
    "#         print(x_train_batch.size())\n",
    "#         print(y_train_batch.size())\n",
    "\n",
    "#     #     x_valid_batch = torch.cat(all_valid_data[i*2:i*2+2])\n",
    "#     #     y_valid_batch = torch.cat(all_valid_label[i*2:i*2+2])\n",
    "\n",
    "#         x_valid_batch = torch.cat(all_valid_data[:i*2+2])\n",
    "#         y_valid_batch = torch.cat(all_valid_label[:i*2+2])\n",
    "\n",
    "#         x_valid_batch = torch.Tensor(x_valid_batch).to(device, dtype=torch.float32)\n",
    "#         y_valid_batch = torch.Tensor(y_valid_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "#         train_ds = TensorDataset(x_train_batch, y_train_batch)\n",
    "#         train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#         valid_ds = TensorDataset(x_valid_batch, y_valid_batch)\n",
    "#         valid_loader = DataLoader(dataset=valid_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#         optimizer_config = {\"lr\": 0.001}\n",
    "#         clf = NNClassifier(model, nn.CrossEntropyLoss(reduction='mean'), optim.Adam, optimizer_config)\n",
    "\n",
    "#         clf.fit({\"train\": train_loader, \"val\": valid_loader}, epochs=2000, earlystop_path=f'./ckpt/joint.pt')\n",
    "\n",
    "#         all_test_acc = []\n",
    "\n",
    "#         for j in range(n_class):\n",
    "\n",
    "#             if j < i*2+2:\n",
    "\n",
    "#                 x_test_batch = all_test_data[j]\n",
    "#                 y_test_batch = all_test_label[j]\n",
    "\n",
    "#                 x_test_batch = torch.Tensor(x_test_batch).to(device, dtype=torch.float32)\n",
    "#                 y_test_batch = torch.Tensor(y_test_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "#                 test_ds = TensorDataset(x_test_batch, y_test_batch)\n",
    "#                 test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#                 test_output, test_loss = clf.evaluate(test_loader)\n",
    "#                 test_acc = accuracy_score(test_output['true_y'], test_output['output'])\n",
    "\n",
    "#                 cf_li = list(confusion_matrix(test_output['true_y'], test_output['output'], labels=range(i*2+2))[j])\n",
    "\n",
    "#                 cf_matrix = {}\n",
    "#                 for k in range(len(cf_li)):\n",
    "#                     cf_matrix[label_li[k]] = cf_li[k]\n",
    "\n",
    "#                 print('test %d: acc= %.3f' %(j, test_acc), cf_matrix)\n",
    "                \n",
    "#                 method_acc.append(test_acc)\n",
    "\n",
    "#                 all_test_acc.append(test_acc)\n",
    "\n",
    "#                 seq_acc[j].append(test_acc)\n",
    "\n",
    "#             else:\n",
    "#                 seq_acc[j].append(0)\n",
    "                \n",
    "#         method_bound.append(method_acc) \n",
    "\n",
    "#         print('overall: avg acc = %.3f' %(np.mean(all_test_acc)))  \n",
    "#         all_overall_acc.append(np.mean(all_test_acc))\n",
    "#         print('\\n')\n",
    "\n",
    "#     print('sequential: ', seq_acc)\n",
    "\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     for i in range(5):\n",
    "#         plt.subplot(2,3,i+1)\n",
    "#         plt.plot(seq_acc[i*2], label=str(i*2))\n",
    "#         plt.plot(seq_acc[i*2+1], label=str(i*2+1))\n",
    "#         plt.xticks(range(5))\n",
    "#         plt.legend(loc='best')\n",
    "#     plt.show()\n",
    "\n",
    "#     print('overall result: ', all_overall_acc)\n",
    "#     print('overall avg: ', np.mean(all_overall_acc))\n",
    "\n",
    "#     plt.plot(all_overall_acc)\n",
    "#     plt.xticks(range(5))\n",
    "#     plt.show()\n",
    "    \n",
    "#     upper_bound = all_upper_bound[s]\n",
    "    \n",
    "#     print('upper bound:', upper_bound)\n",
    "#     print('method bound:', method_bound)\n",
    "    \n",
    "#     forget_result = []\n",
    "    \n",
    "#     for ind in range(len(upper_bound)):\n",
    "#         list1 = upper_bound[ind]\n",
    "#         list2 = method_bound[ind]\n",
    "#         result = list(map(lambda a, b: round(a - b, 3), list1, list2))\n",
    "#         forget_result.append(result)\n",
    "        \n",
    "#     print('forget result:', forget_result)\n",
    "    \n",
    "#     forget_summary = []\n",
    "    \n",
    "#     for ind in range(len(forget_result)):\n",
    "#         avg_std = []\n",
    "#         avg_std.append(round(np.mean(forget_result[ind]), 3))\n",
    "#         avg_std.append(round(np.std(forget_result[ind]), 3))\n",
    "#         forget_summary.append(avg_std)\n",
    "        \n",
    "#     print('forget summary:', forget_summary)\n",
    "#     print('metric1:', round(np.mean(np.array(forget_summary)[:,0]), 3), 'metric2:', round(np.mean(np.array(forget_summary)[:,1]), 3))\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9072f460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
