{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c3b92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir=./outputs/seed=1_epoch=1q__\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cl_gym as cl\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"/home/jaeyoung/nas/cl_gym\"))\n",
    "\n",
    "# dataset = 'MNIST'\n",
    "seed = 1\n",
    "dataset = 'FMNIST'\n",
    "# dataset = 'CIFAR10'\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "def make_params() -> dict:\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import uuid\n",
    "\n",
    "    params = {\n",
    "            # dataset\n",
    "            'dataset': \"FMNIST\",\n",
    "\n",
    "            # benchmark\n",
    "            'seed': seed,\n",
    "            'num_tasks': 5,\n",
    "            'epochs_per_task': 1,\n",
    "            'per_task_examples': np.inf,\n",
    "            'per_task_memory_examples': 64,\n",
    "            'batch_size_train': 64,\n",
    "            'batch_size_memory': 64,\n",
    "            'batch_size_validation': 256,\n",
    "            'lambda': 10,\n",
    "\n",
    "            # algorithm\n",
    "            'optimizer': 'SGD',\n",
    "            'learning_rate': 0.001,\n",
    "            'momentum': 0.8,\n",
    "            'learning_rate_decay': 1.0,\n",
    "            'criterion': torch.nn.CrossEntropyLoss(),\n",
    "            'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'), }\n",
    "\n",
    "#     trial_id = str(uuid.uuid4())\n",
    "    trial_id = f\"seed={params['seed']}_epoch={params['epochs_per_task']}q__\"\n",
    "    params['trial_id'] = trial_id\n",
    "    params['output_dir'] = os.path.join(\"./outputs/{}\".format(trial_id))\n",
    "    print(f\"output_dir={params['output_dir']}\")\n",
    "    Path(params['output_dir']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return params\n",
    "\n",
    "params = make_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b43ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "if params['dataset'] == 'MNIST':\n",
    "    benchmark = cl.benchmarks.SplitMNIST(num_tasks=params['num_tasks'],\n",
    "                                        per_task_memory_examples=params['per_task_memory_examples'])\n",
    "    label_li = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "elif params['dataset'] == 'FMNIST':\n",
    "    from datasets.FashionMNIST import FashionMNIST\n",
    "    benchmark = FashionMNIST(num_tasks=params['num_tasks'],\n",
    "                             per_task_memory_examples=params['per_task_memory_examples'],\n",
    "                             per_task_examples = min(params['per_task_examples'], 12000))\n",
    "\n",
    "    label_li = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', \n",
    "                  'Ankel boot']\n",
    "    # base_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    train_dataset = benchmark.fashion_mnist_train\n",
    "    test_dataset = benchmark.fashion_mnist_test\n",
    "    \n",
    "elif params['dataset'] == 'CIFAR10':\n",
    "    label_li = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    benchmark = cl.benchmarks.SplitCIFAR10(num_tasks=params['num_tasks'],\n",
    "                                        per_task_memory_examples=params['per_task_memory_examples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c7b886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Task 1 -----------------------\n",
      "[1] Eval metrics for task 1 >> {'accuracy': 95.55, 'loss': 0.0006121047511696815}\n",
      "training_task_end\n",
      "len(train_loader.dataset)=64\n",
      "---------------------------- Task 2 -----------------------\n",
      "new_bias_grads.shape=torch.Size([12000, 10])\n",
      "new_weight_grads.shape=torch.Size([12000, 2560])\n",
      "forget_matrix.shape=torch.Size([12000, 4])\n",
      "targets.shape=torch.Size([12000])\n",
      "num_dict={2: 0, 3: 0}\n",
      "best_ind=191\n",
      "len(select_curr_indexes)=192\n",
      "len(accumulate_select_indexes)=12000\n",
      "[2] Eval metrics for task 1 >> {'accuracy': 95.6, 'loss': 0.0006449163258075714}\n",
      "[2] Eval metrics for task 2 >> {'accuracy': 0.0, 'loss': 0.021197006940841676}\n",
      "training_task_end\n",
      "len(train_loader.dataset)=128\n",
      "---------------------------- Task 3 -----------------------\n",
      "new_bias_grads.shape=torch.Size([12000, 10])\n",
      "new_weight_grads.shape=torch.Size([12000, 2560])\n",
      "forget_matrix.shape=torch.Size([12000, 6])\n",
      "targets.shape=torch.Size([12000])\n",
      "num_dict={4: 0, 5: 0}\n",
      "best_ind=324\n",
      "len(select_curr_indexes)=325\n",
      "len(accumulate_select_indexes)=12000\n",
      "[3] Eval metrics for task 1 >> {'accuracy': 78.15, 'loss': 0.004177611410617828}\n",
      "[3] Eval metrics for task 2 >> {'accuracy': 84.95, 'loss': 0.005409203469753265}\n",
      "[3] Eval metrics for task 3 >> {'accuracy': 0.0, 'loss': 0.010336918592453002}\n",
      "training_task_end\n",
      "len(train_loader.dataset)=192\n",
      "---------------------------- Task 4 -----------------------\n",
      "new_bias_grads.shape=torch.Size([12000, 10])\n",
      "new_weight_grads.shape=torch.Size([12000, 2560])\n",
      "forget_matrix.shape=torch.Size([12000, 8])\n",
      "targets.shape=torch.Size([12000])\n",
      "num_dict={6: 0, 7: 0}\n",
      "best_ind=72\n",
      "len(select_curr_indexes)=73\n",
      "len(accumulate_select_indexes)=12000\n",
      "[4] Eval metrics for task 1 >> {'accuracy': 82.05, 'loss': 0.0038336873948574064}\n",
      "[4] Eval metrics for task 2 >> {'accuracy': 83.4, 'loss': 0.005459551930427551}\n",
      "[4] Eval metrics for task 3 >> {'accuracy': 0.0, 'loss': 0.009974338531494141}\n",
      "[4] Eval metrics for task 4 >> {'accuracy': 0.0, 'loss': 0.011051654100418092}\n",
      "training_task_end\n",
      "len(train_loader.dataset)=256\n",
      "---------------------------- Task 5 -----------------------\n",
      "new_bias_grads.shape=torch.Size([12000, 10])\n",
      "new_weight_grads.shape=torch.Size([12000, 2560])\n",
      "forget_matrix.shape=torch.Size([12000, 10])\n",
      "targets.shape=torch.Size([12000])\n",
      "num_dict={8: 0, 9: 0}\n",
      "best_ind=64\n",
      "len(select_curr_indexes)=65\n",
      "len(accumulate_select_indexes)=12000\n",
      "[5] Eval metrics for task 1 >> {'accuracy': 82.95, 'loss': 0.003742066413164139}\n",
      "[5] Eval metrics for task 2 >> {'accuracy': 83.65, 'loss': 0.005488032698631287}\n",
      "[5] Eval metrics for task 3 >> {'accuracy': 0.0, 'loss': 0.00973107635974884}\n",
      "[5] Eval metrics for task 4 >> {'accuracy': 0.0, 'loss': 0.010806589722633361}\n",
      "[5] Eval metrics for task 5 >> {'accuracy': 0.0, 'loss': 0.010126323819160461}\n",
      "training_task_end\n",
      "len(train_loader.dataset)=320\n",
      "final avg-acc 33.32000000000001\n",
      "final avg-forget 3.487499999999997\n"
     ]
    }
   ],
   "source": [
    "from model import NormalNN, NNClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from torchvision import models\n",
    "from torchinfo import summary\n",
    "# from resnet import ResNet18\n",
    "import random\n",
    "from model import NormalNN, NNClassifier_CL\n",
    "import copy\n",
    "\n",
    "\n",
    "from algorithms.mean_std_min import Heuristic2\n",
    "from metrics.fair_metric_manager import FairMetricCollector\n",
    "\n",
    "if dataset in ['MNIST', 'FMNIST']:\n",
    "    n_feature = 28*28\n",
    "elif dataset in ['CIFAR10']:\n",
    "    n_feature = 32*32*3\n",
    "    \n",
    "if dataset == 'MNIST':\n",
    "    params['alpha'] = 0.05\n",
    "    params['lambda'] = 50\n",
    "\n",
    "elif dataset == 'FMNIST':\n",
    "    params['alpha'] = 0.05\n",
    "    params['lambda'] = 10\n",
    "\n",
    "# alpha_li = [0.05]\n",
    "# lamb_li = [50]\n",
    "\n",
    "# alpha_li = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]\n",
    "# lamb_li = [0.01, 0.05, 0.1, 1, 10, 50, 100]\n",
    "\n",
    "backbone = cl.backbones.MLP2Layers(input_dim=784, hidden_dim_1=256, hidden_dim_2=256, output_dim=10)\n",
    "algorithm = Heuristic2(backbone, benchmark, params, requires_memory=True)\n",
    "metric_manager_callback = cl.utils.callbacks.MetricCollector(num_tasks=params['num_tasks'],\n",
    "                                                        eval_interval='epoch',\n",
    "                                                        epochs_per_task=params['epochs_per_task'])\n",
    "\n",
    "# from trainers.FairContinualTrainer import FairContinualTrainer\n",
    "# trainer = FairContinualTrainer(algorithm, params, callbacks=[metric_manager_callback])\n",
    "\n",
    "trainer = cl.trainer.ContinualTrainer(algorithm, params, callbacks=[metric_manager_callback])\n",
    "trainer.run()\n",
    "print(\"final avg-acc\", metric_manager_callback.meters['accuracy'].compute_final())\n",
    "print(\"final avg-forget\", metric_manager_callback.meters['forgetting'].compute_final())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "497fb778",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dataseta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataseta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dataseta'"
     ]
    }
   ],
   "source": [
    "params['dataseta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324660bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, Subset, ConcatDataset\n",
    "\n",
    "task = 1\n",
    "trainset = Subset(benchmark.trains[task], range(100))\n",
    "batch_size = 32\n",
    "shuffle=False\n",
    "num_workers = 0\n",
    "pin_memory = True\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size, shuffle, num_workers=num_workers,\n",
    "                                  pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bfa274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 1, 1, 1])\n",
      "tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
      "        0, 1, 1, 1, 0, 1, 0, 0])\n",
      "tensor([1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (inp, targ, t_id, *_) in enumerate(train_loader):\n",
    "    print(targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3387f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark.trains[task].targets[32:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d66be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(benchmark.trains[task].targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6eff29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = train_loader.dataset.dataset.targets\n",
    "targets.unique().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd7102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "num_dict = {x:0 for x in targets.unique().cpu().numpy()}\n",
    "for k in num_dict:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c6bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(num_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4187b211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0],\n",
       "        [    1],\n",
       "        [    2],\n",
       "        ...,\n",
       "        [11994],\n",
       "        [11995],\n",
       "        [11999]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(targets==0).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c61ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 1, 1, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda4b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(targets==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14742022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5880fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = params['device']\n",
    "\n",
    "for i in range(5):\n",
    "    torch.cuda.empty_cache()\n",
    "    if i == 0:\n",
    "        x_train_batch = torch.cat(all_train_data[i*2:i*2+2])\n",
    "        y_train_batch = torch.cat(all_train_label[i*2:i*2+2])\n",
    "\n",
    "    elif i > 0:\n",
    "        x_train_batch = torch.cat(all_train_data[i*2:i*2+2])\n",
    "        y_train_batch = torch.cat(all_train_label[i*2:i*2+2])\n",
    "        if i == 1:\n",
    "            x_prev_buffer1 = torch.cat(all_train_data[i*2-2:i*2-1])\n",
    "            y_prev_buffer1 = torch.cat(all_train_label[i*2-2:i*2-1])\n",
    "            indices1 = torch.randperm(len(x_prev_buffer1))[:buffer_size]\n",
    "            x_prev_buffer1 = x_prev_buffer1[indices1]\n",
    "            y_prev_buffer1 = y_prev_buffer1[indices1]\n",
    "\n",
    "            x_prev_buffer2 = torch.cat(all_train_data[i*2-1:i*2])\n",
    "            y_prev_buffer2 = torch.cat(all_train_label[i*2-1:i*2])\n",
    "            indices2 = torch.randperm(len(x_prev_buffer2))[:buffer_size]\n",
    "            x_prev_buffer2 = x_prev_buffer2[indices2]\n",
    "            y_prev_buffer2 = y_prev_buffer2[indices2]\n",
    "\n",
    "            x_prev_buffer = torch.cat([x_prev_buffer1, x_prev_buffer2])\n",
    "            y_prev_buffer = torch.cat([y_prev_buffer1, y_prev_buffer2])\n",
    "\n",
    "        else:\n",
    "\n",
    "            # select gradient-based herding buffer\n",
    "            x_new_buffer = torch.cat(all_train_data[i*2-2:i*2])\n",
    "            y_new_buffer = torch.cat(all_train_label[i*2-2:i*2])\n",
    "\n",
    "            x_new_buffer = x_new_buffer[select_buffer_indexes]\n",
    "            y_new_buffer = y_new_buffer[select_buffer_indexes]\n",
    "\n",
    "            x_prev_buffer = torch.cat([x_prev_buffer, x_new_buffer])\n",
    "            y_prev_buffer = torch.cat([y_prev_buffer, y_new_buffer])\n",
    "            \n",
    "#                     print('memory2:', torch.cuda.memory_allocated(device=device)/1024/1024/1024)\n",
    "\n",
    "        # buffer에 있는 class별 loss가 buffer_losses list에 들어감\n",
    "        buffer_losses = []\n",
    "        \n",
    "\n",
    "        # computation of mean gradients and losses for buffers \n",
    "        for n in range(i*2):\n",
    "            buffer_ind = [m for m in range(len(y_prev_buffer)) if y_prev_buffer[m] == n]\n",
    "            x_buffer = x_prev_buffer[buffer_ind]\n",
    "            y_buffer = y_prev_buffer[buffer_ind]\n",
    "            x_buffer = torch.Tensor(x_buffer).to(device, dtype=torch.float32)\n",
    "            y_buffer = torch.Tensor(y_buffer).to(device, dtype=torch.int64)\n",
    "\n",
    "            if n == 0:\n",
    "                model.zero_grad()\n",
    "\n",
    "                # mean gradient computation\n",
    "                out, emb = model(x_buffer)\n",
    "\n",
    "                init_out_buffer = out\n",
    "                init_emb_buffer = emb\n",
    "                init_y_buffer = y_buffer.view(-1, 1)\n",
    "\n",
    "\n",
    "                loss = loss_sample(out, y_buffer).sum()\n",
    "                buffer_l0_grads = torch.autograd.grad(loss, out)[0] #torch.Size([32, 10])\n",
    "                if dataset in ['MNIST', 'FMNIST']:\n",
    "                    buffer_l0_expand = torch.repeat_interleave(buffer_l0_grads, 256, dim=1)\n",
    "                else:\n",
    "                    buffer_l0_expand = torch.repeat_interleave(buffer_l0_grads, 512, dim=1)\n",
    "                buffer_l1_grads = buffer_l0_expand * emb.repeat(1, n_class)\n",
    "\n",
    "                buffer_l0_grads = buffer_l0_grads.mean(dim=0).view(1, -1)\n",
    "                buffer_l1_grads = buffer_l1_grads.mean(dim=0).view(1, -1)\n",
    "\n",
    "                # mean loss computation\n",
    "                buffer_ds = TensorDataset(x_buffer, y_buffer)\n",
    "                buffer_loader = DataLoader(dataset=buffer_ds, batch_size=batch_size, shuffle=True)\n",
    "                buffer_output, buffer_loss = clf.evaluate(buffer_loader)\n",
    "                buffer_losses.append(buffer_loss)\n",
    "\n",
    "            else:\n",
    "                model.zero_grad()\n",
    "\n",
    "                # mean gradient computation\n",
    "                out, emb = model(x_buffer)\n",
    "\n",
    "                init_out_buffer = torch.cat((init_out_buffer, out), dim=0)\n",
    "                init_emb_buffer = torch.cat((init_emb_buffer, emb), dim=0)\n",
    "                init_y_buffer = torch.cat((init_y_buffer, y_buffer.view(-1, 1)), dim=0)\n",
    "\n",
    "                loss = loss_sample(out, y_buffer).sum()\n",
    "                batch_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "                if dataset in ['MNIST', 'FMNIST']:\n",
    "                    batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 256, dim=1)\n",
    "                else:\n",
    "                    batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 512, dim=1)\n",
    "\n",
    "                batch_l1_grads = batch_l0_expand * emb.repeat(1, n_class)\n",
    "\n",
    "                batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)\n",
    "                batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)\n",
    "\n",
    "                buffer_l0_grads = torch.cat((buffer_l0_grads, batch_l0_grads), dim=0)\n",
    "                buffer_l1_grads = torch.cat((buffer_l1_grads, batch_l1_grads), dim=0)\n",
    "\n",
    "                # mean loss computation\n",
    "                buffer_ds = TensorDataset(x_buffer, y_buffer)\n",
    "                buffer_loader = DataLoader(dataset=buffer_ds, batch_size=batch_size, shuffle=True)\n",
    "                buffer_output, buffer_loss = clf.evaluate(buffer_loader)\n",
    "                buffer_losses.append(buffer_loss)\n",
    "                \n",
    "        \n",
    "        # initialize individual sample gradients of new data\n",
    "        new_l0_grads = torch.empty((0, n_class), device=device, dtype=torch.float32)\n",
    "        if dataset in ['MNIST', 'FMNIST']:\n",
    "            new_l1_grads = torch.empty((0, n_class*256), device=device, dtype=torch.float32)\n",
    "        else:\n",
    "            new_l1_grads = torch.empty((0, n_class*512), device=device, dtype=torch.float32)\n",
    "\n",
    "        # computation of mean and individual gradients and mean losses for new data \n",
    "        for n in range(2):\n",
    "            model.zero_grad()\n",
    "\n",
    "            # mean gradient computation\n",
    "            x_new_buffer = torch.cat(all_train_data[i*2+n:i*2+n+1])\n",
    "            y_new_buffer = torch.cat(all_train_label[i*2+n:i*2+n+1])\n",
    "            x_new_buffer = torch.Tensor(x_new_buffer).to(device, dtype=torch.float32)\n",
    "            y_new_buffer = torch.Tensor(y_new_buffer).to(device, dtype=torch.int64)\n",
    "            \n",
    "            new_buffer_ds = TensorDataset(x_new_buffer, y_new_buffer)\n",
    "            new_buffer_loader = DataLoader(dataset=new_buffer_ds, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            for batch_idx, batch_data in enumerate(new_buffer_loader):\n",
    "                model.zero_grad()\n",
    "                if batch_idx == 0:\n",
    "                    x_batch, y_batch = batch_data\n",
    "\n",
    "                    out, emb = model(x_batch)\n",
    "\n",
    "                    init_out_buffer = torch.cat((init_out_buffer, out), dim=0)\n",
    "                    init_emb_buffer = torch.cat((init_emb_buffer, emb), dim=0)\n",
    "                    init_y_buffer = torch.cat((init_y_buffer, y_batch.view(-1, 1)), dim=0)\n",
    "\n",
    "                    loss = loss_sample(out, y_batch).sum()\n",
    "                    batch_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        if dataset in ['MNIST', 'FMNIST']:\n",
    "                            batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 256, dim=1)\n",
    "                        else:\n",
    "                            batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 512, dim=1)\n",
    "                        batch_l1_grads = batch_l0_expand * emb.repeat(1, n_class)\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    x_batch, y_batch = batch_data\n",
    "\n",
    "                    out, emb = model(x_batch)\n",
    "\n",
    "                    init_out_buffer = torch.cat((init_out_buffer, out), dim=0)\n",
    "                    init_emb_buffer = torch.cat((init_emb_buffer, emb), dim=0)\n",
    "                    init_y_buffer = torch.cat((init_y_buffer, y_batch.view(-1, 1)), dim=0)\n",
    "\n",
    "                    loss = loss_sample(out, y_batch).sum()\n",
    "                    next_batch_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        if dataset in ['MNIST', 'FMNIST']:\n",
    "                            next_batch_l0_expand = torch.repeat_interleave(next_batch_l0_grads, 256, dim=1)\n",
    "                        else:\n",
    "                            next_batch_l0_expand = torch.repeat_interleave(next_batch_l0_grads, 512, dim=1)\n",
    "                        next_batch_l1_grads = next_batch_l0_expand * emb.repeat(1, n_class)\n",
    "\n",
    "                        batch_l0_grads = torch.cat((batch_l0_grads, next_batch_l0_grads), dim=0)\n",
    "                        batch_l1_grads = torch.cat((batch_l1_grads, next_batch_l1_grads), dim=0)\n",
    "\n",
    "            # individual gradients\n",
    "            ind_l0_grads = batch_l0_grads.clone()\n",
    "            ind_l1_grads = batch_l1_grads.clone()\n",
    "            \n",
    "            new_l0_grads = torch.cat((new_l0_grads, ind_l0_grads), dim=0)\n",
    "            new_l1_grads = torch.cat((new_l1_grads, ind_l1_grads), dim=0)\n",
    "                    \n",
    "            # mean gradients\n",
    "            batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)\n",
    "            batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)\n",
    "\n",
    "            buffer_l0_grads = torch.cat((buffer_l0_grads, batch_l0_grads), dim=0)\n",
    "            buffer_l1_grads = torch.cat((buffer_l1_grads, batch_l1_grads), dim=0)\n",
    "\n",
    "            # mean loss computation\n",
    "            buffer_ds = TensorDataset(x_new_buffer, y_new_buffer)\n",
    "            buffer_loader = DataLoader(dataset=buffer_ds, batch_size=batch_size, shuffle=True)\n",
    "            buffer_output, buffer_loss = clf.evaluate(buffer_loader)\n",
    "            buffer_losses.append(buffer_loss)\n",
    "            \n",
    "#                     print('memory4:', torch.cuda.memory_allocated(device=device)/1024/1024/1024)\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        buffer_grads = torch.cat((buffer_l0_grads, buffer_l1_grads), dim=1)\n",
    "        buffer_grads = f.normalize(buffer_grads, p=2, dim=1)\n",
    "\n",
    "        ######################################\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            new_grads = torch.cat((new_l0_grads, new_l1_grads), dim=1)\n",
    "\n",
    "            new_grads_origin = new_grads.clone()\n",
    "            new_grads = f.normalize(new_grads, p=2, dim=1)\n",
    "#                         print('new grads:', new_grads.shape)\n",
    "#                         print('new grads norm:', torch.norm(new_grads, dim=1))\n",
    "#                         print('new grads shape:', new_grads, new_grads.shape)\n",
    "\n",
    "            buffer_losses = torch.tensor(buffer_losses).view(1,-1)\n",
    "#             print('initial loss:', buffer_losses)\n",
    "#             print('initial mean, std:', buffer_losses.mean(dim=1).item(), buffer_losses.std(dim=1).item())\n",
    "\n",
    "            loss_matrix = buffer_losses.repeat(len(new_grads), 1).to(device)\n",
    "            loss_matrix_origin = loss_matrix.clone()\n",
    "            forget_matrix = torch.matmul(new_grads, torch.transpose(buffer_grads, 0, 1)).to(device)\n",
    "\n",
    "#                     print('init forget matrix shape:', forget_matrix.shape)\n",
    "\n",
    "#                     print('init memory:', torch.cuda.memory_allocated(device=device)/1024/1024/1024)\n",
    "\n",
    "        accumulate_select_indexes = []\n",
    "        accumulate_mean = []\n",
    "        accumulate_std = []\n",
    "        accumulate_sum = []\n",
    "\n",
    "        select_indexes = []\n",
    "        non_select_indexes = list(range(len(x_train_batch)))\n",
    "\n",
    "        num_class1 = 0\n",
    "        num_class2 = 0\n",
    "\n",
    "        # current data selection\n",
    "        for b in range(len(x_train_batch)):\n",
    "            torch.cuda.empty_cache()\n",
    "            loss_matrix = loss_matrix - alpha * forget_matrix\n",
    "            loss_mean = torch.mean(loss_matrix, dim=1, keepdim=True)\n",
    "            loss_std = torch.std(loss_matrix, dim=1, keepdim=True)\n",
    "\n",
    "            # select_ind = torch.argmin(loss_mean, dim=0)\n",
    "            # select_ind = torch.argmin(loss_std, dim=0)\n",
    "            select_ind = torch.argmin(loss_mean + loss_std, dim=0)\n",
    "\n",
    "            accumulate_mean.append(copy.deepcopy(loss_mean[select_ind].item()))\n",
    "            accumulate_std.append(copy.deepcopy(loss_std[select_ind].item()))\n",
    "            accumulate_sum.append(copy.deepcopy(loss_mean[select_ind].item() + loss_std[select_ind].item()))\n",
    "\n",
    "            if non_select_indexes[select_ind.item()] < len(x_train_batch)/2:\n",
    "                num_class1 += 1\n",
    "            else:\n",
    "                num_class2 += 1\n",
    "\n",
    "            # metrics인듯?\n",
    "            select_indexes.append(non_select_indexes[select_ind.item()])\n",
    "            accumulate_select_indexes.append(copy.deepcopy(select_indexes))\n",
    "            del non_select_indexes[select_ind.item()]\n",
    "\n",
    "            best_buffer_losses = loss_matrix[select_ind].view(1,-1)\n",
    "            loss_matrix = best_buffer_losses.repeat(len(new_grads)-1, 1).to(device)\n",
    "            new_grads = torch.cat((new_grads[:select_ind.item()], new_grads[select_ind.item()+1:]))\n",
    "            forget_matrix = torch.cat((forget_matrix[:select_ind.item()], forget_matrix[select_ind.item()+1:]))\n",
    "        best_ind = np.argmin(np.array(accumulate_sum))\n",
    "        select_curr_indexes = accumulate_select_indexes[best_ind]\n",
    "\n",
    "        # best_ind=11999\n",
    "        # len(select_curr_indexes)=12000\n",
    "        # len(accumulate_select_indexes)=12000\n",
    "\n",
    "\n",
    "        # best_ind=7337\n",
    "        # len(select_curr_indexes)=7338\n",
    "        # len(accumulate_select_indexes)=12000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # buffer data selection\n",
    "        select_buffer_indexes = []\n",
    "\n",
    "        class1_indexes = list(range(0, int(len(x_train_batch)/2)))\n",
    "        class2_indexes = list(range(int(len(x_train_batch)/2), len(x_train_batch)))\n",
    "\n",
    "        class1_grad_mean = new_grads_origin[class1_indexes].mean(dim=0).view(1, -1)\n",
    "        class2_grad_mean = new_grads_origin[class2_indexes].mean(dim=0).view(1, -1)\n",
    "\n",
    "        candidate_class1_indexes = list(range(0, int(len(x_train_batch)/2)))\n",
    "        class1_buffer_indexes = []\n",
    "        for m in range(buffer_size):\n",
    "            buffer_ind = torch.argmin(\n",
    "                torch.norm(class1_grad_mean.repeat(int(len(new_grads_origin)/2)-m, 1) \\\n",
    "                        - (torch.sum(new_grads_origin[class1_buffer_indexes], dim=0)\n",
    "                              .repeat(int(len(new_grads_origin)/2)-m,1) \\\n",
    "                        + new_grads_origin[candidate_class1_indexes])/(m+1), dim=1), dim=0)\n",
    "            class1_buffer_indexes.append(copy.deepcopy(candidate_class1_indexes[buffer_ind]))\n",
    "            del candidate_class1_indexes[buffer_ind]\n",
    "\n",
    "        candidate_class2_indexes = list(range(int(len(x_train_batch)/2), len(x_train_batch)))\n",
    "        class2_buffer_indexes = []\n",
    "        for m in range(buffer_size):\n",
    "            buffer_ind = torch.argmin(torch.norm(class2_grad_mean.repeat(int(len(new_grads_origin)/2)-m, 1) \\\n",
    "                            - (torch.sum(new_grads_origin[class2_buffer_indexes], dim=0)\n",
    "                               .repeat(int(len(new_grads_origin)/2)-m,1) \\\n",
    "                            + new_grads_origin[candidate_class2_indexes])/(m+1), dim=1), dim=0)\n",
    "            class2_buffer_indexes.append(copy.deepcopy(candidate_class2_indexes[buffer_ind]))\n",
    "            del candidate_class2_indexes[buffer_ind]\n",
    "\n",
    "        for ind in class1_buffer_indexes:\n",
    "            select_buffer_indexes.append(ind)\n",
    "\n",
    "        for ind in class2_buffer_indexes:\n",
    "            select_buffer_indexes.append(ind)\n",
    "\n",
    "        select_curr_indexes = list(set(select_curr_indexes))\n",
    "\n",
    "        # current data selection\n",
    "        x_train_batch = x_train_batch[select_curr_indexes]\n",
    "        y_train_batch = y_train_batch[select_curr_indexes]\n",
    "\n",
    "    x_train_batch = torch.Tensor(x_train_batch).to(device, dtype=torch.float32)\n",
    "    y_train_batch = torch.Tensor(y_train_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "    train_ds = TensorDataset(x_train_batch, y_train_batch)\n",
    "    train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer_config = {\"lr\": 0.001}\n",
    "    \n",
    "    model.zero_grad()\n",
    "\n",
    "    # Model training using current data and buffer data\n",
    "    if i == 0:\n",
    "        clf = NNClassifier(model, nn.CrossEntropyLoss(reduction='mean'), optim.Adam, optimizer_config)\n",
    "        clf.fit({\"train\": train_loader, \"val\": train_loader}, epochs=1, earlystop_path=f'./ckpt/joint.pt')\n",
    "    elif i > 0:\n",
    "#                     print('len prev buffer:', len(x_prev_buffer))\n",
    "        clf = NNClassifier_CL(model, nn.CrossEntropyLoss(reduction='mean'), optim.Adam, optimizer_config)\n",
    "        clf.fit({\"train\": train_loader, \"val\": train_loader, \"buffer\": (x_prev_buffer, y_prev_buffer)}, \n",
    "                epochs=1, sample_size=64, lamb=lamb, device=device, earlystop_path=f'./ckpt/joint.pt', seed=s)\n",
    "\n",
    "    # Model evaluation\n",
    "    all_test_acc = []\n",
    "    all_test_loss = []\n",
    "\n",
    "    # evaluation using sci-kit learn tool\n",
    "    for j in range(n_class):\n",
    "\n",
    "        if j < i*2+2:\n",
    "\n",
    "            x_test_batch = all_test_data[j]\n",
    "            y_test_batch = all_test_label[j]\n",
    "\n",
    "            x_test_batch = torch.Tensor(x_test_batch).to(device, dtype=torch.float32)\n",
    "            y_test_batch = torch.Tensor(y_test_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "            test_ds = TensorDataset(x_test_batch, y_test_batch)\n",
    "            test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            test_output, test_loss = clf.evaluate(test_loader)\n",
    "            test_acc = accuracy_score(test_output['true_y'], test_output['output'])\n",
    "\n",
    "            cf_li = list(confusion_matrix(test_output['true_y'], test_output['output'], labels=range(i*2+2))[j])\n",
    "\n",
    "            cf_matrix = {}\n",
    "            for k in range(len(cf_li)):\n",
    "                cf_matrix[label_li[k]] = cf_li[k]\n",
    "\n",
    "            all_test_acc.append(test_acc)\n",
    "            all_test_loss.append(test_loss)\n",
    "            seq_acc[j].append(test_acc)\n",
    "\n",
    "        else:\n",
    "            seq_acc[j].append(0)\n",
    "\n",
    "#                 print('overall: acc = %.3f, fair = %.3f' %(np.mean(all_test_acc), np.std(all_test_acc)))\n",
    "    overall_acc.append(np.mean(all_test_acc))\n",
    "    overall_fair.append(np.std(all_test_acc))\n",
    "\n",
    "\n",
    "all_overall_acc.append(np.mean(overall_acc))\n",
    "all_overall_fair.append(np.mean(overall_fair))\n",
    "\n",
    "alpha_dict[alpha].append([np.mean(all_overall_acc), np.mean(all_overall_fair)])\n",
    "print(alpha)\n",
    "\n",
    "print('alpha:', alpha, alpha_dict[alpha])\n",
    "print('avg:', np.mean([e[0] for e in alpha_dict[alpha]]), np.mean([e[1] for e in alpha_dict[alpha]]))\n",
    "print('\\n')\n",
    "\n",
    "print('-------------------------------------------------------------------------------------------------')\n",
    "        \n",
    "for k, v in alpha_dict.items():\n",
    "    print(k, v)\n",
    "    print('avg:', np.mean([e[0] for e in v]), np.mean([e[1] for e in v]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b0074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
