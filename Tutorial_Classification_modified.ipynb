{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c3b92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir=./outputs/dataset=MNIST/seed=1_epoch=1_lr=0.001_alpha=0.0_tau=0.0\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cl_gym as cl\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "seed = 1\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# torch.backends.cudnn.enabled = False\n",
    "\n",
    "def make_params() -> dict:\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import uuid\n",
    "\n",
    "    params = {\n",
    "            # dataset\n",
    "            'dataset': \"MNIST\",\n",
    "            # 'dataset': \"FMNIST\",\n",
    "\n",
    "            # benchmark\n",
    "            'seed': seed,\n",
    "            'num_tasks': 5,\n",
    "            'epochs_per_task': 1,\n",
    "            # 'per_task_examples': np.inf,\n",
    "            'per_task_examples': 10000,\n",
    "            'per_task_memory_examples': 20,\n",
    "            'batch_size_train': 64,\n",
    "            'batch_size_memory': 64,\n",
    "            'batch_size_validation': 256,\n",
    "            'tau': 10,\n",
    "            # 'tau': 0.0,\n",
    "\n",
    "            # algorithm\n",
    "            'optimizer': 'sgd',\n",
    "            'learning_rate': 0.001,\n",
    "            'momentum': 0.9,\n",
    "            'learning_rate_decay': 1.0,\n",
    "            'criterion': torch.nn.CrossEntropyLoss(),\n",
    "            'device': torch.device('cuda:7' if torch.cuda.is_available() else 'cpu'),\n",
    "             \n",
    "            # sample selection\n",
    "            'alpha':0.001\n",
    "              }\n",
    "\n",
    "#     trial_id = str(uuid.uuid4())\n",
    "    trial_id = f\"dataset={params['dataset']}/seed={params['seed']}_epoch={params['epochs_per_task']}_lr={params['learning_rate']}_alpha={params['alpha']}_tau={params['tau']}\"\n",
    "    params['trial_id'] = trial_id\n",
    "    params['output_dir'] = os.path.join(\"./outputs/{}\".format(trial_id))\n",
    "    print(f\"output_dir={params['output_dir']}\")\n",
    "    Path(params['output_dir']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return params\n",
    "\n",
    "params = make_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b43ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "from datasets import MNIST\n",
    "from datasets import FashionMNIST\n",
    "from datasets import CIFAR10, CIFAR100\n",
    "\n",
    "if params['dataset'] == 'MNIST':\n",
    "    benchmark = MNIST(num_tasks=params['num_tasks'],\n",
    "                      per_task_memory_examples=params['per_task_memory_examples'],\n",
    "                      per_task_examples = params['per_task_examples'],\n",
    "                      random_class_idx = False)\n",
    "    label_li = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    n_feature = 28*28\n",
    "\n",
    "elif params['dataset'] == 'FMNIST':\n",
    "    benchmark = FashionMNIST(num_tasks=params['num_tasks'],\n",
    "                             per_task_memory_examples=params['per_task_memory_examples'],\n",
    "                             per_task_examples = params['per_task_examples'],\n",
    "                             random_class_idx = False)\n",
    "\n",
    "    label_li = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', \n",
    "                  'Ankel boot']\n",
    "    n_feature = 28*28\n",
    "    \n",
    "elif params['dataset'] == 'CIFAR10':\n",
    "    label_li = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    benchmark = CIFAR10(num_tasks=params['num_tasks'],\n",
    "                        per_task_memory_examples=params['per_task_memory_examples'],\n",
    "                        per_task_examples = params['per_task_examples'],\n",
    "                        random_class_idx = False)\n",
    "    n_feature = 32*32*3\n",
    "\n",
    "elif params['dataset'] == 'CIFAR100':\n",
    "    label_li = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    benchmark = CIFAR100(num_tasks=params['num_tasks'],\n",
    "                        per_task_memory_examples=params['per_task_memory_examples'],\n",
    "                        per_task_examples = params['per_task_examples'],\n",
    "                        random_class_idx = False)\n",
    "    n_feature = 32*32*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c7b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms.imbalance import Heuristic2\n",
    "from metrics import MetricCollector2\n",
    "\n",
    "backbone = cl.backbones.MLP2Layers(input_dim=784, hidden_dim_1=256, hidden_dim_2=256, output_dim=10)\n",
    "algorithm = Heuristic2(backbone, benchmark, params, requires_memory=True)\n",
    "metric_manager_callback = MetricCollector2(num_tasks=params['num_tasks'],\n",
    "                                                        eval_interval='epoch',\n",
    "                                                        epochs_per_task=params['epochs_per_task'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c49d0d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "mem = copy.deepcopy(benchmark.trains)\n",
    "mem[1].dataset = copy.deepcopy(mem[1].dataset)\n",
    "original_shape = mem[1].dataset.data.shape\n",
    "mem[1].dataset.data = torch.zeros([0, *original_shape[1:]], dtype=torch.uint8)\n",
    "mem[1].dataset.targets = torch.zeros([0], dtype=int)\n",
    "mem[1].selected_indices = np.zeros([0], dtype=int)\n",
    "mem[1].targets = np.zeros([0], dtype=int)\n",
    "mem[1].sample_weight = torch.ones([0])\n",
    "    \n",
    "\n",
    "def add_data(self, X, y, weight=None):\n",
    "    if not X.shape[0] == y.shape[0]:\n",
    "        raise ValueError(f\"Wrong size: {X.shape=}, {y.shape=}\")\n",
    "    if weight is None:\n",
    "        weight = torch.ones_like(y)\n",
    "    else:\n",
    "        if not weight.shape == y.shape:\n",
    "            ValueError(f\"Wrong size: {X.shape=}, {y.shape=}, {weight.shape=}\") \n",
    "    original_dataset_len = len(self.dataset)\n",
    "    self.dataset.data = torch.cat([self.dataset.data, X], dim=0)\n",
    "    self.dataset.targets = torch.cat([self.dataset.targets, y], dim=0)\n",
    "    self.targets = np.concatenate([self.targets, y], axis=0)\n",
    "    append_idx = np.arange(original_dataset_len, original_dataset_len+len(y), dtype=int)\n",
    "    self.selected_indices = np.concatenate([self.selected_indices, append_idx], axis=0)\n",
    "    self.sample_weight = torch.cat([self.sample_weight, weight], dim=0)\n",
    "\n",
    "def replace_data(self, X, y, idx, weight=None):\n",
    "    if not X.shape[0] == y.shape[0]:\n",
    "        raise ValueError(f\"Wrong size: {X.shape=}, {y.shape=}\")\n",
    "    if weight is None:\n",
    "        weight = torch.ones_like(y, dtype=torch.float32)\n",
    "    else:\n",
    "        if not weight.shape == y.shape:\n",
    "            ValueError(f\"Wrong size: {X.shape=}, {y.shape=}, {weight.shape=}\") \n",
    "    self.dataset.data[idx] = X.clone()\n",
    "    self.dataset.targets[idx] = y.clone()\n",
    "    self.targets[idx] = y.clone().cpu().numpy()\n",
    "    self.sample_weight[idx] = weight.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c1bb4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Task 1 -----------------------\n",
      "solver=<function LS_solver at 0x7f6040385160>\n",
      "[1] Eval metrics for task 1 >> {'accuracy': 99.71478018520183, 'loss': 5.772249579394399e-05, 'std': 0.020902634181430013}\n",
      "training_task_end\n",
      "---------------------------- Task 2 -----------------------\n",
      "solver=<function LS_solver at 0x7f6040385160>\n",
      "[2] Eval metrics for task 1 >> {'accuracy': 0.0, 'loss': 0.02555385977382074, 'std': 0.0}\n",
      "[2] Eval metrics for task 2 >> {'accuracy': 96.04392125259038, 'loss': 0.0004548145408845205, 'std': 0.9857817177066519}\n",
      "training_task_end\n",
      "---------------------------- Task 3 -----------------------\n",
      "solver=<function LS_solver at 0x7f6040385160>\n",
      "[3] Eval metrics for task 1 >> {'accuracy': 0.0, 'loss': 0.03241517470519875, 'std': 0.0}\n",
      "[3] Eval metrics for task 2 >> {'accuracy': 0.0, 'loss': 0.031084835470948702, 'std': 0.0}\n",
      "[3] Eval metrics for task 3 >> {'accuracy': 97.41170668444559, 'loss': 0.00038423587189412805, 'std': 1.6717963705442374}\n",
      "training_task_end\n",
      "---------------------------- Task 4 -----------------------\n",
      "solver=<function LS_solver at 0x7f6040385160>\n",
      "[4] Eval metrics for task 1 >> {'accuracy': 0.0, 'loss': 0.02552629588061754, 'std': 0.0}\n",
      "[4] Eval metrics for task 2 >> {'accuracy': 0.0, 'loss': 0.028413314894060664, 'std': 0.0}\n",
      "[4] Eval metrics for task 3 >> {'accuracy': 0.0, 'loss': 0.02815173427984834, 'std': 0.0}\n",
      "[4] Eval metrics for task 4 >> {'accuracy': 99.04744401030032, 'loss': 0.0001654697025020317, 'std': 0.11748292080615008}\n",
      "training_task_end\n",
      "---------------------------- Task 5 -----------------------\n",
      "solver=<function LS_solver at 0x7f6040385160>\n",
      "[5] Eval metrics for task 1 >> {'accuracy': 0.0, 'loss': 0.028105325901761968, 'std': 0.0}\n",
      "[5] Eval metrics for task 2 >> {'accuracy': 0.0, 'loss': 0.03198723344681429, 'std': 0.0}\n",
      "[5] Eval metrics for task 3 >> {'accuracy': 0.0, 'loss': 0.03517372478415897, 'std': 0.0}\n",
      "[5] Eval metrics for task 4 >> {'accuracy': 0.0, 'loss': 0.028551641667837583, 'std': 0.0}\n",
      "[5] Eval metrics for task 5 >> {'accuracy': 96.21222142402159, 'loss': 0.0005232478880365269, 'std': 0.31899760471974514}\n",
      "training_task_end\n",
      "final avg-acc 19.242444284804318\n",
      "final avg-forget 98.05446303313454\n"
     ]
    }
   ],
   "source": [
    "from trainers import ContinualTrainer\n",
    "\n",
    "trainer = ContinualTrainer(algorithm, params, callbacks=[metric_manager_callback])\n",
    "trainer.run()\n",
    "print(\"final avg-acc\", metric_manager_callback.meters['accuracy'].compute_final())\n",
    "print(\"final avg-forget\", metric_manager_callback.meters['forgetting'].compute_final())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c076b062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.020902634181430013,\n",
       " 48.027019324448275,\n",
       " 45.93046182082289,\n",
       " 42.888841573207785,\n",
       " 38.48515298280048]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_manager_callback.meters['std'].get_std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "076dc8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.021, 0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.986, 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 1.672, 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.117, 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   , 0.319]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_manager_callback.meters['std'].get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7039ed8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[99.715,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   , 96.044,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   , 97.412,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   , 99.047,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   , 96.212]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_manager_callback.meters['accuracy'].get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "173c4765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.84232299873832"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_manager_callback.meters['accuracy'].compute_overall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "706ad34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:44.84232299873832\n",
      "fairness:35.070475667092175\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy:{np.mean(metric_manager_callback.meters['accuracy'].compute_overall())}\")\n",
    "print(f\"fairness:{np.mean(metric_manager_callback.meters['std'].compute_overall())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "354dc9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.070475667092175"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_manager_callback.meters['std'].compute_overall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b848e47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
