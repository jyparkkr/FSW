{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c3b92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir=./outputs/demo/dataset=MNIST/seed=10_epoch=5_lr=0.001_alpha=0.0_tau=10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cl_gym as cl\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "seed = 10\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.set_num_threads(8)\n",
    "\n",
    "\n",
    "def make_params() -> dict:\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import uuid\n",
    "\n",
    "    params = {\n",
    "            # dataset\n",
    "            # 'dataset': \"MNIST\",\n",
    "            'dataset': \"FashionMNIST\",\n",
    "            'random_class_idx': False,\n",
    "\n",
    "            # benchmark\n",
    "            'seed': seed,\n",
    "            'num_tasks': 5,\n",
    "            'epochs_per_task': 5,\n",
    "            'per_task_examples': np.inf,\n",
    "            # 'per_task_examples': 10000,\n",
    "            'per_task_memory_examples': 64,\n",
    "            'batch_size_train': 64,\n",
    "            'batch_size_memory': 64,\n",
    "            'batch_size_validation': 256,\n",
    "            'tau': 10,\n",
    "            # 'tau': 0.0,\n",
    "\n",
    "            # algorithm\n",
    "            'optimizer': 'sgd',\n",
    "            'learning_rate': 0.001,\n",
    "            'momentum': 0.9,\n",
    "            'learning_rate_decay': 1.0,\n",
    "            'criterion': torch.nn.CrossEntropyLoss(),\n",
    "            'device': torch.device('cuda:6' if torch.cuda.is_available() else 'cpu'),\n",
    "             \n",
    "            # sample selection\n",
    "            'alpha':0.00,\n",
    "            'lambda': .0,\n",
    "            'lambda_old': 0.0,\n",
    "              }\n",
    "\n",
    "#     trial_id = str(uuid.uuid4())\n",
    "    trial_id = f\"demo/dataset={params['dataset']}/seed={params['seed']}_epoch={params['epochs_per_task']}_lr={params['learning_rate']}_alpha={params['alpha']}_tau={params['tau']}\"\n",
    "    params['trial_id'] = trial_id\n",
    "    params['output_dir'] = os.path.join(\"./outputs/{}\".format(trial_id))\n",
    "    print(f\"output_dir={params['output_dir']}\")\n",
    "    Path(params['output_dir']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return params\n",
    "\n",
    "params = make_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b43ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "from datasets import MNIST\n",
    "from datasets import FashionMNIST\n",
    "from datasets import CIFAR10, CIFAR100\n",
    "\n",
    "if params['dataset'] == 'MNIST':\n",
    "    benchmark = MNIST(num_tasks=params['num_tasks'],\n",
    "                    per_task_memory_examples=params['per_task_memory_examples'],\n",
    "                    per_task_examples = params['per_task_examples'],\n",
    "                    random_class_idx = params['random_class_idx'])\n",
    "    input_dim = (28, 28)\n",
    "elif params['dataset'] == 'FashionMNIST':\n",
    "    benchmark = FashionMNIST(num_tasks=params['num_tasks'],\n",
    "                            per_task_memory_examples=params['per_task_memory_examples'],\n",
    "                            per_task_examples = params['per_task_examples'],\n",
    "                            random_class_idx = params['random_class_idx'])\n",
    "    input_dim = (28, 28)\n",
    "elif params['dataset'] == 'CIFAR10':\n",
    "    benchmark = CIFAR10(num_tasks=params['num_tasks'],\n",
    "                        per_task_memory_examples=params['per_task_memory_examples'],\n",
    "                        per_task_examples = params['per_task_examples'],\n",
    "                        random_class_idx = params['random_class_idx'])\n",
    "    input_dim = (3, 32, 32)\n",
    "elif params['dataset'] == 'CIFAR100':        \n",
    "    benchmark = CIFAR100(num_tasks=params['num_tasks'],\n",
    "                        per_task_memory_examples=params['per_task_memory_examples'],\n",
    "                        per_task_examples = params['per_task_examples'],\n",
    "                        random_class_idx = params['random_class_idx'])\n",
    "    input_dim = (3, 32, 32)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "class_idx = benchmark.class_idx\n",
    "num_classes = len(class_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c7b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms.imbalance import Heuristic2\n",
    "from metrics import MetricCollector2\n",
    "from backbones import MLP2Layers2\n",
    "\n",
    "backbone = MLP2Layers2(\n",
    "    input_dim=input_dim, \n",
    "    hidden_dim_1=256, \n",
    "    hidden_dim_2=256, \n",
    "    output_dim=num_classes,\n",
    "    class_idx=class_idx,\n",
    "    config=params\n",
    "    ).to(params['device'])\n",
    "algorithm = Heuristic2(backbone, benchmark, params, requires_memory=True)\n",
    "metric_manager_callback = MetricCollector2(num_tasks=params['num_tasks'],\n",
    "                                                        eval_interval='epoch',\n",
    "                                                        epochs_per_task=params['epochs_per_task'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8efa3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trainers import ContinualTrainer\n",
    "# from trainers.fair_trainer import FairContinualTrainer2 as ContinualTrainer\n",
    "from trainers.imbalance_trainer import ImbalanceContinualTrainer1 as ContinualTrainer\n",
    "\n",
    "trainer = ContinualTrainer(algorithm, params, callbacks=[metric_manager_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c1bb4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Task 1 -----------------------\n",
      "[1] Eval metrics for task 1 >> {'accuracy': 0.9984693877551021, 'loss': 6.54510017616529e-05, 'std': 0.0015306122448979775, 'EER': -1}\n",
      "[2] Eval metrics for task 1 >> {'accuracy': 0.9989795918367347, 'loss': 3.497156999283085e-05, 'std': 0.0010204081632653184, 'EER': -1}\n",
      "[3] Eval metrics for task 1 >> {'accuracy': 0.9989795918367347, 'loss': 2.3636513061592484e-05, 'std': 0.0010204081632653184, 'EER': -1}\n",
      "[4] Eval metrics for task 1 >> {'accuracy': 0.9989795918367347, 'loss': 1.8554245976039845e-05, 'std': 0.0010204081632653184, 'EER': -1}\n",
      "[5] Eval metrics for task 1 >> {'accuracy': 0.9989795918367347, 'loss': 1.5473695599581898e-05, 'std': 0.0010204081632653184, 'EER': -1}\n",
      "training_task_end\n",
      "---------------------------- Task 2 -----------------------\n",
      "[6] Eval metrics for task 1 >> {'accuracy': 0.9755349276274387, 'loss': 0.0003580598349971411, 'std': 0.011249213341724351, 'EER': -1}\n",
      "[6] Eval metrics for task 2 >> {'accuracy': 0.9501698134929772, 'loss': 0.0005726147585354169, 'std': 0.011216325120884163, 'EER': -1}\n",
      "[7] Eval metrics for task 1 >> {'accuracy': 0.9692506518025712, 'loss': 0.000562947770100677, 'std': 0.013128202822979385, 'EER': -1}\n",
      "[7] Eval metrics for task 2 >> {'accuracy': 0.965241960242536, 'loss': 0.0003792813757432663, 'std': 0.0010946734208304165, 'EER': -1}\n",
      "[8] Eval metrics for task 1 >> {'accuracy': 0.9604153555695406, 'loss': 0.0007359410713186782, 'std': 0.01755821271239777, 'EER': -1}\n",
      "[8] Eval metrics for task 2 >> {'accuracy': 0.9702663289584772, 'loss': 0.00033951400214556266, 'std': 0.0129019878732059, 'EER': -1}\n",
      "[9] Eval metrics for task 1 >> {'accuracy': 0.9560325451766609, 'loss': 0.0007534865228278699, 'std': 0.019297851299109936, 'EER': -1}\n",
      "[9] Eval metrics for task 2 >> {'accuracy': 0.9736050349220968, 'loss': 0.00028378160872900756, 'std': 0.004612786860081375, 'EER': -1}\n",
      "[10] Eval metrics for task 1 >> {'accuracy': 0.9520902634181425, 'loss': 0.000998452992726725, 'std': 0.02147801852018344, 'EER': -1}\n",
      "[10] Eval metrics for task 2 >> {'accuracy': 0.9745423670274004, 'loss': 0.00025986108917972834, 'std': 0.0007051577250748453, 'EER': -1}\n",
      "training_task_end\n",
      "---------------------------- Task 3 -----------------------\n",
      "[11] Eval metrics for task 1 >> {'accuracy': 0.9406814708262159, 'loss': 0.001022847606217044, 'std': 0.022314123887440418, 'EER': -1}\n",
      "[11] Eval metrics for task 2 >> {'accuracy': 0.8537426126333564, 'loss': 0.0019226543583435597, 'std': 0.015564318059712923, 'EER': -1}\n",
      "[11] Eval metrics for task 3 >> {'accuracy': 0.9596971952544912, 'loss': 0.0005716708157780077, 'std': 0.02808284547870643, 'EER': -1}\n",
      "[12] Eval metrics for task 1 >> {'accuracy': 0.9380382990200485, 'loss': 0.0010612669284180264, 'std': 0.019670952081273008, 'EER': -1}\n",
      "[12] Eval metrics for task 2 >> {'accuracy': 0.8188473789239389, 'loss': 0.002613736165141966, 'std': 0.003924898303783841, 'EER': -1}\n",
      "[12] Eval metrics for task 3 >> {'accuracy': 0.9736079018750057, 'loss': 0.0003030716044989314, 'std': 0.016208798735992236, 'EER': -1}\n",
      "[13] Eval metrics for task 1 >> {'accuracy': 0.93446687044862, 'loss': 0.0011926401027832752, 'std': 0.023242380652701566, 'EER': -1}\n",
      "[13] Eval metrics for task 2 >> {'accuracy': 0.7922730063704044, 'loss': 0.0032333304319278966, 'std': 0.008114590528820298, 'EER': -1}\n",
      "[13] Eval metrics for task 3 >> {'accuracy': 0.9801288666855416, 'loss': 0.00022135242787979964, 'std': 0.013761153681057237, 'EER': -1}\n",
      "[14] Eval metrics for task 1 >> {'accuracy': 0.9328665827564506, 'loss': 0.00118696736810337, 'std': 0.02572372561359343, 'EER': -1}\n",
      "[14] Eval metrics for task 2 >> {'accuracy': 0.7903139151124414, 'loss': 0.003431058566320421, 'std': 0.008135697290659272, 'EER': -1}\n",
      "[14] Eval metrics for task 3 >> {'accuracy': 0.9828801841213594, 'loss': 0.000179444358206037, 'std': 0.0120281661841396, 'EER': -1}\n",
      "[15] Eval metrics for task 1 >> {'accuracy': 0.9309651173244629, 'loss': 0.001274184125253213, 'std': 0.025863076508136296, 'EER': -1}\n",
      "[15] Eval metrics for task 2 >> {'accuracy': 0.7865856934530663, 'loss': 0.0036440327340311915, 'std': 0.009453910507329799, 'EER': -1}\n",
      "[15] Eval metrics for task 3 >> {'accuracy': 0.9880277734649704, 'loss': 0.00015047666186361456, 'std': 0.004843916962728234, 'EER': -1}\n",
      "training_task_end\n",
      "---------------------------- Task 4 -----------------------\n",
      "[16] Eval metrics for task 1 >> {'accuracy': 0.9358581317989751, 'loss': 0.0010346595030586206, 'std': 0.02361323383979147, 'EER': -1}\n",
      "[16] Eval metrics for task 2 >> {'accuracy': 0.8115060633970373, 'loss': 0.0032005162068608926, 'std': 0.004335520761378442, 'EER': -1}\n",
      "[16] Eval metrics for task 3 >> {'accuracy': 0.9260786077648799, 'loss': 0.0011840986292670096, 'std': 0.028096544984610894, 'EER': -1}\n",
      "[16] Eval metrics for task 4 >> {'accuracy': 0.9560855543731672, 'loss': 0.0005987708204522474, 'std': 0.0030584144984281414, 'EER': -1}\n",
      "[17] Eval metrics for task 1 >> {'accuracy': 0.9232671041985077, 'loss': 0.0012778072985633326, 'std': 0.021226287871976945, 'EER': -1}\n",
      "[17] Eval metrics for task 2 >> {'accuracy': 0.8136340087497121, 'loss': 0.0035480965479814342, 'std': 0.02002935758692148, 'EER': -1}\n",
      "[17] Eval metrics for task 3 >> {'accuracy': 0.9083982537696473, 'loss': 0.0014787845019342551, 'std': 0.04068524928534245, 'EER': -1}\n",
      "[17] Eval metrics for task 4 >> {'accuracy': 0.9707947816056473, 'loss': 0.00038253393095485634, 'std': 2.233901692078044e-05, 'EER': -1}\n",
      "[18] Eval metrics for task 1 >> {'accuracy': 0.9294120291288321, 'loss': 0.0012930097591228801, 'std': 0.021248763822709682, 'EER': -1}\n",
      "[18] Eval metrics for task 2 >> {'accuracy': 0.8126755698825696, 'loss': 0.0036805061063383514, 'std': 0.021977895463965003, 'EER': -1}\n",
      "[18] Eval metrics for task 3 >> {'accuracy': 0.8925068269204424, 'loss': 0.0016432623105598742, 'std': 0.03824673723434374, 'EER': -1}\n",
      "[18] Eval metrics for task 4 >> {'accuracy': 0.9758007522156242, 'loss': 0.0003320613701538017, 'std': 0.000852944282430157, 'EER': -1}\n",
      "[19] Eval metrics for task 1 >> {'accuracy': 0.9302234109502832, 'loss': 0.001291068268160448, 'std': 0.023080553807426074, 'EER': -1}\n",
      "[19] Eval metrics for task 2 >> {'accuracy': 0.7966238775040295, 'loss': 0.004140757527804398, 'std': 0.03208899378309921, 'EER': -1}\n",
      "[19] Eval metrics for task 3 >> {'accuracy': 0.8807868996191537, 'loss': 0.001868333401140561, 'std': 0.04894833459673226, 'EER': -1}\n",
      "[19] Eval metrics for task 4 >> {'accuracy': 0.9787901188435699, 'loss': 0.0002860426068050864, 'std': 0.001754628238142042, 'EER': -1}\n",
      "[20] Eval metrics for task 1 >> {'accuracy': 0.9285309718601097, 'loss': 0.0013851791408890529, 'std': 0.02036770655398723, 'EER': -1}\n",
      "[20] Eval metrics for task 2 >> {'accuracy': 0.7937274541407628, 'loss': 0.004329442890337235, 'std': 0.03597551615626676, 'EER': -1}\n",
      "[20] Eval metrics for task 3 >> {'accuracy': 0.8726870667531257, 'loss': 0.0021374871598491036, 'std': 0.05093818782935894, 'EER': -1}\n",
      "[20] Eval metrics for task 4 >> {'accuracy': 0.9808067228256012, 'loss': 0.00025350821447396447, 'std': 0.0016835495479395335, 'EER': -1}\n",
      "training_task_end\n",
      "---------------------------- Task 5 -----------------------\n",
      "[21] Eval metrics for task 1 >> {'accuracy': 0.9113953070214871, 'loss': 0.0015468831744318032, 'std': 0.015476939674548273, 'EER': -1}\n",
      "[21] Eval metrics for task 2 >> {'accuracy': 0.7476350832757694, 'loss': 0.004577546144208992, 'std': 0.014961815949036783, 'EER': -1}\n",
      "[21] Eval metrics for task 3 >> {'accuracy': 0.7993490451444385, 'loss': 0.003100128413010051, 'std': 0.02244321554802592, 'EER': -1}\n",
      "[21] Eval metrics for task 4 >> {'accuracy': 0.8875768665264048, 'loss': 0.0016203906390842235, 'std': 0.06753795602056817, 'EER': -1}\n",
      "[21] Eval metrics for task 5 >> {'accuracy': 0.9252355087579343, 'loss': 0.0009656816916338314, 'std': 0.00737103237189729, 'EER': -1}\n",
      "[22] Eval metrics for task 1 >> {'accuracy': 0.8982468758428481, 'loss': 0.001912313868813481, 'std': 0.020695855434684884, 'EER': -1}\n",
      "[22] Eval metrics for task 2 >> {'accuracy': 0.7437591142835214, 'loss': 0.00511709383956132, 'std': 0.011085846956788714, 'EER': -1}\n",
      "[22] Eval metrics for task 3 >> {'accuracy': 0.7795384179810582, 'loss': 0.0038634953402149766, 'std': 0.012722274483300322, 'EER': -1}\n",
      "[22] Eval metrics for task 4 >> {'accuracy': 0.8693918913430216, 'loss': 0.0021043467857806463, 'std': 0.06588994581772983, 'EER': -1}\n",
      "[22] Eval metrics for task 5 >> {'accuracy': 0.9531312642073495, 'loss': 0.0006283218068302249, 'std': 0.0016941978049708761, 'EER': -1}\n",
      "[23] Eval metrics for task 1 >> {'accuracy': 0.8991279331115707, 'loss': 0.0019225116528517811, 'std': 0.02157691270340739, 'EER': -1}\n",
      "[23] Eval metrics for task 2 >> {'accuracy': 0.7359438560135083, 'loss': 0.005617746935535248, 'std': 0.009211182746181601, 'EER': -1}\n",
      "[23] Eval metrics for task 3 >> {'accuracy': 0.776637547605783, 'loss': 0.003995466862060726, 'std': 0.006458175408473654, 'EER': -1}\n",
      "[23] Eval metrics for task 4 >> {'accuracy': 0.8535789135926826, 'loss': 0.0024109821605298215, 'std': 0.07439603421525065, 'EER': -1}\n",
      "[23] Eval metrics for task 5 >> {'accuracy': 0.954917549040158, 'loss': 0.0005504153349516191, 'std': 0.01138572152475764, 'EER': -1}\n",
      "[24] Eval metrics for task 1 >> {'accuracy': 0.8967162635979502, 'loss': 0.002033890082762878, 'std': 0.022226467679582806, 'EER': -1}\n",
      "[24] Eval metrics for task 2 >> {'accuracy': 0.7318990329265485, 'loss': 0.0058843299644350654, 'std': 0.021007943817637598, 'EER': -1}\n",
      "[24] Eval metrics for task 3 >> {'accuracy': 0.762880960426694, 'loss': 0.004529573937744825, 'std': 0.015123112893061674, 'EER': -1}\n",
      "[24] Eval metrics for task 4 >> {'accuracy': 0.8548248214909466, 'loss': 0.002523403873616475, 'std': 0.06688707830028512, 'EER': -1}\n",
      "[24] Eval metrics for task 5 >> {'accuracy': 0.9627602094496553, 'loss': 0.0004567516290414111, 'std': 0.004385581104759395, 'EER': -1}\n",
      "[25] Eval metrics for task 1 >> {'accuracy': 0.898617729029938, 'loss': 0.0019805439961435665, 'std': 0.02208711678504005, 'EER': -1}\n",
      "[25] Eval metrics for task 2 >> {'accuracy': 0.731888479545629, 'loss': 0.006019441434615506, 'std': 0.021987489446619102, 'EER': -1}\n",
      "[25] Eval metrics for task 3 >> {'accuracy': 0.7437336176741892, 'loss': 0.004714295156737402, 'std': 0.002702227539660096, 'EER': -1}\n",
      "[25] Eval metrics for task 4 >> {'accuracy': 0.848952706270359, 'loss': 0.002695151356171745, 'std': 0.07171535218475583, 'EER': -1}\n",
      "[25] Eval metrics for task 5 >> {'accuracy': 0.9645999149339721, 'loss': 0.0004167774381267261, 'std': 0.005667676740953642, 'EER': -1}\n",
      "training_task_end\n",
      "final avg-acc 0.8375584894908175\n",
      "final avg-forget 0.17979098065864788\n"
     ]
    }
   ],
   "source": [
    "trainer.run()\n",
    "print(\"final avg-acc\", metric_manager_callback.meters['accuracy'].compute_final())\n",
    "print(\"final avg-forget\", metric_manager_callback.meters['forgetting'].compute_final())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beaf952b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.999, 0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.952, 0.975, 0.   , 0.   , 0.   ],\n",
       "       [0.931, 0.787, 0.988, 0.   , 0.   ],\n",
       "       [0.929, 0.794, 0.873, 0.981, 0.   ],\n",
       "       [0.899, 0.732, 0.744, 0.849, 0.965]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_manager_callback.meters['accuracy'].get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26972f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9191303957052114"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(metric_manager_callback.meters['accuracy'].compute_overall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e950fb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.011, 0.077, 0.061, 0.08]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.round(x, 3) for x in metric_manager_callback.meters['EER'].compute_overall()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ebfb58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04572080420849783"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(metric_manager_callback.meters['EER'].compute_overall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06af72df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.001, 0.019, 0.086, 0.077, 0.096]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.round(x, 3) for x in metric_manager_callback.meters['std'].compute_overall()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7039ed8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.055794389121720245"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(metric_manager_callback.meters['std'].compute_overall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "706ad34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.9191303957052114\n",
      "EER:0.04572080420849783\n",
      "std:0.055794389121720245\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy:{np.mean(metric_manager_callback.meters['accuracy'].compute_overall())}\")\n",
    "print(f\"EER:{np.mean(metric_manager_callback.meters['EER'].compute_overall())}\")\n",
    "print(f\"std:{np.mean(metric_manager_callback.meters['std'].compute_overall())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b848e47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
