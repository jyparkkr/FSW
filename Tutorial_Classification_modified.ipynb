{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c3b92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir=./outputs/seed=1_epoch=1q__\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cl_gym as cl\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "# dataset = 'MNIST'\n",
    "seed = 1\n",
    "dataset = 'FMNIST'\n",
    "# dataset = 'CIFAR10'\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "def make_params() -> dict:\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import uuid\n",
    "\n",
    "    params = {\n",
    "            # dataset\n",
    "            'dataset': \"FMNIST\",\n",
    "\n",
    "            # benchmark\n",
    "            'seed': seed,\n",
    "            'num_tasks': 5,\n",
    "            'epochs_per_task': 1,\n",
    "            'per_task_examples': np.inf,\n",
    "            'per_task_memory_examples': 64,\n",
    "            'batch_size_train': 64,\n",
    "            'batch_size_memory': 64,\n",
    "            'batch_size_validation': 256,\n",
    "            'lambda': 10,\n",
    "\n",
    "            # algorithm\n",
    "            'optimizer': 'Adam',\n",
    "            'learning_rate': 0.001,\n",
    "            'momentum': 0.8,\n",
    "            'learning_rate_decay': 1.0,\n",
    "            'criterion': torch.nn.CrossEntropyLoss(),\n",
    "            'device': torch.device('cuda:6' if torch.cuda.is_available() else 'cpu'), }\n",
    "\n",
    "#     trial_id = str(uuid.uuid4())\n",
    "    trial_id = f\"seed={params['seed']}_epoch={params['epochs_per_task']}q__\"\n",
    "    params['trial_id'] = trial_id\n",
    "    params['output_dir'] = os.path.join(\"./outputs/{}\".format(trial_id))\n",
    "    print(f\"output_dir={params['output_dir']}\")\n",
    "    Path(params['output_dir']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return params\n",
    "\n",
    "params = make_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b43ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "if params['dataset'] == 'MNIST':\n",
    "    benchmark = cl.benchmarks.SplitMNIST(num_tasks=params['num_tasks'],\n",
    "                                        per_task_memory_examples=params['per_task_memory_examples'])\n",
    "    label_li = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "elif params['dataset'] == 'FMNIST':\n",
    "    from datasets.FashionMNIST import FashionMNIST\n",
    "    benchmark = FashionMNIST(num_tasks=params['num_tasks'],\n",
    "                             per_task_memory_examples=params['per_task_memory_examples'],\n",
    "                             per_task_examples = min(params['per_task_examples'], 12000))\n",
    "\n",
    "    label_li = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', \n",
    "                  'Ankel boot']\n",
    "    # base_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    train_dataset = benchmark.fashion_mnist_train\n",
    "    test_dataset = benchmark.fashion_mnist_test\n",
    "    \n",
    "elif params['dataset'] == 'CIFAR10':\n",
    "    label_li = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    benchmark = cl.benchmarks.SplitCIFAR10(num_tasks=params['num_tasks'],\n",
    "                                        per_task_memory_examples=params['per_task_memory_examples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c741cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c7b886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Task 1 -----------------------\n",
      "[1] Eval metrics for task 1 >> {'accuracy': 98.6, 'loss': 0.00015132083231583237}\n",
      "training_task_end\n",
      "load_memory_joint: len(train_loader.dataset)=64\n",
      "---------------------------- Task 2 -----------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# from trainers.FairContinualTrainer import FairContinualTrainer\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# trainer = FairContinualTrainer(algorithm, params, callbacks=[metric_manager_callback])\u001b[39;00m\n\u001b[1;32m     35\u001b[0m trainer \u001b[38;5;241m=\u001b[39m cl\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mContinualTrainer(algorithm, params, callbacks\u001b[38;5;241m=\u001b[39m[metric_manager_callback])\n\u001b[0;32m---> 36\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal avg-acc\u001b[39m\u001b[38;5;124m\"\u001b[39m, metric_manager_callback\u001b[38;5;241m.\u001b[39mmeters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcompute_final())\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal avg-forget\u001b[39m\u001b[38;5;124m\"\u001b[39m, metric_manager_callback\u001b[38;5;241m.\u001b[39mmeters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforgetting\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcompute_final())\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/cl_gym/trainer/base.py:105\u001b[0m, in \u001b[0;36mContinualTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_setup()\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# fit: main training loop\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# teardown\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_teardown()\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/cl_gym/trainer/base.py:93\u001b[0m, in \u001b[0;36mContinualTrainer._run_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_before_fit()\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_after_fit()\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/cl_gym/trainer/base.py:83\u001b[0m, in \u001b[0;36mContinualTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_before_training_task()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_algorithm_on_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_after_training_task()\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/cl_gym/trainer/base.py:34\u001b[0m, in \u001b[0;36mContinualTrainer.train_algorithm_on_task\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_algorithm_on_task\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m---> 34\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_train_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mprepare_optimizer(task)\n\u001b[1;32m     36\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mprepare_criterion(task)\n",
      "File \u001b[0;32m/mnt/shared/jaeyoung/FSSCIL/algorithms/imbalance.py:257\u001b[0m, in \u001b[0;36mHeuristic2.prepare_train_loader\u001b[0;34m(self, task_id)\u001b[0m\n\u001b[1;32m    254\u001b[0m     num_dict_list[k]\u001b[38;5;241m.\u001b[39mappend(num_dict[k])\n\u001b[1;32m    256\u001b[0m select_indexes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_select_indexes[select_ind\u001b[38;5;241m.\u001b[39mitem()])\n\u001b[0;32m--> 257\u001b[0m accumulate_select_indexes\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselect_indexes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# del self.benchmark.seq_indices_train[task_id][select_ind.item()]\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_select_indexes[select_ind\u001b[38;5;241m.\u001b[39mitem()]\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/copy.py:205\u001b[0m, in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    203\u001b[0m append \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mappend\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[0;32m--> 205\u001b[0m     append(\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/copy.py:128\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    124\u001b[0m     d[PyStringMap] \u001b[38;5;241m=\u001b[39m PyStringMap\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m d, t\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeepcopy\u001b[39m(x, memo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, _nil\u001b[38;5;241m=\u001b[39m[]):\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deep copy operation on arbitrary Python objects.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    See the module's __doc__ string for more info.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m memo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from torchvision import models\n",
    "from torchinfo import summary\n",
    "# from resnet import ResNet18\n",
    "import random\n",
    "import copy\n",
    "\n",
    "\n",
    "from algorithms.imbalance import Heuristic2\n",
    "from metrics.fair_metric_manager import FairMetricCollector\n",
    "\n",
    "if dataset in ['MNIST', 'FMNIST']:\n",
    "    n_feature = 28*28\n",
    "elif dataset in ['CIFAR10']:\n",
    "    n_feature = 32*32*3\n",
    "    \n",
    "params['alpha'] = 0.0501\n",
    "params['lambda'] = 10\n",
    "\n",
    "# alpha_li = [0.05]\n",
    "# lamb_li = [50]\n",
    "\n",
    "# alpha_li = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]\n",
    "# lamb_li = [0.01, 0.05, 0.1, 1, 10, 50, 100]\n",
    "\n",
    "backbone = cl.backbones.MLP2Layers(input_dim=784, hidden_dim_1=256, hidden_dim_2=256, output_dim=10)\n",
    "algorithm = Heuristic2(backbone, benchmark, params, requires_memory=True)\n",
    "metric_manager_callback = cl.utils.callbacks.MetricCollector(num_tasks=params['num_tasks'],\n",
    "                                                        eval_interval='epoch',\n",
    "                                                        epochs_per_task=params['epochs_per_task'])\n",
    "\n",
    "# from trainers.FairContinualTrainer import FairContinualTrainer\n",
    "# trainer = FairContinualTrainer(algorithm, params, callbacks=[metric_manager_callback])\n",
    "\n",
    "trainer = cl.trainer.ContinualTrainer(algorithm, params, callbacks=[metric_manager_callback])\n",
    "trainer.run()\n",
    "print(\"final avg-acc\", metric_manager_callback.meters['accuracy'].compute_final())\n",
    "print(\"final avg-forget\", metric_manager_callback.meters['forgetting'].compute_final())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8324a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_id = 3\n",
    "inc_num = 2\n",
    "benchmark.class_idx[(task_id-1)*(inc_num):task_id*inc_num]\n",
    "benchmark.class_idx[:(task_id-1)*(inc_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b9b185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b4f6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to literal (898532129.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    k=0, loss_.shape=torch.Size([1, 1])\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to literal\n"
     ]
    }
   ],
   "source": [
    "k=0, loss_.shape=torch.Size([1, 1])\n",
    "k=0, grads_.shape=torch.Size([1, 2570])\n",
    "k=1, loss_.shape=torch.Size([1, 1])\n",
    "k=1, grads_.shape=torch.Size([1, 2570])\n",
    "k=2, loss_.shape=torch.Size([1, 1])\n",
    "k=2, grads_.shape=torch.Size([1, 2570])\n",
    "k=3, loss_.shape=torch.Size([1, 1])\n",
    "k=3, grads_.shape=torch.Size([1, 2570])\n",
    "loss_matrix.shape=torch.Size([3783, 4])\n",
    "3 is missing\n",
    "k=0, loss_.shape=torch.Size([1, 1])\n",
    "k=0, grads_.shape=torch.Size([1, 2570])\n",
    "k=1, loss_.shape=torch.Size([1, 1])\n",
    "k=1, grads_.shape=torch.Size([1, 2570])\n",
    "k=2, loss_.shape=torch.Size([1, 1])\n",
    "k=2, grads_.shape=torch.Size([1, 2570])\n",
    "k=3, loss_.shape=torch.Size([1, 0])\n",
    "k=3, grads_.shape=torch.Size([1, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becbcc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "---------------------------- Task 1 -----------------------\n",
    "[1] Eval metrics for task 1 >> {'accuracy': 98.6, 'loss': 0.00015132083231583237}\n",
    "training_task_end\n",
    "load_memory_joint: len(train_loader.dataset)=64\n",
    "---------------------------- Task 2 -----------------------\n",
    "len(select_curr_indexes)=5075\n",
    "[2] Eval metrics for task 1 >> {'accuracy': 85.85, 'loss': 0.0034543287754058836}\n",
    "[2] Eval metrics for task 2 >> {'accuracy': 86.6, 'loss': 0.0027514055371284487}\n",
    "training_task_end\n",
    "load_memory_joint: len(train_loader.dataset)=128\n",
    "---------------------------- Task 3 -----------------------\n",
    "len(select_curr_indexes)=9259\n",
    "[3] Eval metrics for task 1 >> {'accuracy': 82.85, 'loss': 0.005165461122989655}\n",
    "[3] Eval metrics for task 2 >> {'accuracy': 59.0, 'loss': 0.007712359070777893}\n",
    "[3] Eval metrics for task 3 >> {'accuracy': 97.55, 'loss': 0.0003111732173711061}\n",
    "training_task_end\n",
    "load_memory_joint: len(train_loader.dataset)=192\n",
    "---------------------------- Task 4 -----------------------\n",
    "len(select_curr_indexes)=11205\n",
    "[4] Eval metrics for task 1 >> {'accuracy': 66.7, 'loss': 0.010747700095176697}\n",
    "[4] Eval metrics for task 2 >> {'accuracy': 48.0, 'loss': 0.012582610368728637}\n",
    "[4] Eval metrics for task 3 >> {'accuracy': 57.25, 'loss': 0.008546485185623169}\n",
    "[4] Eval metrics for task 4 >> {'accuracy': 92.6, 'loss': 0.0011509314700961113}\n",
    "training_task_end\n",
    "load_memory_joint: len(train_loader.dataset)=256\n",
    "---------------------------- Task 5 -----------------------\n",
    "len(select_curr_indexes)=7916\n",
    "[5] Eval metrics for task 1 >> {'accuracy': 74.3, 'loss': 0.007032525479793548}\n",
    "[5] Eval metrics for task 2 >> {'accuracy': 56.75, 'loss': 0.011757457256317138}\n",
    "[5] Eval metrics for task 3 >> {'accuracy': 63.9, 'loss': 0.00855764478445053}\n",
    "[5] Eval metrics for task 4 >> {'accuracy': 80.9, 'loss': 0.003836171865463257}\n",
    "[5] Eval metrics for task 5 >> {'accuracy': 92.7, 'loss': 0.0013385150507092475}\n",
    "training_task_end\n",
    "load_memory_joint: len(train_loader.dataset)=320\n",
    "final avg-acc 73.71000000000001\n",
    "final avg-forget 24.874999999999993\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1b35c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04a1586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33b583",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'taskd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtaskd\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'taskd' is not defined"
     ]
    }
   ],
   "source": [
    "taskd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d8346",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "delete() missing 1 required positional argument: 'obj'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: delete() missing 1 required positional argument: 'obj'"
     ]
    }
   ],
   "source": [
    "np.delete(np.random.randint(0, 10, size=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7aaa68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 7, 8, 4, 9, 0, 2, 1, 6, 5]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.choice(10, size=10, replace=False).tolist()\n",
    "sorted(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af81222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(5):\n",
    "    if len(classwise_loss[x]) == 0:\n",
    "        del classwise_loss[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b9c92c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324660bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, Subset, ConcatDataset\n",
    "\n",
    "task = 1\n",
    "trainset = Subset(benchmark.trains[task], range(100))\n",
    "batch_size = 32\n",
    "shuffle=False\n",
    "num_workers = 0\n",
    "pin_memory = True\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size, shuffle, num_workers=num_workers,\n",
    "                                  pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bfa274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 1, 1, 1])\n",
      "tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
      "        0, 1, 1, 1, 0, 1, 0, 0])\n",
      "tensor([1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (inp, targ, t_id, *_) in enumerate(train_loader):\n",
    "    print(targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3387f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark.trains[task].targets[32:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d66be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(benchmark.trains[task].targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6eff29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = train_loader.dataset.dataset.targets\n",
    "targets.unique().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd7102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "num_dict = {x:0 for x in targets.unique().cpu().numpy()}\n",
    "for k in num_dict:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c6bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(num_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4187b211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0],\n",
       "        [    1],\n",
       "        [    2],\n",
       "        ...,\n",
       "        [11994],\n",
       "        [11995],\n",
       "        [11999]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(targets==0).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c61ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 1, 1, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda4b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,     1,     2,  ..., 11994, 11995, 11999])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(targets==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14742022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5880fb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m----> 6\u001b[0m     x_train_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43mall_train_data\u001b[49m[i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m:i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m      7\u001b[0m     y_train_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(all_train_label[i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m:i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_train_data' is not defined"
     ]
    }
   ],
   "source": [
    "device = params['device']\n",
    "\n",
    "for i in range(5):\n",
    "    torch.cuda.empty_cache()\n",
    "    if i == 0:\n",
    "        x_train_batch = torch.cat(all_train_data[i*2:i*2+2])\n",
    "        y_train_batch = torch.cat(all_train_label[i*2:i*2+2])\n",
    "\n",
    "    elif i > 0:\n",
    "        x_train_batch = torch.cat(all_train_data[i*2:i*2+2])\n",
    "        y_train_batch = torch.cat(all_train_label[i*2:i*2+2])\n",
    "        if i == 1:\n",
    "            x_prev_buffer1 = torch.cat(all_train_data[i*2-2:i*2-1])\n",
    "            y_prev_buffer1 = torch.cat(all_train_label[i*2-2:i*2-1])\n",
    "            indices1 = torch.randperm(len(x_prev_buffer1))[:buffer_size]\n",
    "            x_prev_buffer1 = x_prev_buffer1[indices1]\n",
    "            y_prev_buffer1 = y_prev_buffer1[indices1]\n",
    "\n",
    "            x_prev_buffer2 = torch.cat(all_train_data[i*2-1:i*2])\n",
    "            y_prev_buffer2 = torch.cat(all_train_label[i*2-1:i*2])\n",
    "            indices2 = torch.randperm(len(x_prev_buffer2))[:buffer_size]\n",
    "            x_prev_buffer2 = x_prev_buffer2[indices2]\n",
    "            y_prev_buffer2 = y_prev_buffer2[indices2]\n",
    "\n",
    "            x_prev_buffer = torch.cat([x_prev_buffer1, x_prev_buffer2])\n",
    "            y_prev_buffer = torch.cat([y_prev_buffer1, y_prev_buffer2])\n",
    "\n",
    "        else:\n",
    "\n",
    "            # select gradient-based herding buffer\n",
    "            x_new_buffer = torch.cat(all_train_data[i*2-2:i*2])\n",
    "            y_new_buffer = torch.cat(all_train_label[i*2-2:i*2])\n",
    "\n",
    "            x_new_buffer = x_new_buffer[select_buffer_indexes]\n",
    "            y_new_buffer = y_new_buffer[select_buffer_indexes]\n",
    "\n",
    "            x_prev_buffer = torch.cat([x_prev_buffer, x_new_buffer])\n",
    "            y_prev_buffer = torch.cat([y_prev_buffer, y_new_buffer])\n",
    "            \n",
    "#                     print('memory2:', torch.cuda.memory_allocated(device=device)/1024/1024/1024)\n",
    "\n",
    "        # buffer에 있는 class별 loss가 buffer_losses list에 들어감\n",
    "        buffer_losses = []\n",
    "        \n",
    "\n",
    "        # computation of mean gradients and losses for buffers \n",
    "        for n in range(i*2):\n",
    "            buffer_ind = [m for m in range(len(y_prev_buffer)) if y_prev_buffer[m] == n]\n",
    "            x_buffer = x_prev_buffer[buffer_ind]\n",
    "            y_buffer = y_prev_buffer[buffer_ind]\n",
    "            x_buffer = torch.Tensor(x_buffer).to(device, dtype=torch.float32)\n",
    "            y_buffer = torch.Tensor(y_buffer).to(device, dtype=torch.int64)\n",
    "\n",
    "            if n == 0:\n",
    "                model.zero_grad()\n",
    "\n",
    "                # mean gradient computation\n",
    "                out, emb = model(x_buffer)\n",
    "\n",
    "                init_out_buffer = out\n",
    "                init_emb_buffer = emb\n",
    "                init_y_buffer = y_buffer.view(-1, 1)\n",
    "\n",
    "\n",
    "                loss = loss_sample(out, y_buffer).sum()\n",
    "                buffer_l0_grads = torch.autograd.grad(loss, out)[0] #torch.Size([32, 10])\n",
    "                if dataset in ['MNIST', 'FMNIST']:\n",
    "                    buffer_l0_expand = torch.repeat_interleave(buffer_l0_grads, 256, dim=1)\n",
    "                else:\n",
    "                    buffer_l0_expand = torch.repeat_interleave(buffer_l0_grads, 512, dim=1)\n",
    "                buffer_l1_grads = buffer_l0_expand * emb.repeat(1, n_class)\n",
    "\n",
    "                buffer_l0_grads = buffer_l0_grads.mean(dim=0).view(1, -1)\n",
    "                buffer_l1_grads = buffer_l1_grads.mean(dim=0).view(1, -1)\n",
    "\n",
    "                # mean loss computation\n",
    "                buffer_ds = TensorDataset(x_buffer, y_buffer)\n",
    "                buffer_loader = DataLoader(dataset=buffer_ds, batch_size=batch_size, shuffle=True)\n",
    "                buffer_output, buffer_loss = clf.evaluate(buffer_loader)\n",
    "                buffer_losses.append(buffer_loss)\n",
    "\n",
    "            else:\n",
    "                model.zero_grad()\n",
    "\n",
    "                # mean gradient computation\n",
    "                out, emb = model(x_buffer)\n",
    "\n",
    "                init_out_buffer = torch.cat((init_out_buffer, out), dim=0)\n",
    "                init_emb_buffer = torch.cat((init_emb_buffer, emb), dim=0)\n",
    "                init_y_buffer = torch.cat((init_y_buffer, y_buffer.view(-1, 1)), dim=0)\n",
    "\n",
    "                loss = loss_sample(out, y_buffer).sum()\n",
    "                batch_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "                if dataset in ['MNIST', 'FMNIST']:\n",
    "                    batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 256, dim=1)\n",
    "                else:\n",
    "                    batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 512, dim=1)\n",
    "\n",
    "                batch_l1_grads = batch_l0_expand * emb.repeat(1, n_class)\n",
    "\n",
    "                batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)\n",
    "                batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)\n",
    "\n",
    "                buffer_l0_grads = torch.cat((buffer_l0_grads, batch_l0_grads), dim=0)\n",
    "                buffer_l1_grads = torch.cat((buffer_l1_grads, batch_l1_grads), dim=0)\n",
    "\n",
    "                # mean loss computation\n",
    "                buffer_ds = TensorDataset(x_buffer, y_buffer)\n",
    "                buffer_loader = DataLoader(dataset=buffer_ds, batch_size=batch_size, shuffle=True)\n",
    "                buffer_output, buffer_loss = clf.evaluate(buffer_loader)\n",
    "                buffer_losses.append(buffer_loss)\n",
    "                \n",
    "        \n",
    "        # initialize individual sample gradients of new data\n",
    "        new_l0_grads = torch.empty((0, n_class), device=device, dtype=torch.float32)\n",
    "        if dataset in ['MNIST', 'FMNIST']:\n",
    "            new_l1_grads = torch.empty((0, n_class*256), device=device, dtype=torch.float32)\n",
    "        else:\n",
    "            new_l1_grads = torch.empty((0, n_class*512), device=device, dtype=torch.float32)\n",
    "\n",
    "        # computation of mean and individual gradients and mean losses for new data \n",
    "        for n in range(2):\n",
    "            model.zero_grad()\n",
    "\n",
    "            # mean gradient computation\n",
    "            x_new_buffer = torch.cat(all_train_data[i*2+n:i*2+n+1])\n",
    "            y_new_buffer = torch.cat(all_train_label[i*2+n:i*2+n+1])\n",
    "            x_new_buffer = torch.Tensor(x_new_buffer).to(device, dtype=torch.float32)\n",
    "            y_new_buffer = torch.Tensor(y_new_buffer).to(device, dtype=torch.int64)\n",
    "            \n",
    "            new_buffer_ds = TensorDataset(x_new_buffer, y_new_buffer)\n",
    "            new_buffer_loader = DataLoader(dataset=new_buffer_ds, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            for batch_idx, batch_data in enumerate(new_buffer_loader):\n",
    "                model.zero_grad()\n",
    "                if batch_idx == 0:\n",
    "                    x_batch, y_batch = batch_data\n",
    "\n",
    "                    out, emb = model(x_batch)\n",
    "\n",
    "                    init_out_buffer = torch.cat((init_out_buffer, out), dim=0)\n",
    "                    init_emb_buffer = torch.cat((init_emb_buffer, emb), dim=0)\n",
    "                    init_y_buffer = torch.cat((init_y_buffer, y_batch.view(-1, 1)), dim=0)\n",
    "\n",
    "                    loss = loss_sample(out, y_batch).sum()\n",
    "                    batch_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        if dataset in ['MNIST', 'FMNIST']:\n",
    "                            batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 256, dim=1)\n",
    "                        else:\n",
    "                            batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 512, dim=1)\n",
    "                        batch_l1_grads = batch_l0_expand * emb.repeat(1, n_class)\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    x_batch, y_batch = batch_data\n",
    "\n",
    "                    out, emb = model(x_batch)\n",
    "\n",
    "                    init_out_buffer = torch.cat((init_out_buffer, out), dim=0)\n",
    "                    init_emb_buffer = torch.cat((init_emb_buffer, emb), dim=0)\n",
    "                    init_y_buffer = torch.cat((init_y_buffer, y_batch.view(-1, 1)), dim=0)\n",
    "\n",
    "                    loss = loss_sample(out, y_batch).sum()\n",
    "                    next_batch_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        if dataset in ['MNIST', 'FMNIST']:\n",
    "                            next_batch_l0_expand = torch.repeat_interleave(next_batch_l0_grads, 256, dim=1)\n",
    "                        else:\n",
    "                            next_batch_l0_expand = torch.repeat_interleave(next_batch_l0_grads, 512, dim=1)\n",
    "                        next_batch_l1_grads = next_batch_l0_expand * emb.repeat(1, n_class)\n",
    "\n",
    "                        batch_l0_grads = torch.cat((batch_l0_grads, next_batch_l0_grads), dim=0)\n",
    "                        batch_l1_grads = torch.cat((batch_l1_grads, next_batch_l1_grads), dim=0)\n",
    "\n",
    "            # individual gradients\n",
    "            ind_l0_grads = batch_l0_grads.clone()\n",
    "            ind_l1_grads = batch_l1_grads.clone()\n",
    "            \n",
    "            new_l0_grads = torch.cat((new_l0_grads, ind_l0_grads), dim=0)\n",
    "            new_l1_grads = torch.cat((new_l1_grads, ind_l1_grads), dim=0)\n",
    "                    \n",
    "            # mean gradients\n",
    "            batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)\n",
    "            batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)\n",
    "\n",
    "            buffer_l0_grads = torch.cat((buffer_l0_grads, batch_l0_grads), dim=0)\n",
    "            buffer_l1_grads = torch.cat((buffer_l1_grads, batch_l1_grads), dim=0)\n",
    "\n",
    "            # mean loss computation\n",
    "            buffer_ds = TensorDataset(x_new_buffer, y_new_buffer)\n",
    "            buffer_loader = DataLoader(dataset=buffer_ds, batch_size=batch_size, shuffle=True)\n",
    "            buffer_output, buffer_loss = clf.evaluate(buffer_loader)\n",
    "            buffer_losses.append(buffer_loss)\n",
    "            \n",
    "#                     print('memory4:', torch.cuda.memory_allocated(device=device)/1024/1024/1024)\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        buffer_grads = torch.cat((buffer_l0_grads, buffer_l1_grads), dim=1)\n",
    "        buffer_grads = f.normalize(buffer_grads, p=2, dim=1)\n",
    "\n",
    "        ######################################\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            new_grads = torch.cat((new_l0_grads, new_l1_grads), dim=1)\n",
    "\n",
    "            new_grads_origin = new_grads.clone()\n",
    "            new_grads = f.normalize(new_grads, p=2, dim=1)\n",
    "#                         print('new grads:', new_grads.shape)\n",
    "#                         print('new grads norm:', torch.norm(new_grads, dim=1))\n",
    "#                         print('new grads shape:', new_grads, new_grads.shape)\n",
    "\n",
    "            buffer_losses = torch.tensor(buffer_losses).view(1,-1)\n",
    "#             print('initial loss:', buffer_losses)\n",
    "#             print('initial mean, std:', buffer_losses.mean(dim=1).item(), buffer_losses.std(dim=1).item())\n",
    "\n",
    "            loss_matrix = buffer_losses.repeat(len(new_grads), 1).to(device)\n",
    "            loss_matrix_origin = loss_matrix.clone()\n",
    "            forget_matrix = torch.matmul(new_grads, torch.transpose(buffer_grads, 0, 1)).to(device)\n",
    "\n",
    "#                     print('init forget matrix shape:', forget_matrix.shape)\n",
    "\n",
    "#                     print('init memory:', torch.cuda.memory_allocated(device=device)/1024/1024/1024)\n",
    "\n",
    "        accumulate_select_indexes = []\n",
    "        accumulate_mean = []\n",
    "        accumulate_std = []\n",
    "        accumulate_sum = []\n",
    "\n",
    "        select_indexes = []\n",
    "        non_select_indexes = list(range(len(x_train_batch)))\n",
    "\n",
    "        num_class1 = 0\n",
    "        num_class2 = 0\n",
    "\n",
    "        # current data selection\n",
    "        for b in range(len(x_train_batch)):\n",
    "            torch.cuda.empty_cache()\n",
    "            loss_matrix = loss_matrix - alpha * forget_matrix\n",
    "            loss_mean = torch.mean(loss_matrix, dim=1, keepdim=True)\n",
    "            loss_std = torch.std(loss_matrix, dim=1, keepdim=True)\n",
    "\n",
    "            # select_ind = torch.argmin(loss_mean, dim=0)\n",
    "            # select_ind = torch.argmin(loss_std, dim=0)\n",
    "            select_ind = torch.argmin(loss_mean + loss_std, dim=0)\n",
    "\n",
    "            accumulate_mean.append(copy.deepcopy(loss_mean[select_ind].item()))\n",
    "            accumulate_std.append(copy.deepcopy(loss_std[select_ind].item()))\n",
    "            accumulate_sum.append(copy.deepcopy(loss_mean[select_ind].item() + loss_std[select_ind].item()))\n",
    "\n",
    "            if non_select_indexes[select_ind.item()] < len(x_train_batch)/2:\n",
    "                num_class1 += 1\n",
    "            else:\n",
    "                num_class2 += 1\n",
    "\n",
    "            # metrics인듯?\n",
    "            select_indexes.append(non_select_indexes[select_ind.item()])\n",
    "            accumulate_select_indexes.append(copy.deepcopy(select_indexes))\n",
    "            del non_select_indexes[select_ind.item()]\n",
    "\n",
    "            best_buffer_losses = loss_matrix[select_ind].view(1,-1)\n",
    "            loss_matrix = best_buffer_losses.repeat(len(new_grads)-1, 1).to(device)\n",
    "            new_grads = torch.cat((new_grads[:select_ind.item()], new_grads[select_ind.item()+1:]))\n",
    "            forget_matrix = torch.cat((forget_matrix[:select_ind.item()], forget_matrix[select_ind.item()+1:]))\n",
    "        best_ind = np.argmin(np.array(accumulate_sum))\n",
    "        select_curr_indexes = accumulate_select_indexes[best_ind]\n",
    "\n",
    "        # best_ind=11999\n",
    "        # len(select_curr_indexes)=12000\n",
    "        # len(accumulate_select_indexes)=12000\n",
    "\n",
    "\n",
    "        # best_ind=7337\n",
    "        # len(select_curr_indexes)=7338\n",
    "        # len(accumulate_select_indexes)=12000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # buffer data selection\n",
    "        select_buffer_indexes = []\n",
    "\n",
    "        class1_indexes = list(range(0, int(len(x_train_batch)/2)))\n",
    "        class2_indexes = list(range(int(len(x_train_batch)/2), len(x_train_batch)))\n",
    "\n",
    "        class1_grad_mean = new_grads_origin[class1_indexes].mean(dim=0).view(1, -1)\n",
    "        class2_grad_mean = new_grads_origin[class2_indexes].mean(dim=0).view(1, -1)\n",
    "\n",
    "        candidate_class1_indexes = list(range(0, int(len(x_train_batch)/2)))\n",
    "        class1_buffer_indexes = []\n",
    "        for m in range(buffer_size):\n",
    "            buffer_ind = torch.argmin(\n",
    "                torch.norm(class1_grad_mean.repeat(int(len(new_grads_origin)/2)-m, 1) \\\n",
    "                        - (torch.sum(new_grads_origin[class1_buffer_indexes], dim=0)\n",
    "                              .repeat(int(len(new_grads_origin)/2)-m,1) \\\n",
    "                        + new_grads_origin[candidate_class1_indexes])/(m+1), dim=1), dim=0)\n",
    "            class1_buffer_indexes.append(copy.deepcopy(candidate_class1_indexes[buffer_ind]))\n",
    "            del candidate_class1_indexes[buffer_ind]\n",
    "\n",
    "        candidate_class2_indexes = list(range(int(len(x_train_batch)/2), len(x_train_batch)))\n",
    "        class2_buffer_indexes = []\n",
    "        for m in range(buffer_size):\n",
    "            buffer_ind = torch.argmin(torch.norm(class2_grad_mean.repeat(int(len(new_grads_origin)/2)-m, 1) \\\n",
    "                            - (torch.sum(new_grads_origin[class2_buffer_indexes], dim=0)\n",
    "                               .repeat(int(len(new_grads_origin)/2)-m,1) \\\n",
    "                            + new_grads_origin[candidate_class2_indexes])/(m+1), dim=1), dim=0)\n",
    "            class2_buffer_indexes.append(copy.deepcopy(candidate_class2_indexes[buffer_ind]))\n",
    "            del candidate_class2_indexes[buffer_ind]\n",
    "\n",
    "        for ind in class1_buffer_indexes:\n",
    "            select_buffer_indexes.append(ind)\n",
    "\n",
    "        for ind in class2_buffer_indexes:\n",
    "            select_buffer_indexes.append(ind)\n",
    "\n",
    "        select_curr_indexes = list(set(select_curr_indexes))\n",
    "\n",
    "        # current data selection\n",
    "        x_train_batch = x_train_batch[select_curr_indexes]\n",
    "        y_train_batch = y_train_batch[select_curr_indexes]\n",
    "\n",
    "    x_train_batch = torch.Tensor(x_train_batch).to(device, dtype=torch.float32)\n",
    "    y_train_batch = torch.Tensor(y_train_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "    train_ds = TensorDataset(x_train_batch, y_train_batch)\n",
    "    train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer_config = {\"lr\": 0.001}\n",
    "    \n",
    "    model.zero_grad()\n",
    "\n",
    "    # Model training using current data and buffer data\n",
    "    if i == 0:\n",
    "        clf = NNClassifier(model, nn.CrossEntropyLoss(reduction='mean'), optim.Adam, optimizer_config)\n",
    "        clf.fit({\"train\": train_loader, \"val\": train_loader}, epochs=1, earlystop_path=f'./ckpt/joint.pt')\n",
    "    elif i > 0:\n",
    "#                     print('len prev buffer:', len(x_prev_buffer))\n",
    "        clf = NNClassifier_CL(model, nn.CrossEntropyLoss(reduction='mean'), optim.Adam, optimizer_config)\n",
    "        clf.fit({\"train\": train_loader, \"val\": train_loader, \"buffer\": (x_prev_buffer, y_prev_buffer)}, \n",
    "                epochs=1, sample_size=64, lamb=lamb, device=device, earlystop_path=f'./ckpt/joint.pt', seed=s)\n",
    "\n",
    "    # Model evaluation\n",
    "    all_test_acc = []\n",
    "    all_test_loss = []\n",
    "\n",
    "    # evaluation using sci-kit learn tool\n",
    "    for j in range(n_class):\n",
    "\n",
    "        if j < i*2+2:\n",
    "\n",
    "            x_test_batch = all_test_data[j]\n",
    "            y_test_batch = all_test_label[j]\n",
    "\n",
    "            x_test_batch = torch.Tensor(x_test_batch).to(device, dtype=torch.float32)\n",
    "            y_test_batch = torch.Tensor(y_test_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "            test_ds = TensorDataset(x_test_batch, y_test_batch)\n",
    "            test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            test_output, test_loss = clf.evaluate(test_loader)\n",
    "            test_acc = accuracy_score(test_output['true_y'], test_output['output'])\n",
    "\n",
    "            cf_li = list(confusion_matrix(test_output['true_y'], test_output['output'], labels=range(i*2+2))[j])\n",
    "\n",
    "            cf_matrix = {}\n",
    "            for k in range(len(cf_li)):\n",
    "                cf_matrix[label_li[k]] = cf_li[k]\n",
    "\n",
    "            all_test_acc.append(test_acc)\n",
    "            all_test_loss.append(test_loss)\n",
    "            seq_acc[j].append(test_acc)\n",
    "\n",
    "        else:\n",
    "            seq_acc[j].append(0)\n",
    "\n",
    "#                 print('overall: acc = %.3f, fair = %.3f' %(np.mean(all_test_acc), np.std(all_test_acc)))\n",
    "    overall_acc.append(np.mean(all_test_acc))\n",
    "    overall_fair.append(np.std(all_test_acc))\n",
    "\n",
    "\n",
    "all_overall_acc.append(np.mean(overall_acc))\n",
    "all_overall_fair.append(np.mean(overall_fair))\n",
    "\n",
    "alpha_dict[alpha].append([np.mean(all_overall_acc), np.mean(all_overall_fair)])\n",
    "print(alpha)\n",
    "\n",
    "print('alpha:', alpha, alpha_dict[alpha])\n",
    "print('avg:', np.mean([e[0] for e in alpha_dict[alpha]]), np.mean([e[1] for e in alpha_dict[alpha]]))\n",
    "print('\\n')\n",
    "\n",
    "print('-------------------------------------------------------------------------------------------------')\n",
    "        \n",
    "for k, v in alpha_dict.items():\n",
    "    print(k, v)\n",
    "    print('avg:', np.mean([e[0] for e in v]), np.mean([e[1] for e in v]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b0074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
