{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asNHiu3VMV0m"
      },
      "source": [
        "## CL-Gym Example: Stable A-GEM on Rotated MNIST\n",
        "\n",
        "In this example, we use Averaged Gradient Episodic Memory (A-GEM) to train on Rotated MNIST benchmark. We use the stable version of [AGEM](https://arxiv.org/abs/1812.00420.pdf) using [Stable SGD](https://proceedings.neurips.cc/paper/2020/file/518a38cc9a0173d0b2dc088166981cf8-Paper.pdf) parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkkjxEZcNf9o"
      },
      "source": [
        "## 1. Defining Parameters\n",
        "First, we need to define our parameters/config for our experiment.\n",
        "We define all our parameters inside a python dictionary. The parameters define different aspects of continual learning examples. For example:\n",
        "-  How many tasks should we learn?\n",
        "-  What our batch-size will be?\n",
        "-  What Optimizer will we use?\n",
        "-  Where should we store our outputs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sqM1kbU8xfFC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cl_gym as cl\n",
        "# first let's create params/config for our experiment\n",
        "\n",
        "def make_params() -> dict:\n",
        "    import os\n",
        "    from pathlib import Path\n",
        "    import uuid\n",
        "\n",
        "    params = {\n",
        "            # benchmark\n",
        "            'num_tasks': 5,\n",
        "            'epochs_per_task': 5,\n",
        "            'per_task_memory_examples': 25,\n",
        "            'batch_size_train': 64,\n",
        "            'batch_size_memory': 32,\n",
        "            'batch_size_validation': 256,\n",
        "\n",
        "            # algorithm\n",
        "            'optimizer': 'SGD',\n",
        "            'learning_rate': 0.01,\n",
        "            'momentum': 0.8,\n",
        "            'learning_rate_decay': 1.0,\n",
        "            'criterion': torch.nn.CrossEntropyLoss(),\n",
        "            'device': torch.device('cuda:3' if torch.cuda.is_available() else 'cpu'), }\n",
        "\n",
        "    trial_id = str(uuid.uuid4())\n",
        "    params['trial_id'] = trial_id\n",
        "    params['output_dir'] = os.path.join(\"./outputs/{}\".format(trial_id))\n",
        "    Path(params['output_dir']).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHxYEiSkONpn"
      },
      "source": [
        "## 2. Training our continual learning algorithm\n",
        "\n",
        "Before seeing the code, let's explain the components one more time:\n",
        "### 2.1 Benchmark\n",
        "* We use the `RotatedMNIST` benchmark for this example. The benchmark includes gradual rotations of MNIST digits for each task. Something like this.\n",
        "<div>\n",
        "<img src=\"https://user-images.githubusercontent.com/8312051/122752221-845a0300-d245-11eb-8892-7c4119ffe1a5.png\" width=\"300\"/>\n",
        "</div>\n",
        "\n",
        "In CL-Gym we use RotatedMNIST as follows:\n",
        "```python\n",
        "benchmark = cl.benchmarks.RotatedMNIST(num_tasks=5)\n",
        "```\n",
        "---------\n",
        "\n",
        "### 2.2 Backbone\n",
        "\n",
        "* We use a MLP model with two hidden layers like this:\n",
        "<div>\n",
        "<img src=\"https://user-images.githubusercontent.com/8312051/122753641-67beca80-d247-11eb-87d3-dec5cc2e63d6.png\" width=\"300\"/>\n",
        "</div>\n",
        "\n",
        "To import our backbone, we use:\n",
        "```python\n",
        "backbone = cl.backbones.MLP2Layers(input_dim=784, hidden_dim_1=100, hidden_dim_2=100, output_dim=10)\n",
        "```\n",
        "\n",
        "You can also create your own PyTorch models. The backbone in CL-Gym is simply a lightweight wrapper around PyTorch's ``nn.Module``.\n",
        "\n",
        "--------\n",
        "\n",
        "### 2.3 Collecting metrics with Callbacks\n",
        "\n",
        "The `MetricCollector` callback evaluates the model at the end of each epoch, logs the metrics, plots the accuraies to file, and stores the validation accuracies as numpy arrays to file (see outputs folder).\n",
        "```python\n",
        "metric_callback = cl.callbacks.MetricCollecto(num_tasks=5,\n",
        "                                              eval_interval='epoch',\n",
        "                                              epochs_per_task=1)\n",
        "```\n",
        "\n",
        "-------\n",
        "\n",
        "### 2.4  Using off-the-shelf continual learning algorithms\n",
        "\n",
        "CL-Gym includes several continual learning algorithms. Here we use A-GEM algorithm with better parameters than the original paper:\n",
        "\n",
        "```python\n",
        "cl.algorithms.AGEM(backbone, benchmark, params)\n",
        "```\n",
        "\n",
        "You can also use other algorithms. For example, for Experience Replay method, you can use:\n",
        "```python\n",
        "cl.algorithms.ERRingBuffer(backbone, benchmark, params)\n",
        "```\n",
        "\n",
        "\n",
        "-------\n",
        "\n",
        "### 2.5 Gluing everything together with the Trainer\n",
        "\n",
        "The `Trainer` will orchestrate the experiment by handling the non-research part of continual learning experiments.\n",
        "\n",
        "```\n",
        "trainer = cl.trainer.ContinualTrainer(algorithm, params, callbacks=[metric_manager_callback])\n",
        "```\n",
        "\n",
        "\n",
        "The code below implements this note:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFLqD58bhJRf",
        "outputId": "a34cc040-4c26-4d0e-87e5-36bf4a005aff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------- Task 1 -----------------------\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/jaeyoung/nas/cl_gym/CL_Gym_RotMNIST2].ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdi2.kaist.ac.kr/home/jaeyoung/nas/cl_gym/CL_Gym_RotMNIST2%5D.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfinal avg-forget\u001b[39m\u001b[39m\"\u001b[39m, metric_manager_callback\u001b[39m.\u001b[39mmeters[\u001b[39m'\u001b[39m\u001b[39mforgetting\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcompute_final())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdi2.kaist.ac.kr/home/jaeyoung/nas/cl_gym/CL_Gym_RotMNIST2%5D.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m params \u001b[39m=\u001b[39m make_params()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdi2.kaist.ac.kr/home/jaeyoung/nas/cl_gym/CL_Gym_RotMNIST2%5D.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m train(params)\n",
            "\u001b[1;32m/home/jaeyoung/nas/cl_gym/CL_Gym_RotMNIST2].ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdi2.kaist.ac.kr/home/jaeyoung/nas/cl_gym/CL_Gym_RotMNIST2%5D.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Make trainer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdi2.kaist.ac.kr/home/jaeyoung/nas/cl_gym/CL_Gym_RotMNIST2%5D.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m trainer \u001b[39m=\u001b[39m cl\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mContinualTrainer(algorithm, params, callbacks\u001b[39m=\u001b[39m[metric_manager_callback])\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdi2.kaist.ac.kr/home/jaeyoung/nas/cl_gym/CL_Gym_RotMNIST2%5D.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdi2.kaist.ac.kr/home/jaeyoung/nas/cl_gym/CL_Gym_RotMNIST2%5D.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfinal avg-acc\u001b[39m\u001b[39m\"\u001b[39m, metric_manager_callback\u001b[39m.\u001b[39mmeters[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcompute_final())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdi2.kaist.ac.kr/home/jaeyoung/nas/cl_gym/CL_Gym_RotMNIST2%5D.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfinal avg-forget\u001b[39m\u001b[39m\"\u001b[39m, metric_manager_callback\u001b[39m.\u001b[39mmeters[\u001b[39m'\u001b[39m\u001b[39mforgetting\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcompute_final())\n",
            "File \u001b[0;32m~/anaconda3/envs/cl-gym/lib/python3.8/site-packages/cl_gym/trainer/base.py:107\u001b[0m, in \u001b[0;36mContinualTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_setup()\n\u001b[1;32m    106\u001b[0m \u001b[39m# fit: main training loop\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_fit()\n\u001b[1;32m    108\u001b[0m \u001b[39m# teardown\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_teardown()\n",
            "File \u001b[0;32m~/anaconda3/envs/cl-gym/lib/python3.8/site-packages/cl_gym/trainer/base.py:95\u001b[0m, in \u001b[0;36mContinualTrainer._run_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_fit\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     94\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_before_fit()\n\u001b[0;32m---> 95\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit()\n\u001b[1;32m     96\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_after_fit()\n",
            "File \u001b[0;32m~/anaconda3/envs/cl-gym/lib/python3.8/site-packages/cl_gym/trainer/base.py:85\u001b[0m, in \u001b[0;36mContinualTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_before_training_task()\n\u001b[1;32m     84\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtick(\u001b[39m'\u001b[39m\u001b[39mtask\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_algorithm_on_task(task)\n\u001b[1;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_after_training_task()\n",
            "File \u001b[0;32m~/anaconda3/envs/cl-gym/lib/python3.8/site-packages/cl_gym/trainer/base.py:43\u001b[0m, in \u001b[0;36mContinualTrainer.train_algorithm_on_task\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgorithm\u001b[39m.\u001b[39mbackbone\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgorithm\u001b[39m.\u001b[39mbackbone \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgorithm\u001b[39m.\u001b[39mbackbone\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 43\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (inp, targ, task_ids) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     44\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00minp\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtarg\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/cl-gym/lib/python3.8/site-packages/torch/utils/data/dataloader.py:517\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 517\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    518\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    520\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    521\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/anaconda3/envs/cl-gym/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    556\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    558\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    559\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
            "File \u001b[0;32m~/anaconda3/envs/cl-gym/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:44\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 44\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     45\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/anaconda3/envs/cl-gym/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 44\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     45\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/anaconda3/envs/cl-gym/lib/python3.8/site-packages/cl_gym/benchmarks/base.py:368\u001b[0m, in \u001b[0;36mDynamicTransformDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39m# transforms\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_transform(img)\n\u001b[1;32m    369\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[0;32m~/anaconda3/envs/cl-gym/lib/python3.8/site-packages/torchvision/transforms/transforms.py:60\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     59\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 60\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
            "File \u001b[0;32m~/anaconda3/envs/cl-gym/lib/python3.8/site-packages/torchvision/transforms/transforms.py:97\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m     90\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
            "File \u001b[0;32m~/anaconda3/envs/cl-gym/lib/python3.8/site-packages/torchvision/transforms/functional.py:136\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    134\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39marray(pic, np\u001b[39m.\u001b[39muint8, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n\u001b[1;32m    135\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mByteTensor(torch\u001b[39m.\u001b[39mByteStorage\u001b[39m.\u001b[39mfrom_buffer(pic\u001b[39m.\u001b[39;49mtobytes()))\n\u001b[1;32m    138\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mview(pic\u001b[39m.\u001b[39msize[\u001b[39m1\u001b[39m], pic\u001b[39m.\u001b[39msize[\u001b[39m0\u001b[39m], \u001b[39mlen\u001b[39m(pic\u001b[39m.\u001b[39mgetbands()))\n\u001b[1;32m    139\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/cl-gym/lib/python3.8/site-packages/PIL/Image.py:744\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    742\u001b[0m data \u001b[39m=\u001b[39m []\n\u001b[1;32m    743\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     l, s, d \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39;49mencode(bufsize)\n\u001b[1;32m    745\u001b[0m     data\u001b[39m.\u001b[39mappend(d)\n\u001b[1;32m    746\u001b[0m     \u001b[39mif\u001b[39;00m s:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from datasets.FairMNIST import NoiseMNIST\n",
        "from trainers.FairContinualTrainer import FairContinualTrainer\n",
        "from algorithms.agem_sensitive import AGEM_Sensitive\n",
        "\n",
        "def train(params):\n",
        "    # benchmark: Rotated MNIST\n",
        "    benchmark = cl.benchmarks.RotatedMNIST(num_tasks=params['num_tasks'],\n",
        "                                           per_task_memory_examples=params['per_task_memory_examples'],\n",
        "                                           per_task_rotation=22.5)\n",
        "\n",
        "    # backbone: MLP with 2 hidden layers\n",
        "    backbone = cl.backbones.MLP2Layers(input_dim=784, hidden_dim_1=32, hidden_dim_2=32, output_dim=10)\n",
        "\n",
        "    # Algorithm: A-GEM\n",
        "    algorithm = cl.algorithms.AGEM(backbone, benchmark, params)\n",
        "    # algorithm = AGEM_Sensitive(backbone, benchmark, params)\n",
        "\n",
        "    # algorithm = cl.algorithms.ERRingBuffer(backbone, benchmark, params)\n",
        "\n",
        "    # Callbacks\n",
        "    metric_manager_callback = cl.callbacks.MetricCollector(num_tasks=params['num_tasks'],\n",
        "                                                           eval_interval='epoch',\n",
        "                                                           epochs_per_task=params['epochs_per_task'])\n",
        "\n",
        "    # Make trainer\n",
        "    trainer = cl.trainer.ContinualTrainer(algorithm, params, callbacks=[metric_manager_callback])\n",
        "\n",
        "    trainer.run()\n",
        "    print(\"final avg-acc\", metric_manager_callback.meters['accuracy'].compute_final())\n",
        "    print(\"final avg-forget\", metric_manager_callback.meters['forgetting'].compute_final())\n",
        "\n",
        "\n",
        "params = make_params()\n",
        "train(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ibbA2luc-AZe"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (461557611.py, line 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    [25] Eval metrics for task 1 >> {'accuracy': 44.18, 'loss': 0.013980306243896484}\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "grad_ref\n",
        "[25] Eval metrics for task 1 >> {'accuracy': 44.18, 'loss': 0.013980306243896484}\n",
        "[25] Eval metrics for task 2 >> {'accuracy': 56.54, 'loss': 0.00928453779220581}\n",
        "[25] Eval metrics for task 3 >> {'accuracy': 70.54, 'loss': 0.00509959619641304}\n",
        "[25] Eval metrics for task 4 >> {'accuracy': 90.14, 'loss': 0.0013555611550807953}\n",
        "[25] Eval metrics for task 5 >> {'accuracy': 96.27, 'loss': 0.0004954223014414311}\n",
        "final avg-acc 71.53399999999999\n",
        "\n",
        "\n",
        "grad_batch\n",
        "[25] Eval metrics for task 1 >> {'accuracy': 17.42, 'loss': 0.023993745803833007}\n",
        "[25] Eval metrics for task 2 >> {'accuracy': 28.28, 'loss': 0.01793102569580078}\n",
        "[25] Eval metrics for task 3 >> {'accuracy': 53.44, 'loss': 0.009112693345546723}\n",
        "[25] Eval metrics for task 4 >> {'accuracy': 86.71, 'loss': 0.001737047752737999}\n",
        "[25] Eval metrics for task 5 >> {'accuracy': 96.97, 'loss': 0.00040261796098202467}\n",
        "final avg-acc 56.564\n",
        "final avg-forget 50.2775\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
