{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c3b92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir=./outputs/dataset=BiasedMNIST/seed=3_epoch=5_lr=0.001_tau=5_alpha=0.001_lmbd_1.0_lmbdold_0.0\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cl_gym as cl\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "seed = 3\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed) \n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# torch.backends.cudnn.enabled = False\n",
    "\n",
    "def make_params() -> dict:\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import uuid\n",
    "\n",
    "    params = {\n",
    "            # dataset\n",
    "            'dataset': \"BiasedMNIST\",\n",
    "            'fairness_agg': 'mean',\n",
    "\n",
    "            # benchmark\n",
    "            'seed': seed,\n",
    "            'num_tasks': 5,\n",
    "            'epochs_per_task': 5,\n",
    "            # 'per_task_examples': np.inf,\n",
    "            'per_task_examples': 10000,\n",
    "            'per_task_memory_examples': 128,\n",
    "            'batch_size_train': 64,\n",
    "            'batch_size_memory': 64,\n",
    "            'batch_size_validation': 256,\n",
    "            'tau': 5,\n",
    "\n",
    "            # algorithm\n",
    "            'optimizer': 'sgd',\n",
    "            'learning_rate': 0.001,\n",
    "            'momentum': 0.9,\n",
    "            'learning_rate_decay': 1.0,\n",
    "            'criterion': torch.nn.CrossEntropyLoss(),\n",
    "            'device': torch.device('cuda:3' if torch.cuda.is_available() else 'cpu'),\n",
    "             \n",
    "            # sample selection\n",
    "            'alpha': 0.001,\n",
    "            'lambda': 1.0,\n",
    "            'lambda_old': 0.0,\n",
    "              }\n",
    "    \n",
    "\n",
    "\n",
    "#     trial_id = str(uuid.uuid4())\n",
    "    trial_id = f\"dataset={params['dataset']}/seed={params['seed']}_epoch={params['epochs_per_task']}_lr={params['learning_rate']}_tau={params['tau']}_alpha={params['alpha']}\"\n",
    "    if params['lambda'] != 0:\n",
    "        trial_id+=f\"_lmbd_{params['lambda']}_lmbdold_{params['lambda_old']}\"\n",
    "    params['trial_id'] = trial_id\n",
    "    params['output_dir'] = os.path.join(\"./outputs/{}\".format(trial_id))\n",
    "    print(f\"output_dir={params['output_dir']}\")\n",
    "    Path(params['output_dir']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return params\n",
    "\n",
    "params = make_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99826537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"MNIST\" in params['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c7b886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "from datasets import BiasedMNIST\n",
    "\n",
    "if  params['dataset'] in [\"BiasedMNIST\"]:\n",
    "    benchmark = BiasedMNIST(num_tasks=params['num_tasks'],\n",
    "                                per_task_memory_examples=params['per_task_memory_examples'],\n",
    "                                per_task_examples = params['per_task_examples'])\n",
    "\n",
    "    n_feature = 3*28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87fd9653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAAfklEQVR4nGP8/4+B6oCJ+kaOGjpkDGUhQg2uRIfTQQRdSk4yxu9SZBOZcIhjAUSGKWlBPwARhdVKwqFMV5eSnIwIKsLjQbgUPtMJ2syE2+J/GNZDuXgiisjgxuItPIbiD1Z8aYCYvI9mHCabZEPJSXNYDaU08Q6dQnrUUOoDAA8BEUXMYt+3AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "from cl_gym.benchmarks.transforms import MNIST_MEAN, MNIST_STD\n",
    "COLOR_MAP = {\n",
    "    0: (1, 0, 0),\n",
    "    1: (0, 1, 0),\n",
    "    2: (1, 1, 0),\n",
    "    3: (0, 0, 1),\n",
    "    4: (1, 0.65, 0),\n",
    "    5: (0.5, 0, 0.5),\n",
    "    6: (0, 1, 1),\n",
    "    7: (1, 0.75, 0.8),\n",
    "    8: (0.8, 1, 0),\n",
    "    9: (.588, .294, 0.)\n",
    "}\n",
    "\n",
    "r = (1 - 0.1913)\n",
    "color_values = np.array(list(COLOR_MAP.values()))\n",
    "m_rgb = color_values.mean(axis=0)\n",
    "std_rgb = color_values.std(axis=0)\n",
    "\n",
    "bmnist_mean = [r*m + MNIST_MEAN[0] for m in m_rgb]\n",
    "bmnist_std = [(r*s**2+(1-r)*MNIST_STD[0]**2+r*(1-r)*(bmnist_mean[i] - m_rgb[i])**2)**0.5 for i, s in enumerate(std_rgb)]\n",
    "\n",
    "unnormalize = torchvision.transforms.Normalize([-m/s for m, s in zip(bmnist_mean, bmnist_std)], [1/s for s in bmnist_std])\n",
    "sample_idx = 95\n",
    "to_pil_image(unnormalize(benchmark.trains[2][sample_idx][0]), mode=\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8806699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAAAcCAIAAAA1Cjd5AAACh0lEQVR4nO2cXZKDIBCEB2pvlL3/DXIl2AcrxkVBfnpgwPkqD9FKBiXTNoMYQ47geHxIIosP+WY491n45fjhOX6kSXjumSsKEBWSogBQISkKgJ/bT/jDYNqo8Ihenw551/bGFqH66+mwG6jgQblrIEG/wd0nLOBoOaLlh7oXEpqgxkUr013V0BbWyquhRg++yySnPXhL5Nh0kUdraRn6Owxni5cqSuwvpOWSH1NgizKZSE+6QqZkPTkv78RbGDJU42k0rZZmLQG9CBi5A+bkQiy3NySxizxf7f2HdjyAPCdGkOtFdhTzseP+xpEYFnMllcudyk7Zj8fgyIhogYqs/b4SH+tCwm0C5cjxpaNgTOT92lRMV0i4CrYl0FlFwSZupqGRs+cAtdTimZ2ZqzrKPNpR3Q1qt9Rkyk3pRa5njsrxJeKxICYVDRenkOsWqBdi5oMzJUh1FAQU5R79C6E1bk4OPAdo012GcNUZXy0/aTR61Nk31lARjXYk6Z3IsVbgEmxwURbXmVHKfG6PS0aCKT1nju6SUkHKEVJ56uTPHOyfLBn+dc7mKWxE5q2k4TMNtM4NWU6mSPEZWaZAIgGOxH8AVfdhJQyupNFoRx18A6vMomg1DUtwUqK8cdqI1Qw7u5XlyFKadH18s6J8CnLGkEUlfYdszGnifmhnyEpRToJNMGlpDV3iIE0nRcgsjUSRlVvni4cUaSVW0zn3b3OQiurqKyFVmf+8lFuWm2xgeLCvcWn29t1MRxouofQq74fPiScQJSRXU7NZe1MIVanomP2Q5B6ukHz0iYkKjID/tTseQTzbcvIwvRL8hP6vHZppLhZwJDgSrvfFPDGhPA3NPEUBoEJSFAB/IGzFlaox/UAAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=280x28>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_label = 0\n",
    "incremental_step = 1\n",
    "# cat_img = torch.cat([img for img in benchmark.trains[incremental_step].inputs[benchmark.trains[incremental_step].targets == target_label][20:30]], dim=2)\n",
    "cat_img = torch.cat([img for img, target, *_ in benchmark.tests[incremental_step]][20:30], dim=2)\n",
    "to_pil_image(unnormalize(cat_img), mode=\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e73e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainers import FairContinualTrainer\n",
    "from trainers.fair_trainer import FairContinualTrainer2\n",
    "from metrics import FairMetricCollector\n",
    "from algorithms.sensitive import Heuristic3\n",
    "\n",
    "backbone = cl.backbones.MLP2Layers(input_dim=n_feature, hidden_dim_1=256, hidden_dim_2=256, output_dim=10)\n",
    "algorithm = Heuristic3(backbone, benchmark, params, requires_memory=True)\n",
    "metric_manager_callback = FairMetricCollector(num_tasks=params['num_tasks'],\n",
    "                                                        eval_interval='epoch',\n",
    "                                                        epochs_per_task=params['epochs_per_task'])\n",
    "\n",
    "trainer = FairContinualTrainer2(algorithm, params, callbacks=[metric_manager_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f782b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark.trains[1][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b8e3d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function mean at 0x7f4ee1c6aef0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if params['fairness_agg'] == \"mean\":\n",
    "    agg = np.mean\n",
    "elif params['fairness_agg'] == \"max\":\n",
    "    agg = np.max\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "metric_manager_callback.meters['multiclass_eo'].agg = agg\n",
    "metric_manager_callback.meters['multiclass_eo'].agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2eac756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Task 1 -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Eval metrics for task 1 >> {'accuracy': 80.73811022206239, 'loss': 0.0024567958874623544, 'multiclass_eo': [0.2655601659751037, 0.5088028169014085], 'accuracy_s0': 100.0, 'accuracy_s1': 61.281850856174394, 'classwise_accuracy': {0: array([852, 980]), 1: array([ 846, 1135])}}\n",
      "[2] Eval metrics for task 1 >> {'accuracy': 85.94151757619348, 'loss': 0.0017660588801048044, 'multiclass_eo': [0.3926056338028169, 0.17219917012448138], 'accuracy_s0': 100.0, 'accuracy_s1': 71.75975980363509, 'classwise_accuracy': {1: array([ 912, 1135]), 0: array([897, 980])}}\n",
      "[3] Eval metrics for task 1 >> {'accuracy': 89.67005304324374, 'loss': 0.001120564144272049, 'multiclass_eo': [0.12448132780082988, 0.2904929577464789], 'accuracy_s0': 100.0, 'accuracy_s1': 79.25128572263456, 'classwise_accuracy': {0: array([920, 980]), 1: array([ 970, 1135])}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal avg-acc\u001b[39m\u001b[38;5;124m\"\u001b[39m, metric_manager_callback\u001b[38;5;241m.\u001b[39mmeters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcompute_final())\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal avg-forget\u001b[39m\u001b[38;5;124m\"\u001b[39m, metric_manager_callback\u001b[38;5;241m.\u001b[39mmeters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforgetting\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcompute_final())\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/cl_gym/trainer/base.py:105\u001b[0m, in \u001b[0;36mContinualTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_setup()\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# fit: main training loop\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# teardown\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_teardown()\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/cl_gym/trainer/base.py:93\u001b[0m, in \u001b[0;36mContinualTrainer._run_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_before_fit()\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_after_fit()\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/cl_gym/trainer/base.py:83\u001b[0m, in \u001b[0;36mContinualTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_before_training_task()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_algorithm_on_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_after_training_task()\n",
      "File \u001b[0;32m/mnt/shared/jaeyoung/FSSCIL/trainers/fair_trainer.py:97\u001b[0m, in \u001b[0;36mFairContinualTrainer2.train_algorithm_on_task\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mbackbone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, items \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     98\u001b[0m     inp, targ, task_ids, sample_weight, sensitive_label, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m items\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# if batch_idx == 0:\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m#     print(f\"{sample_weight.to(device)=}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/mnt/shared/jaeyoung/FSSCIL/datasets/base.py:80\u001b[0m, in \u001b[0;36mSplitDataset4.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     77\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/torchvision/transforms/functional.py:166\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    165\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[0;32m--> 166\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    169\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/PIL/Image.py:701\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 701\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (\u001b[38;5;167;01mMemoryError\u001b[39;00m, \u001b[38;5;167;01mRecursionError\u001b[39;00m)):\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/PIL/Image.py:767\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    764\u001b[0m e \u001b[38;5;241m=\u001b[39m _getencoder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode, encoder_name, args)\n\u001b[1;32m    765\u001b[0m e\u001b[38;5;241m.\u001b[39msetimage(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim)\n\u001b[0;32m--> 767\u001b[0m bufsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m65536\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m)  \u001b[38;5;66;03m# see RawEncode.c\u001b[39;00m\n\u001b[1;32m    769\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/PIL/Image.py:541\u001b[0m, in \u001b[0;36mImage.size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msize\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_size\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.run()\n",
    "print(\"final avg-acc\", metric_manager_callback.meters['accuracy'].compute_final())\n",
    "print(\"final avg-forget\", metric_manager_callback.meters['forgetting'].compute_final())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebde5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[94.781,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [94.738, 86.402,  0.   ,  0.   ,  0.   ],\n",
       "       [93.802, 89.003, 85.144,  0.   ,  0.   ],\n",
       "       [91.151, 83.405, 85.075, 89.185,  0.   ],\n",
       "       [89.815, 76.829, 81.923, 88.378, 87.403]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_manager_callback.meters['accuracy'].get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b0074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[94.781, 90.57, 89.316, 87.204, 84.87]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.round(x, 3) for x in metric_manager_callback.meters['accuracy'].compute_overall()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e893eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.3481835003114"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(metric_manager_callback.meters['accuracy'].compute_overall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82bd073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.105, 0.146, 0.177, 0.165, 0.197]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.round(x, 3) for x in metric_manager_callback.meters['multiclass_eo'].compute_overall()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7973e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15817295661807243"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(metric_manager_callback.meters['multiclass_eo'].compute_overall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61271a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_id=2\n",
      "sensitive samples / all samples = 613 / 12089\n",
      "sensitive samples / selected samples = 415 / 704\n"
     ]
    }
   ],
   "source": [
    "task_id = 2\n",
    "\n",
    "print(f\"{task_id=}\")\n",
    "print(f\"sensitive samples / all samples = {(benchmark.trains[task_id].sensitives != benchmark.trains[task_id].targets).sum().item()} / {benchmark.trains[task_id].sensitives.shape[0]}\")\n",
    "\n",
    "updated_seq_indices = benchmark.seq_indices_train[task_id]\n",
    "print(f\"sensitive samples / selected samples = {(benchmark.trains[task_id].sensitives[updated_seq_indices] != benchmark.trains[task_id].targets[updated_seq_indices]).sum().item()} / {len(updated_seq_indices)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32febda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incremental_step=2\n",
      "2 : 5958 --> 1238\n",
      "3 : 6131 --> 1554\n",
      "incremental_step=3\n",
      "4 : 5842 --> 1410\n",
      "5 : 5421 --> 1921\n",
      "incremental_step=4\n",
      "6 : 5918 --> 1653\n",
      "7 : 6265 --> 1590\n",
      "incremental_step=5\n",
      "8 : 5851 --> 1666\n",
      "9 : 5949 --> 2019\n"
     ]
    }
   ],
   "source": [
    "step_class = 2\n",
    "for i in range(2, 6):\n",
    "    incremental_step = i\n",
    "    print(f\"{incremental_step=}\")\n",
    "    one_idx = benchmark.trains[incremental_step].sample_weight > 0.9\n",
    "\n",
    "    print(f\"{2*i-2} : {(benchmark.trains[incremental_step].targets == (2*i-2)).sum().item()} --> {(benchmark.trains[incremental_step].targets[one_idx] == (2*i-2)).sum().item()}\")\n",
    "    print(f\"{2*i-1} : {(benchmark.trains[incremental_step].targets == (2*i-1)).sum().item()} --> {(benchmark.trains[incremental_step].targets[one_idx] == (2*i-1)).sum().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0880e76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.046 0.164 \n",
      "0.108 0.065 0.062 0.35 \n",
      "0.113 0.083 0.212 0.104 0.152 0.402 \n",
      "0.13 0.117 0.208 0.145 0.202 0.214 0.114 0.19 \n",
      "0.186 0.123 0.125 0.268 0.281 0.236 0.212 0.134 0.15 0.259 \n"
     ]
    }
   ],
   "source": [
    "for eos in metric_manager_callback.meters['multiclass_eo'].get_data():\n",
    "    for eo in eos:\n",
    "        print(round(eo, 3),end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6233f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: array([958, 980]), 1: array([1042, 1135])},\n",
       " {0: array([914, 980]),\n",
       "  1: array([1092, 1135]),\n",
       "  3: array([ 980, 1010]),\n",
       "  2: array([ 782, 1032])},\n",
       " {0: array([906, 980]),\n",
       "  1: array([1080, 1135]),\n",
       "  3: array([ 873, 1010]),\n",
       "  2: array([ 945, 1032]),\n",
       "  4: array([906, 982]),\n",
       "  5: array([696, 892])},\n",
       " {1: array([1043, 1135]),\n",
       "  0: array([886, 980]),\n",
       "  3: array([ 848, 1010]),\n",
       "  2: array([ 855, 1032]),\n",
       "  5: array([752, 892]),\n",
       "  4: array([843, 982]),\n",
       "  7: array([ 943, 1028]),\n",
       "  6: array([830, 958])},\n",
       " {0: array([865, 980]),\n",
       "  1: array([1037, 1135]),\n",
       "  2: array([ 847, 1032]),\n",
       "  3: array([ 723, 1010]),\n",
       "  4: array([791, 982]),\n",
       "  5: array([743, 892]),\n",
       "  7: array([ 891, 1028]),\n",
       "  6: array([863, 958]),\n",
       "  8: array([885, 974]),\n",
       "  9: array([ 847, 1009])}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_manager_callback.meters['classwise_accuracy'].get_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
