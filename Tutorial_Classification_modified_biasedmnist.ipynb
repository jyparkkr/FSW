{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c3b92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir=./outputs/seed=1_epoch=1q__\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cl_gym as cl\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "# dataset = 'MNIST'\n",
    "seed = 1\n",
    "# dataset = 'CIFAR10'\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "def make_params() -> dict:\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import uuid\n",
    "\n",
    "    params = {\n",
    "            # dataset\n",
    "            'dataset': \"BiasedMNIST\",\n",
    "\n",
    "            # benchmark\n",
    "            'seed': seed,\n",
    "            'num_tasks': 5,\n",
    "            'epochs_per_task': 1,\n",
    "            'per_task_examples': np.inf,\n",
    "            'per_task_memory_examples': 20,\n",
    "            'batch_size_train': 64,\n",
    "            'batch_size_memory': 64,\n",
    "            'batch_size_validation': 256,\n",
    "            'lambda': 10,\n",
    "\n",
    "            # algorithm\n",
    "            'optimizer': 'sgd',\n",
    "            'learning_rate': 0.001,\n",
    "            'momentum': 0.9,\n",
    "            'learning_rate_decay': 1.0,\n",
    "            'criterion': torch.nn.CrossEntropyLoss(),\n",
    "            'device': torch.device('cuda:6' if torch.cuda.is_available() else 'cpu'), }\n",
    "\n",
    "#     trial_id = str(uuid.uuid4())\n",
    "    trial_id = f\"seed={params['seed']}_epoch={params['epochs_per_task']}q__\"\n",
    "    params['trial_id'] = trial_id\n",
    "    params['output_dir'] = os.path.join(\"./outputs/{}\".format(trial_id))\n",
    "    print(f\"output_dir={params['output_dir']}\")\n",
    "    Path(params['output_dir']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return params\n",
    "\n",
    "params = make_params()\n",
    "dataset = params['dataset']\n",
    "\n",
    "def get_metric_task(task, meter, r = 3):\n",
    "    return np.round(meter.data[task,1:task+1,-1], r)\n",
    "\n",
    "def get_metric_mat(meter, r = 3):\n",
    "    return np.round(meter.data[1:6,1:6,-1], r)\n",
    "\n",
    "def get_overall_metric(meter, r = 3):\n",
    "    avg_list = list()\n",
    "    for i in range(1, 6):\n",
    "        avg_list.append(np.mean(meter.data[i,1:i+1,-1]))\n",
    "    return np.round(np.mean(avg_list), r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c7b886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Too many dimensions: 3 > 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# alpha_li = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# lamb_li = [0.01, 0.05, 0.1, 1, 10, 50, 100]\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m benchmark \u001b[38;5;241m=\u001b[39m \u001b[43mBiasedMNIST\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_tasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_tasks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mper_task_memory_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mper_task_memory_examples\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mper_task_examples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mper_task_examples\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/shared/jaeyoung/FSSCIL/datasets/MNIST.py:42\u001b[0m, in \u001b[0;36mMNIST.__init__\u001b[0;34m(self, num_tasks, per_task_examples, per_task_joint_examples, per_task_memory_examples, per_task_subset_examples, task_input_transforms, task_target_transforms, random_class_idx)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_tasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mper_task_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mper_task_joint_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mper_task_memory_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mper_task_subset_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_input_transforms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_target_transforms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/cl_gym/benchmarks/mnist.py:150\u001b[0m, in \u001b[0;36mSplitMNIST.__init__\u001b[0;34m(self, num_tasks, per_task_examples, per_task_joint_examples, per_task_memory_examples, per_task_subset_examples, task_input_transforms, task_target_transforms)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_input_transforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     task_input_transforms \u001b[38;5;241m=\u001b[39m get_default_mnist_transform(num_tasks)\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_tasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mper_task_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mper_task_joint_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mper_task_memory_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mper_task_subset_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_input_transforms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_target_transforms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/cl_gym/benchmarks/mnist.py:43\u001b[0m, in \u001b[0;36mContinualMNIST.__init__\u001b[0;34m(self, num_tasks, per_task_examples, per_task_joint_examples, per_task_memory_examples, per_task_subset_examples, task_input_transforms, task_target_transforms)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    num_tasks: The number of tasks for the benchmark.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    of size `num_tasks` where each element of the list can be a torchvision (Composed) transform.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(num_tasks, per_task_examples, per_task_joint_examples, per_task_memory_examples,\n\u001b[1;32m     41\u001b[0m                  per_task_subset_examples, task_input_transforms, task_target_transforms)\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_datasets()\n",
      "File \u001b[0;32m/mnt/shared/jaeyoung/FSSCIL/datasets/BiasedMNIST.py:183\u001b[0m, in \u001b[0;36mBiasedMNIST.load_datasets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load_mnist()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_tasks \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrains[task] \u001b[38;5;241m=\u001b[39m \u001b[43mSplitDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmnist_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtests[task] \u001b[38;5;241m=\u001b[39m SplitDataset(task, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmnist_test)\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/cl_gym/benchmarks/base.py:392\u001b[0m, in \u001b[0;36mSplitDataset.__init__\u001b[0;34m(self, task_id, classes_per_split, dataset)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_id \u001b[38;5;241m=\u001b[39m task_id\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_per_split \u001b[38;5;241m=\u001b[39m classes_per_split\n\u001b[0;32m--> 392\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__build_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/cl_gym/benchmarks/base.py:406\u001b[0m, in \u001b[0;36mSplitDataset.__build_split\u001b[0;34m(self, dataset, task_id)\u001b[0m\n\u001b[1;32m    404\u001b[0m selected_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(np\u001b[38;5;241m.\u001b[39mlogical_and(start_class \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m target_classes, target_classes \u001b[38;5;241m<\u001b[39m end_class))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m selected_indices:\n\u001b[0;32m--> 406\u001b[0m     img, target \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    407\u001b[0m     target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(target)\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs\u001b[38;5;241m.\u001b[39mappend(img)\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/torchvision/datasets/mnist.py:142\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    138\u001b[0m img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[index])\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "File \u001b[0;32m~/anaconda3/envs/cil/lib/python3.9/site-packages/PIL/Image.py:3094\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m>\u001b[39m ndmax:\n\u001b[1;32m   3093\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo many dimensions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m > \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndmax\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 3094\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   3096\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m shape[\u001b[38;5;241m1\u001b[39m], shape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   3097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strides \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Too many dimensions: 3 > 2."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "# from torchvision import models\n",
    "# from torchinfo import summary\n",
    "# from resnet import ResNet18\n",
    "import random\n",
    "import copy\n",
    "\n",
    "\n",
    "from datasets import BiasedMNIST\n",
    "from trainers.FairContinualTrainer import FairContinualTrainer\n",
    "from metrics.fair_metric_manager import FairMetricCollector\n",
    "from algorithms.sensitive import Heuristic2\n",
    "\n",
    "if  params['dataset'] in [\"BiasedMNIST\"]:\n",
    "    n_feature = 3*28*28\n",
    "\n",
    "if params['dataset'] == 'BiasedMNIST':\n",
    "    params['alpha'] = 0.005\n",
    "    params['lambda'] = 50\n",
    "\n",
    "# alpha_li = [0.05]\n",
    "# lamb_li = [50]\n",
    "noise = 0.0\n",
    "params['alpha'] = 0.05\n",
    "\n",
    "# alpha_li = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]\n",
    "# lamb_li = [0.01, 0.05, 0.1, 1, 10, 50, 100]\n",
    "benchmark = BiasedMNIST(num_tasks=params['num_tasks'],\n",
    "                             per_task_memory_examples=params['per_task_memory_examples'],\n",
    "                             per_task_examples = min(params['per_task_examples'], 15000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed27917",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MNIST' object has no attribute 'sensitive_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmnist_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msensitive_labels\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MNIST' object has no attribute 'sensitive_labels'"
     ]
    }
   ],
   "source": [
    "benchmark.mnist_train.sensitive_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73e1c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_feature' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m backbone \u001b[38;5;241m=\u001b[39m cl\u001b[38;5;241m.\u001b[39mbackbones\u001b[38;5;241m.\u001b[39mMLP2Layers(input_dim\u001b[38;5;241m=\u001b[39m\u001b[43mn_feature\u001b[49m, hidden_dim_1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, hidden_dim_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      2\u001b[0m algorithm \u001b[38;5;241m=\u001b[39m Heuristic2(backbone, benchmark, params, requires_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m metric_manager_callback \u001b[38;5;241m=\u001b[39m FairMetricCollector(num_tasks\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_tasks\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      4\u001b[0m                                                         eval_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m                                                         epochs_per_task\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs_per_task\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_feature' is not defined"
     ]
    }
   ],
   "source": [
    "backbone = cl.backbones.MLP2Layers(input_dim=n_feature, hidden_dim_1=256, hidden_dim_2=256, output_dim=10)\n",
    "algorithm = Heuristic2(backbone, benchmark, params, requires_memory=True)\n",
    "metric_manager_callback = FairMetricCollector(num_tasks=params['num_tasks'],\n",
    "                                                        eval_interval='epoch',\n",
    "                                                        epochs_per_task=params['epochs_per_task'])\n",
    "\n",
    "# from trainers.FairContinualTrainer import FairContinualTrainer\n",
    "# trainer = FairContinualTrainer(algorithm, params, callbacks=[metric_manager_callback])\n",
    "\n",
    "trainer = FairContinualTrainer(algorithm, params, callbacks=[metric_manager_callback])\n",
    "trainer.run()\n",
    "print(\"final avg-acc\", metric_manager_callback.meters['accuracy'].compute_final())\n",
    "print(\"final avg-forget\", metric_manager_callback.meters['forgetting'].compute_final())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "becbcc5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1291979344.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    ---------------------------- Task 1 -----------------------\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "---------------------------- Task 1 -----------------------\n",
    "[1] Eval metrics for task 1 >> {'accuracy': 98.6, 'loss': 0.00015132083231583237}\n",
    "training_task_end\n",
    "load_memory_joint: len(train_loader.dataset)=64\n",
    "---------------------------- Task 2 -----------------------\n",
    "len(select_curr_indexes)=5075\n",
    "[2] Eval metrics for task 1 >> {'accuracy': 85.85, 'loss': 0.0034543287754058836}\n",
    "[2] Eval metrics for task 2 >> {'accuracy': 86.6, 'loss': 0.0027514055371284487}\n",
    "training_task_end\n",
    "load_memory_joint: len(train_loader.dataset)=128\n",
    "---------------------------- Task 3 -----------------------\n",
    "len(select_curr_indexes)=9259\n",
    "[3] Eval metrics for task 1 >> {'accuracy': 82.85, 'loss': 0.005165461122989655}\n",
    "[3] Eval metrics for task 2 >> {'accuracy': 59.0, 'loss': 0.007712359070777893}\n",
    "[3] Eval metrics for task 3 >> {'accuracy': 97.55, 'loss': 0.0003111732173711061}\n",
    "training_task_end\n",
    "load_memory_joint: len(train_loader.dataset)=192\n",
    "---------------------------- Task 4 -----------------------\n",
    "len(select_curr_indexes)=11205\n",
    "[4] Eval metrics for task 1 >> {'accuracy': 66.7, 'loss': 0.010747700095176697}\n",
    "[4] Eval metrics for task 2 >> {'accuracy': 48.0, 'loss': 0.012582610368728637}\n",
    "[4] Eval metrics for task 3 >> {'accuracy': 57.25, 'loss': 0.008546485185623169}\n",
    "[4] Eval metrics for task 4 >> {'accuracy': 92.6, 'loss': 0.0011509314700961113}\n",
    "training_task_end\n",
    "load_memory_joint: len(train_loader.dataset)=256\n",
    "---------------------------- Task 5 -----------------------\n",
    "len(select_curr_indexes)=7916\n",
    "[5] Eval metrics for task 1 >> {'accuracy': 74.3, 'loss': 0.007032525479793548}\n",
    "[5] Eval metrics for task 2 >> {'accuracy': 56.75, 'loss': 0.011757457256317138}\n",
    "[5] Eval metrics for task 3 >> {'accuracy': 63.9, 'loss': 0.00855764478445053}\n",
    "[5] Eval metrics for task 4 >> {'accuracy': 80.9, 'loss': 0.003836171865463257}\n",
    "[5] Eval metrics for task 5 >> {'accuracy': 92.7, 'loss': 0.0013385150507092475}\n",
    "training_task_end\n",
    "load_memory_joint: len(train_loader.dataset)=320\n",
    "final avg-acc 73.71000000000001\n",
    "final avg-forget 24.874999999999993\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1b35c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04a1586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33b583",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'taskd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtaskd\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'taskd' is not defined"
     ]
    }
   ],
   "source": [
    "taskd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d8346",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "delete() missing 1 required positional argument: 'obj'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: delete() missing 1 required positional argument: 'obj'"
     ]
    }
   ],
   "source": [
    "np.delete(np.random.randint(0, 10, size=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7aaa68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 7, 8, 4, 9, 0, 2, 1, 6, 5]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.choice(10, size=10, replace=False).tolist()\n",
    "sorted(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af81222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(5):\n",
    "    if len(classwise_loss[x]) == 0:\n",
    "        del classwise_loss[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b9c92c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324660bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, Subset, ConcatDataset\n",
    "\n",
    "task = 1\n",
    "trainset = Subset(benchmark.trains[task], range(100))\n",
    "batch_size = 32\n",
    "shuffle=False\n",
    "num_workers = 0\n",
    "pin_memory = True\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size, shuffle, num_workers=num_workers,\n",
    "                                  pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bfa274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 1, 1, 1])\n",
      "tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
      "        0, 1, 1, 1, 0, 1, 0, 0])\n",
      "tensor([1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (inp, targ, t_id, *_) in enumerate(train_loader):\n",
    "    print(targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3387f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark.trains[task].targets[32:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d66be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(benchmark.trains[task].targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6eff29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = train_loader.dataset.dataset.targets\n",
    "targets.unique().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd7102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "num_dict = {x:0 for x in targets.unique().cpu().numpy()}\n",
    "for k in num_dict:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c6bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(num_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4187b211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0],\n",
       "        [    1],\n",
       "        [    2],\n",
       "        ...,\n",
       "        [11994],\n",
       "        [11995],\n",
       "        [11999]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(targets==0).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c61ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 1, 1, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda4b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,     1,     2,  ..., 11994, 11995, 11999])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(targets==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14742022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5880fb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m----> 6\u001b[0m     x_train_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43mall_train_data\u001b[49m[i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m:i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m      7\u001b[0m     y_train_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(all_train_label[i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m:i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_train_data' is not defined"
     ]
    }
   ],
   "source": [
    "device = params['device']\n",
    "\n",
    "for i in range(5):\n",
    "    torch.cuda.empty_cache()\n",
    "    if i == 0:\n",
    "        x_train_batch = torch.cat(all_train_data[i*2:i*2+2])\n",
    "        y_train_batch = torch.cat(all_train_label[i*2:i*2+2])\n",
    "\n",
    "    elif i > 0:\n",
    "        x_train_batch = torch.cat(all_train_data[i*2:i*2+2])\n",
    "        y_train_batch = torch.cat(all_train_label[i*2:i*2+2])\n",
    "        if i == 1:\n",
    "            x_prev_buffer1 = torch.cat(all_train_data[i*2-2:i*2-1])\n",
    "            y_prev_buffer1 = torch.cat(all_train_label[i*2-2:i*2-1])\n",
    "            indices1 = torch.randperm(len(x_prev_buffer1))[:buffer_size]\n",
    "            x_prev_buffer1 = x_prev_buffer1[indices1]\n",
    "            y_prev_buffer1 = y_prev_buffer1[indices1]\n",
    "\n",
    "            x_prev_buffer2 = torch.cat(all_train_data[i*2-1:i*2])\n",
    "            y_prev_buffer2 = torch.cat(all_train_label[i*2-1:i*2])\n",
    "            indices2 = torch.randperm(len(x_prev_buffer2))[:buffer_size]\n",
    "            x_prev_buffer2 = x_prev_buffer2[indices2]\n",
    "            y_prev_buffer2 = y_prev_buffer2[indices2]\n",
    "\n",
    "            x_prev_buffer = torch.cat([x_prev_buffer1, x_prev_buffer2])\n",
    "            y_prev_buffer = torch.cat([y_prev_buffer1, y_prev_buffer2])\n",
    "\n",
    "        else:\n",
    "\n",
    "            # select gradient-based herding buffer\n",
    "            x_new_buffer = torch.cat(all_train_data[i*2-2:i*2])\n",
    "            y_new_buffer = torch.cat(all_train_label[i*2-2:i*2])\n",
    "\n",
    "            x_new_buffer = x_new_buffer[select_buffer_indexes]\n",
    "            y_new_buffer = y_new_buffer[select_buffer_indexes]\n",
    "\n",
    "            x_prev_buffer = torch.cat([x_prev_buffer, x_new_buffer])\n",
    "            y_prev_buffer = torch.cat([y_prev_buffer, y_new_buffer])\n",
    "            \n",
    "#                     print('memory2:', torch.cuda.memory_allocated(device=device)/1024/1024/1024)\n",
    "\n",
    "        # buffer에 있는 class별 loss가 buffer_losses list에 들어감\n",
    "        buffer_losses = []\n",
    "        \n",
    "\n",
    "        # computation of mean gradients and losses for buffers \n",
    "        for n in range(i*2):\n",
    "            buffer_ind = [m for m in range(len(y_prev_buffer)) if y_prev_buffer[m] == n]\n",
    "            x_buffer = x_prev_buffer[buffer_ind]\n",
    "            y_buffer = y_prev_buffer[buffer_ind]\n",
    "            x_buffer = torch.Tensor(x_buffer).to(device, dtype=torch.float32)\n",
    "            y_buffer = torch.Tensor(y_buffer).to(device, dtype=torch.int64)\n",
    "\n",
    "            if n == 0:\n",
    "                model.zero_grad()\n",
    "\n",
    "                # mean gradient computation\n",
    "                out, emb = model(x_buffer)\n",
    "\n",
    "                init_out_buffer = out\n",
    "                init_emb_buffer = emb\n",
    "                init_y_buffer = y_buffer.view(-1, 1)\n",
    "\n",
    "\n",
    "                loss = loss_sample(out, y_buffer).sum()\n",
    "                buffer_l0_grads = torch.autograd.grad(loss, out)[0] #torch.Size([32, 10])\n",
    "                if dataset in ['MNIST', 'FMNIST']:\n",
    "                    buffer_l0_expand = torch.repeat_interleave(buffer_l0_grads, 256, dim=1)\n",
    "                else:\n",
    "                    buffer_l0_expand = torch.repeat_interleave(buffer_l0_grads, 512, dim=1)\n",
    "                buffer_l1_grads = buffer_l0_expand * emb.repeat(1, n_class)\n",
    "\n",
    "                buffer_l0_grads = buffer_l0_grads.mean(dim=0).view(1, -1)\n",
    "                buffer_l1_grads = buffer_l1_grads.mean(dim=0).view(1, -1)\n",
    "\n",
    "                # mean loss computation\n",
    "                buffer_ds = TensorDataset(x_buffer, y_buffer)\n",
    "                buffer_loader = DataLoader(dataset=buffer_ds, batch_size=batch_size, shuffle=True)\n",
    "                buffer_output, buffer_loss = clf.evaluate(buffer_loader)\n",
    "                buffer_losses.append(buffer_loss)\n",
    "\n",
    "            else:\n",
    "                model.zero_grad()\n",
    "\n",
    "                # mean gradient computation\n",
    "                out, emb = model(x_buffer)\n",
    "\n",
    "                init_out_buffer = torch.cat((init_out_buffer, out), dim=0)\n",
    "                init_emb_buffer = torch.cat((init_emb_buffer, emb), dim=0)\n",
    "                init_y_buffer = torch.cat((init_y_buffer, y_buffer.view(-1, 1)), dim=0)\n",
    "\n",
    "                loss = loss_sample(out, y_buffer).sum()\n",
    "                batch_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "                if dataset in ['MNIST', 'FMNIST']:\n",
    "                    batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 256, dim=1)\n",
    "                else:\n",
    "                    batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 512, dim=1)\n",
    "\n",
    "                batch_l1_grads = batch_l0_expand * emb.repeat(1, n_class)\n",
    "\n",
    "                batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)\n",
    "                batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)\n",
    "\n",
    "                buffer_l0_grads = torch.cat((buffer_l0_grads, batch_l0_grads), dim=0)\n",
    "                buffer_l1_grads = torch.cat((buffer_l1_grads, batch_l1_grads), dim=0)\n",
    "\n",
    "                # mean loss computation\n",
    "                buffer_ds = TensorDataset(x_buffer, y_buffer)\n",
    "                buffer_loader = DataLoader(dataset=buffer_ds, batch_size=batch_size, shuffle=True)\n",
    "                buffer_output, buffer_loss = clf.evaluate(buffer_loader)\n",
    "                buffer_losses.append(buffer_loss)\n",
    "                \n",
    "        \n",
    "        # initialize individual sample gradients of new data\n",
    "        new_l0_grads = torch.empty((0, n_class), device=device, dtype=torch.float32)\n",
    "        if dataset in ['MNIST', 'FMNIST']:\n",
    "            new_l1_grads = torch.empty((0, n_class*256), device=device, dtype=torch.float32)\n",
    "        else:\n",
    "            new_l1_grads = torch.empty((0, n_class*512), device=device, dtype=torch.float32)\n",
    "\n",
    "        # computation of mean and individual gradients and mean losses for new data \n",
    "        for n in range(2):\n",
    "            model.zero_grad()\n",
    "\n",
    "            # mean gradient computation\n",
    "            x_new_buffer = torch.cat(all_train_data[i*2+n:i*2+n+1])\n",
    "            y_new_buffer = torch.cat(all_train_label[i*2+n:i*2+n+1])\n",
    "            x_new_buffer = torch.Tensor(x_new_buffer).to(device, dtype=torch.float32)\n",
    "            y_new_buffer = torch.Tensor(y_new_buffer).to(device, dtype=torch.int64)\n",
    "            \n",
    "            new_buffer_ds = TensorDataset(x_new_buffer, y_new_buffer)\n",
    "            new_buffer_loader = DataLoader(dataset=new_buffer_ds, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            for batch_idx, batch_data in enumerate(new_buffer_loader):\n",
    "                model.zero_grad()\n",
    "                if batch_idx == 0:\n",
    "                    x_batch, y_batch = batch_data\n",
    "\n",
    "                    out, emb = model(x_batch)\n",
    "\n",
    "                    init_out_buffer = torch.cat((init_out_buffer, out), dim=0)\n",
    "                    init_emb_buffer = torch.cat((init_emb_buffer, emb), dim=0)\n",
    "                    init_y_buffer = torch.cat((init_y_buffer, y_batch.view(-1, 1)), dim=0)\n",
    "\n",
    "                    loss = loss_sample(out, y_batch).sum()\n",
    "                    batch_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        if dataset in ['MNIST', 'FMNIST']:\n",
    "                            batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 256, dim=1)\n",
    "                        else:\n",
    "                            batch_l0_expand = torch.repeat_interleave(batch_l0_grads, 512, dim=1)\n",
    "                        batch_l1_grads = batch_l0_expand * emb.repeat(1, n_class)\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    x_batch, y_batch = batch_data\n",
    "\n",
    "                    out, emb = model(x_batch)\n",
    "\n",
    "                    init_out_buffer = torch.cat((init_out_buffer, out), dim=0)\n",
    "                    init_emb_buffer = torch.cat((init_emb_buffer, emb), dim=0)\n",
    "                    init_y_buffer = torch.cat((init_y_buffer, y_batch.view(-1, 1)), dim=0)\n",
    "\n",
    "                    loss = loss_sample(out, y_batch).sum()\n",
    "                    next_batch_l0_grads = torch.autograd.grad(loss, out)[0]\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        if dataset in ['MNIST', 'FMNIST']:\n",
    "                            next_batch_l0_expand = torch.repeat_interleave(next_batch_l0_grads, 256, dim=1)\n",
    "                        else:\n",
    "                            next_batch_l0_expand = torch.repeat_interleave(next_batch_l0_grads, 512, dim=1)\n",
    "                        next_batch_l1_grads = next_batch_l0_expand * emb.repeat(1, n_class)\n",
    "\n",
    "                        batch_l0_grads = torch.cat((batch_l0_grads, next_batch_l0_grads), dim=0)\n",
    "                        batch_l1_grads = torch.cat((batch_l1_grads, next_batch_l1_grads), dim=0)\n",
    "\n",
    "            # individual gradients\n",
    "            ind_l0_grads = batch_l0_grads.clone()\n",
    "            ind_l1_grads = batch_l1_grads.clone()\n",
    "            \n",
    "            new_l0_grads = torch.cat((new_l0_grads, ind_l0_grads), dim=0)\n",
    "            new_l1_grads = torch.cat((new_l1_grads, ind_l1_grads), dim=0)\n",
    "                    \n",
    "            # mean gradients\n",
    "            batch_l0_grads = batch_l0_grads.mean(dim=0).view(1, -1)\n",
    "            batch_l1_grads = batch_l1_grads.mean(dim=0).view(1, -1)\n",
    "\n",
    "            buffer_l0_grads = torch.cat((buffer_l0_grads, batch_l0_grads), dim=0)\n",
    "            buffer_l1_grads = torch.cat((buffer_l1_grads, batch_l1_grads), dim=0)\n",
    "\n",
    "            # mean loss computation\n",
    "            buffer_ds = TensorDataset(x_new_buffer, y_new_buffer)\n",
    "            buffer_loader = DataLoader(dataset=buffer_ds, batch_size=batch_size, shuffle=True)\n",
    "            buffer_output, buffer_loss = clf.evaluate(buffer_loader)\n",
    "            buffer_losses.append(buffer_loss)\n",
    "            \n",
    "#                     print('memory4:', torch.cuda.memory_allocated(device=device)/1024/1024/1024)\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        buffer_grads = torch.cat((buffer_l0_grads, buffer_l1_grads), dim=1)\n",
    "        buffer_grads = f.normalize(buffer_grads, p=2, dim=1)\n",
    "\n",
    "        ######################################\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            new_grads = torch.cat((new_l0_grads, new_l1_grads), dim=1)\n",
    "\n",
    "            new_grads_origin = new_grads.clone()\n",
    "            new_grads = f.normalize(new_grads, p=2, dim=1)\n",
    "#                         print('new grads:', new_grads.shape)\n",
    "#                         print('new grads norm:', torch.norm(new_grads, dim=1))\n",
    "#                         print('new grads shape:', new_grads, new_grads.shape)\n",
    "\n",
    "            buffer_losses = torch.tensor(buffer_losses).view(1,-1)\n",
    "#             print('initial loss:', buffer_losses)\n",
    "#             print('initial mean, std:', buffer_losses.mean(dim=1).item(), buffer_losses.std(dim=1).item())\n",
    "\n",
    "            loss_matrix = buffer_losses.repeat(len(new_grads), 1).to(device)\n",
    "            loss_matrix_origin = loss_matrix.clone()\n",
    "            forget_matrix = torch.matmul(new_grads, torch.transpose(buffer_grads, 0, 1)).to(device)\n",
    "\n",
    "#                     print('init forget matrix shape:', forget_matrix.shape)\n",
    "\n",
    "#                     print('init memory:', torch.cuda.memory_allocated(device=device)/1024/1024/1024)\n",
    "\n",
    "        accumulate_select_indexes = []\n",
    "        accumulate_mean = []\n",
    "        accumulate_std = []\n",
    "        accumulate_sum = []\n",
    "\n",
    "        select_indexes = []\n",
    "        non_select_indexes = list(range(len(x_train_batch)))\n",
    "\n",
    "        num_class1 = 0\n",
    "        num_class2 = 0\n",
    "\n",
    "        # current data selection\n",
    "        for b in range(len(x_train_batch)):\n",
    "            torch.cuda.empty_cache()\n",
    "            loss_matrix = loss_matrix - alpha * forget_matrix\n",
    "            loss_mean = torch.mean(loss_matrix, dim=1, keepdim=True)\n",
    "            loss_std = torch.std(loss_matrix, dim=1, keepdim=True)\n",
    "\n",
    "            # select_ind = torch.argmin(loss_mean, dim=0)\n",
    "            # select_ind = torch.argmin(loss_std, dim=0)\n",
    "            select_ind = torch.argmin(loss_mean + loss_std, dim=0)\n",
    "\n",
    "            accumulate_mean.append(copy.deepcopy(loss_mean[select_ind].item()))\n",
    "            accumulate_std.append(copy.deepcopy(loss_std[select_ind].item()))\n",
    "            accumulate_sum.append(copy.deepcopy(loss_mean[select_ind].item() + loss_std[select_ind].item()))\n",
    "\n",
    "            if non_select_indexes[select_ind.item()] < len(x_train_batch)/2:\n",
    "                num_class1 += 1\n",
    "            else:\n",
    "                num_class2 += 1\n",
    "\n",
    "            # metrics인듯?\n",
    "            select_indexes.append(non_select_indexes[select_ind.item()])\n",
    "            accumulate_select_indexes.append(copy.deepcopy(select_indexes))\n",
    "            del non_select_indexes[select_ind.item()]\n",
    "\n",
    "            best_buffer_losses = loss_matrix[select_ind].view(1,-1)\n",
    "            loss_matrix = best_buffer_losses.repeat(len(new_grads)-1, 1).to(device)\n",
    "            new_grads = torch.cat((new_grads[:select_ind.item()], new_grads[select_ind.item()+1:]))\n",
    "            forget_matrix = torch.cat((forget_matrix[:select_ind.item()], forget_matrix[select_ind.item()+1:]))\n",
    "        best_ind = np.argmin(np.array(accumulate_sum))\n",
    "        select_curr_indexes = accumulate_select_indexes[best_ind]\n",
    "\n",
    "        # best_ind=11999\n",
    "        # len(select_curr_indexes)=12000\n",
    "        # len(accumulate_select_indexes)=12000\n",
    "\n",
    "\n",
    "        # best_ind=7337\n",
    "        # len(select_curr_indexes)=7338\n",
    "        # len(accumulate_select_indexes)=12000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # buffer data selection\n",
    "        select_buffer_indexes = []\n",
    "\n",
    "        class1_indexes = list(range(0, int(len(x_train_batch)/2)))\n",
    "        class2_indexes = list(range(int(len(x_train_batch)/2), len(x_train_batch)))\n",
    "\n",
    "        class1_grad_mean = new_grads_origin[class1_indexes].mean(dim=0).view(1, -1)\n",
    "        class2_grad_mean = new_grads_origin[class2_indexes].mean(dim=0).view(1, -1)\n",
    "\n",
    "        candidate_class1_indexes = list(range(0, int(len(x_train_batch)/2)))\n",
    "        class1_buffer_indexes = []\n",
    "        for m in range(buffer_size):\n",
    "            buffer_ind = torch.argmin(\n",
    "                torch.norm(class1_grad_mean.repeat(int(len(new_grads_origin)/2)-m, 1) \\\n",
    "                        - (torch.sum(new_grads_origin[class1_buffer_indexes], dim=0)\n",
    "                              .repeat(int(len(new_grads_origin)/2)-m,1) \\\n",
    "                        + new_grads_origin[candidate_class1_indexes])/(m+1), dim=1), dim=0)\n",
    "            class1_buffer_indexes.append(copy.deepcopy(candidate_class1_indexes[buffer_ind]))\n",
    "            del candidate_class1_indexes[buffer_ind]\n",
    "\n",
    "        candidate_class2_indexes = list(range(int(len(x_train_batch)/2), len(x_train_batch)))\n",
    "        class2_buffer_indexes = []\n",
    "        for m in range(buffer_size):\n",
    "            buffer_ind = torch.argmin(torch.norm(class2_grad_mean.repeat(int(len(new_grads_origin)/2)-m, 1) \\\n",
    "                            - (torch.sum(new_grads_origin[class2_buffer_indexes], dim=0)\n",
    "                               .repeat(int(len(new_grads_origin)/2)-m,1) \\\n",
    "                            + new_grads_origin[candidate_class2_indexes])/(m+1), dim=1), dim=0)\n",
    "            class2_buffer_indexes.append(copy.deepcopy(candidate_class2_indexes[buffer_ind]))\n",
    "            del candidate_class2_indexes[buffer_ind]\n",
    "\n",
    "        for ind in class1_buffer_indexes:\n",
    "            select_buffer_indexes.append(ind)\n",
    "\n",
    "        for ind in class2_buffer_indexes:\n",
    "            select_buffer_indexes.append(ind)\n",
    "\n",
    "        select_curr_indexes = list(set(select_curr_indexes))\n",
    "\n",
    "        # current data selection\n",
    "        x_train_batch = x_train_batch[select_curr_indexes]\n",
    "        y_train_batch = y_train_batch[select_curr_indexes]\n",
    "\n",
    "    x_train_batch = torch.Tensor(x_train_batch).to(device, dtype=torch.float32)\n",
    "    y_train_batch = torch.Tensor(y_train_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "    train_ds = TensorDataset(x_train_batch, y_train_batch)\n",
    "    train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer_config = {\"lr\": 0.001}\n",
    "    \n",
    "    model.zero_grad()\n",
    "\n",
    "    # Model training using current data and buffer data\n",
    "    if i == 0:\n",
    "        clf = NNClassifier(model, nn.CrossEntropyLoss(reduction='mean'), optim.Adam, optimizer_config)\n",
    "        clf.fit({\"train\": train_loader, \"val\": train_loader}, epochs=1, earlystop_path=f'./ckpt/joint.pt')\n",
    "    elif i > 0:\n",
    "#                     print('len prev buffer:', len(x_prev_buffer))\n",
    "        clf = NNClassifier_CL(model, nn.CrossEntropyLoss(reduction='mean'), optim.Adam, optimizer_config)\n",
    "        clf.fit({\"train\": train_loader, \"val\": train_loader, \"buffer\": (x_prev_buffer, y_prev_buffer)}, \n",
    "                epochs=1, sample_size=64, lamb=lamb, device=device, earlystop_path=f'./ckpt/joint.pt', seed=s)\n",
    "\n",
    "    # Model evaluation\n",
    "    all_test_acc = []\n",
    "    all_test_loss = []\n",
    "\n",
    "    # evaluation using sci-kit learn tool\n",
    "    for j in range(n_class):\n",
    "\n",
    "        if j < i*2+2:\n",
    "\n",
    "            x_test_batch = all_test_data[j]\n",
    "            y_test_batch = all_test_label[j]\n",
    "\n",
    "            x_test_batch = torch.Tensor(x_test_batch).to(device, dtype=torch.float32)\n",
    "            y_test_batch = torch.Tensor(y_test_batch).to(device, dtype=torch.int64)\n",
    "\n",
    "            test_ds = TensorDataset(x_test_batch, y_test_batch)\n",
    "            test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            test_output, test_loss = clf.evaluate(test_loader)\n",
    "            test_acc = accuracy_score(test_output['true_y'], test_output['output'])\n",
    "\n",
    "            cf_li = list(confusion_matrix(test_output['true_y'], test_output['output'], labels=range(i*2+2))[j])\n",
    "\n",
    "            cf_matrix = {}\n",
    "            for k in range(len(cf_li)):\n",
    "                cf_matrix[label_li[k]] = cf_li[k]\n",
    "\n",
    "            all_test_acc.append(test_acc)\n",
    "            all_test_loss.append(test_loss)\n",
    "            seq_acc[j].append(test_acc)\n",
    "\n",
    "        else:\n",
    "            seq_acc[j].append(0)\n",
    "\n",
    "#                 print('overall: acc = %.3f, fair = %.3f' %(np.mean(all_test_acc), np.std(all_test_acc)))\n",
    "    overall_acc.append(np.mean(all_test_acc))\n",
    "    overall_fair.append(np.std(all_test_acc))\n",
    "\n",
    "\n",
    "all_overall_acc.append(np.mean(overall_acc))\n",
    "all_overall_fair.append(np.mean(overall_fair))\n",
    "\n",
    "alpha_dict[alpha].append([np.mean(all_overall_acc), np.mean(all_overall_fair)])\n",
    "print(alpha)\n",
    "\n",
    "print('alpha:', alpha, alpha_dict[alpha])\n",
    "print('avg:', np.mean([e[0] for e in alpha_dict[alpha]]), np.mean([e[1] for e in alpha_dict[alpha]]))\n",
    "print('\\n')\n",
    "\n",
    "print('-------------------------------------------------------------------------------------------------')\n",
    "        \n",
    "for k, v in alpha_dict.items():\n",
    "    print(k, v)\n",
    "    print('avg:', np.mean([e[0] for e in v]), np.mean([e[1] for e in v]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b0074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
