{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c3b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cl_gym as cl\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "seed = 10\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed) \n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# torch.backends.cudnn.enabled = False\n",
    "torch.set_num_threads(6)\n",
    "\n",
    "\n",
    "def make_params() -> dict:\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import uuid\n",
    "\n",
    "    params = {\n",
    "            # dataset\n",
    "            'dataset': \"BiasedMNIST\",\n",
    "            'fairness_agg': 'mean',\n",
    "            'model': 'MLP',\n",
    "\n",
    "            # benchmark\n",
    "            'seed': seed,\n",
    "            'num_tasks': 5,\n",
    "            'epochs_per_task': 15,\n",
    "            'per_task_examples': np.inf,\n",
    "            # 'per_task_examples': 10000,\n",
    "            'per_task_memory_examples': 128,\n",
    "            'batch_size_train': 64,\n",
    "            'batch_size_memory': 64,\n",
    "            'batch_size_validation': 256,\n",
    "            'tau': 10.0,\n",
    "\n",
    "            # algorithm\n",
    "            'optimizer': 'sgd',\n",
    "            'learning_rate': 0.001,\n",
    "            'momentum': 0.9,\n",
    "            'learning_rate_decay': 1.0,\n",
    "            'criterion': torch.nn.CrossEntropyLoss(),\n",
    "            'device': torch.device('cuda:7' if torch.cuda.is_available() else 'cpu'),\n",
    "             \n",
    "            # sample selection\n",
    "            'alpha': 0.00,\n",
    "            'metric' : \"EO\",\n",
    "            'lambda': 1.0,\n",
    "            'lambda_old': 0.0,\n",
    "              }\n",
    "    \n",
    "\n",
    "\n",
    "#     trial_id = str(uuid.uuid4())\n",
    "    trial_id = f\"demo/dataset={params['dataset']}/seed={params['seed']}_epoch={params['epochs_per_task']}_lr={params['learning_rate']}_tau={params['tau']}_alpha={params['alpha']}\"\n",
    "    if params['lambda'] != 0:\n",
    "        trial_id+=f\"_lmbd_{params['lambda']}_lmbdold_{params['lambda_old']}\"\n",
    "    params['trial_id'] = trial_id\n",
    "    params['output_dir'] = os.path.join(\"./outputs/{}\".format(trial_id))\n",
    "    print(f\"output_dir={params['output_dir']}\")\n",
    "    Path(params['output_dir']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return params\n",
    "\n",
    "params = make_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99826537",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"MNIST\" in params['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c7b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import BiasedMNIST\n",
    "\n",
    "if  params['dataset'] in [\"BiasedMNIST\"]:\n",
    "    benchmark = BiasedMNIST(num_tasks=params['num_tasks'],\n",
    "                                per_task_memory_examples=params['per_task_memory_examples'],\n",
    "                                per_task_examples = params['per_task_examples'],\n",
    "                                random_class_idx=False)\n",
    "    input_dim = (3, 28, 28)\n",
    "    class_idx = benchmark.class_idx\n",
    "    num_classes = len(class_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b2ddb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fd9653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "from cl_gym.benchmarks.transforms import MNIST_MEAN, MNIST_STD\n",
    "COLOR_MAP = {\n",
    "    0: (1, 0, 0),\n",
    "    1: (0, 1, 0),\n",
    "    2: (1, 1, 0),\n",
    "    3: (0, 0, 1),\n",
    "    4: (1, 0.65, 0),\n",
    "    5: (0.5, 0, 0.5),\n",
    "    6: (0, 1, 1),\n",
    "    7: (1, 0.75, 0.8),\n",
    "    8: (0.8, 1, 0),\n",
    "    9: (.588, .294, 0.)\n",
    "}\n",
    "\n",
    "r = (1 - 0.1913)\n",
    "color_values = np.array(list(COLOR_MAP.values()))\n",
    "m_rgb = color_values.mean(axis=0)\n",
    "std_rgb = color_values.std(axis=0)\n",
    "\n",
    "bmnist_mean = [r*m + MNIST_MEAN[0] for m in m_rgb]\n",
    "bmnist_std = [(r*s**2+(1-r)*MNIST_STD[0]**2+r*(1-r)*(bmnist_mean[i] - m_rgb[i])**2)**0.5 for i, s in enumerate(std_rgb)]\n",
    "\n",
    "unnormalize = torchvision.transforms.Normalize([-m/s for m, s in zip(bmnist_mean, bmnist_std)], [1/s for s in bmnist_std])\n",
    "sample_idx = 95\n",
    "to_pil_image(unnormalize(benchmark.trains[2][sample_idx][0]), mode=\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8806699",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label = 0\n",
    "incremental_step = 1\n",
    "# cat_img = torch.cat([img for img in benchmark.trains[incremental_step].inputs[benchmark.trains[incremental_step].targets == target_label][20:30]], dim=2)\n",
    "cat_img = torch.cat([img for img, target, *_ in benchmark.tests[incremental_step]][20:30], dim=2)\n",
    "to_pil_image(unnormalize(cat_img), mode=\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainers import FairContinualTrainer\n",
    "from trainers.fair_trainer import FairContinualTrainer1\n",
    "# from trainers.baselines import BaseMemoryContinualTrainer as ContinualTrainer\n",
    "from metrics import FairMetricCollector\n",
    "from algorithms.fairl import FaIRL\n",
    "from algorithms.fairl import FaIRL\n",
    "from backbones import MLP2Layers2\n",
    "\n",
    "backbone = MLP2Layers2(\n",
    "    input_dim=input_dim, \n",
    "    hidden_dim_1=256, \n",
    "    hidden_dim_2=256, \n",
    "    output_dim=num_classes,\n",
    "    class_idx=class_idx,\n",
    "    config=params\n",
    "    ).to(params['device'])\n",
    "algorithm = FaIRL(backbone, benchmark, params, requires_memory=True)\n",
    "metric_manager_callback = FairMetricCollector(num_tasks=params['num_tasks'],\n",
    "                                                        eval_interval='epoch',\n",
    "                                                        epochs_per_task=params['epochs_per_task'])\n",
    "\n",
    "trainer = FairContinualTrainer1(algorithm, params, callbacks=[metric_manager_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4bc2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if params['fairness_agg'] == \"mean\":\n",
    "    agg = np.mean\n",
    "elif params['fairness_agg'] == \"max\":\n",
    "    agg = np.max\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "fairness_metrics = [\"std\", \"EER\", \"EO\", \"DP\"]\n",
    "for metric in metric_manager_callback.meters:\n",
    "    if metric in fairness_metrics:\n",
    "        metric_manager_callback.meters[metric].agg = agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eac756",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run()\n",
    "print(\"final avg-acc\", metric_manager_callback.meters['accuracy'].compute_final())\n",
    "print(\"final avg-forget\", metric_manager_callback.meters['forgetting'].compute_final())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebde5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_manager_callback.meters['accuracy'].get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e893eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(metric_manager_callback.meters['accuracy'].compute_overall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c786ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.round(x, 3) for x in metric_manager_callback.meters['EO'].compute_overall()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82bd073",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(metric_manager_callback.meters['EO'].compute_overall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7973e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.round(x, 3) for x in metric_manager_callback.meters['DP'].compute_overall()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61271a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(metric_manager_callback.meters['DP'].compute_overall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b743813b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21212733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
