{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asNHiu3VMV0m"
      },
      "source": [
        "## CL-Gym Example: Stable A-GEM on Rotated MNIST\n",
        "\n",
        "In this example, we use Averaged Gradient Episodic Memory (A-GEM) to train on Rotated MNIST benchmark. We use the stable version of [AGEM](https://arxiv.org/abs/1812.00420.pdf) using [Stable SGD](https://proceedings.neurips.cc/paper/2020/file/518a38cc9a0173d0b2dc088166981cf8-Paper.pdf) parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkkjxEZcNf9o"
      },
      "source": [
        "## 1. Defining Parameters\n",
        "First, we need to define our parameters/config for our experiment.\n",
        "We define all our parameters inside a python dictionary. The parameters define different aspects of continual learning examples. For example:\n",
        "-  How many tasks should we learn?\n",
        "-  What our batch-size will be?\n",
        "-  What Optimizer will we use?\n",
        "-  Where should we store our outputs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sqM1kbU8xfFC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cl_gym as cl\n",
        "# first let's create params/config for our experiment\n",
        "\n",
        "def make_params() -> dict:\n",
        "    import os\n",
        "    from pathlib import Path\n",
        "    import uuid\n",
        "\n",
        "    params = {\n",
        "            # benchmark\n",
        "            'num_tasks': 5,\n",
        "            'epochs_per_task': 5,\n",
        "            'per_task_memory_examples': 25,\n",
        "            'batch_size_train': 64,\n",
        "            'batch_size_memory': 32,\n",
        "            'batch_size_validation': 256,\n",
        "\n",
        "            # algorithm\n",
        "            'optimizer': 'SGD',\n",
        "            'learning_rate': 0.01,\n",
        "            'momentum': 0.8,\n",
        "            'learning_rate_decay': 1.0,\n",
        "            'criterion': torch.nn.CrossEntropyLoss(),\n",
        "            'device': torch.device('cuda:2' if torch.cuda.is_available() else 'cpu'), }\n",
        "\n",
        "    trial_id = str(uuid.uuid4())\n",
        "    params['trial_id'] = trial_id\n",
        "    params['output_dir'] = os.path.join(\"./outputs/{}\".format(trial_id))\n",
        "    Path(params['output_dir']).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHxYEiSkONpn"
      },
      "source": [
        "## 2. Training our continual learning algorithm\n",
        "\n",
        "Before seeing the code, let's explain the components one more time:\n",
        "### 2.1 Benchmark\n",
        "* We use the `RotatedMNIST` benchmark for this example. The benchmark includes gradual rotations of MNIST digits for each task. Something like this.\n",
        "<div>\n",
        "<img src=\"https://user-images.githubusercontent.com/8312051/122752221-845a0300-d245-11eb-8892-7c4119ffe1a5.png\" width=\"300\"/>\n",
        "</div>\n",
        "\n",
        "In CL-Gym we use RotatedMNIST as follows:\n",
        "```python\n",
        "benchmark = cl.benchmarks.RotatedMNIST(num_tasks=5)\n",
        "```\n",
        "---------\n",
        "\n",
        "### 2.2 Backbone\n",
        "\n",
        "* We use a MLP model with two hidden layers like this:\n",
        "<div>\n",
        "<img src=\"https://user-images.githubusercontent.com/8312051/122753641-67beca80-d247-11eb-87d3-dec5cc2e63d6.png\" width=\"300\"/>\n",
        "</div>\n",
        "\n",
        "To import our backbone, we use:\n",
        "```python\n",
        "backbone = cl.backbones.MLP2Layers(input_dim=784, hidden_dim_1=100, hidden_dim_2=100, output_dim=10)\n",
        "```\n",
        "\n",
        "You can also create your own PyTorch models. The backbone in CL-Gym is simply a lightweight wrapper around PyTorch's ``nn.Module``.\n",
        "\n",
        "--------\n",
        "\n",
        "### 2.3 Collecting metrics with Callbacks\n",
        "\n",
        "The `MetricCollector` callback evaluates the model at the end of each epoch, logs the metrics, plots the accuraies to file, and stores the validation accuracies as numpy arrays to file (see outputs folder).\n",
        "```python\n",
        "metric_callback = cl.callbacks.MetricCollecto(num_tasks=5,\n",
        "                                              eval_interval='epoch',\n",
        "                                              epochs_per_task=1)\n",
        "```\n",
        "\n",
        "-------\n",
        "\n",
        "### 2.4  Using off-the-shelf continual learning algorithms\n",
        "\n",
        "CL-Gym includes several continual learning algorithms. Here we use A-GEM algorithm with better parameters than the original paper:\n",
        "\n",
        "```python\n",
        "cl.algorithms.AGEM(backbone, benchmark, params)\n",
        "```\n",
        "\n",
        "You can also use other algorithms. For example, for Experience Replay method, you can use:\n",
        "```python\n",
        "cl.algorithms.ERRingBuffer(backbone, benchmark, params)\n",
        "```\n",
        "\n",
        "\n",
        "-------\n",
        "\n",
        "### 2.5 Gluing everything together with the Trainer\n",
        "\n",
        "The `Trainer` will orchestrate the experiment by handling the non-research part of continual learning experiments.\n",
        "\n",
        "```\n",
        "trainer = cl.trainer.ContinualTrainer(algorithm, params, callbacks=[metric_manager_callback])\n",
        "```\n",
        "\n",
        "\n",
        "The code below implements this note:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFLqD58bhJRf",
        "outputId": "a34cc040-4c26-4d0e-87e5-36bf4a005aff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------- Task 1 -----------------------\n",
            "[1] Eval metrics for task 1 >> {'accuracy': 92.92, 'loss': 0.000972979424893856}\n",
            "[2] Eval metrics for task 1 >> {'accuracy': 94.87, 'loss': 0.000679773323982954}\n",
            "[3] Eval metrics for task 1 >> {'accuracy': 95.72, 'loss': 0.0005617750629782677}\n",
            "[4] Eval metrics for task 1 >> {'accuracy': 96.35, 'loss': 0.0004930462460964918}\n",
            "[5] Eval metrics for task 1 >> {'accuracy': 96.32, 'loss': 0.00048549841716885567}\n",
            "---------------------------- Task 2 -----------------------\n",
            "[6] Eval metrics for task 1 >> {'accuracy': 81.74, 'loss': 0.0023741892993450163}\n",
            "[6] Eval metrics for task 2 >> {'accuracy': 94.54, 'loss': 0.0007384905070066452}\n",
            "[7] Eval metrics for task 1 >> {'accuracy': 73.25, 'loss': 0.004258131301403046}\n",
            "[7] Eval metrics for task 2 >> {'accuracy': 95.89, 'loss': 0.0005414123661816121}\n",
            "[8] Eval metrics for task 1 >> {'accuracy': 74.08, 'loss': 0.00424471988081932}\n",
            "[8] Eval metrics for task 2 >> {'accuracy': 96.36, 'loss': 0.0004801077958196402}\n",
            "[9] Eval metrics for task 1 >> {'accuracy': 70.97, 'loss': 0.004958374619483947}\n",
            "[9] Eval metrics for task 2 >> {'accuracy': 95.91, 'loss': 0.0005455406129360199}\n",
            "[10] Eval metrics for task 1 >> {'accuracy': 68.89, 'loss': 0.005779934692382812}\n",
            "[10] Eval metrics for task 2 >> {'accuracy': 96.25, 'loss': 0.0005461489126086235}\n",
            "---------------------------- Task 3 -----------------------\n",
            "[11] Eval metrics for task 1 >> {'accuracy': 53.77, 'loss': 0.008753745174407959}\n",
            "[11] Eval metrics for task 2 >> {'accuracy': 92.14, 'loss': 0.0009852416925132275}\n",
            "[11] Eval metrics for task 3 >> {'accuracy': 94.75, 'loss': 0.000657674091681838}\n",
            "[12] Eval metrics for task 1 >> {'accuracy': 50.04, 'loss': 0.009997680556774139}\n",
            "[12] Eval metrics for task 2 >> {'accuracy': 88.97, 'loss': 0.001372262355685234}\n",
            "[12] Eval metrics for task 3 >> {'accuracy': 95.34, 'loss': 0.0006062506251037121}\n",
            "[13] Eval metrics for task 1 >> {'accuracy': 48.95, 'loss': 0.010461851215362548}\n",
            "[13] Eval metrics for task 2 >> {'accuracy': 86.11, 'loss': 0.0017097686380147933}\n",
            "[13] Eval metrics for task 3 >> {'accuracy': 96.03, 'loss': 0.000506413053162396}\n",
            "[14] Eval metrics for task 1 >> {'accuracy': 44.9, 'loss': 0.011331180572509765}\n",
            "[14] Eval metrics for task 2 >> {'accuracy': 87.09, 'loss': 0.0016155930742621422}\n",
            "[14] Eval metrics for task 3 >> {'accuracy': 96.1, 'loss': 0.0005076381403952836}\n",
            "[15] Eval metrics for task 1 >> {'accuracy': 46.12, 'loss': 0.010737320137023925}\n",
            "[15] Eval metrics for task 2 >> {'accuracy': 84.29, 'loss': 0.0020224670767784118}\n",
            "[15] Eval metrics for task 3 >> {'accuracy': 96.55, 'loss': 0.0004647137176245451}\n",
            "---------------------------- Task 4 -----------------------\n",
            "[16] Eval metrics for task 1 >> {'accuracy': 40.34, 'loss': 0.01189672224521637}\n",
            "[16] Eval metrics for task 2 >> {'accuracy': 60.49, 'loss': 0.0057020501792430876}\n",
            "[16] Eval metrics for task 3 >> {'accuracy': 91.7, 'loss': 0.0010884998127818108}\n",
            "[16] Eval metrics for task 4 >> {'accuracy': 94.45, 'loss': 0.000789185593277216}\n",
            "[17] Eval metrics for task 1 >> {'accuracy': 36.16, 'loss': 0.012633422696590424}\n",
            "[17] Eval metrics for task 2 >> {'accuracy': 54.65, 'loss': 0.007121121501922607}\n",
            "[17] Eval metrics for task 3 >> {'accuracy': 87.9, 'loss': 0.0015905035823583604}\n",
            "[17] Eval metrics for task 4 >> {'accuracy': 95.66, 'loss': 0.0005743102446198464}\n",
            "[18] Eval metrics for task 1 >> {'accuracy': 35.86, 'loss': 0.012451213979721069}\n",
            "[18] Eval metrics for task 2 >> {'accuracy': 55.49, 'loss': 0.007012682056427002}\n",
            "[18] Eval metrics for task 3 >> {'accuracy': 86.6, 'loss': 0.0017485393419861793}\n",
            "[18] Eval metrics for task 4 >> {'accuracy': 95.88, 'loss': 0.0005798313297331333}\n",
            "[19] Eval metrics for task 1 >> {'accuracy': 33.21, 'loss': 0.013509610891342163}\n",
            "[19] Eval metrics for task 2 >> {'accuracy': 53.21, 'loss': 0.007490606248378754}\n",
            "[19] Eval metrics for task 3 >> {'accuracy': 83.68, 'loss': 0.0021432869404554365}\n",
            "[19] Eval metrics for task 4 >> {'accuracy': 96.46, 'loss': 0.00047205988578498365}\n",
            "[20] Eval metrics for task 1 >> {'accuracy': 32.31, 'loss': 0.013175762629508972}\n",
            "[20] Eval metrics for task 2 >> {'accuracy': 52.83, 'loss': 0.007724464631080627}\n",
            "[20] Eval metrics for task 3 >> {'accuracy': 82.47, 'loss': 0.0023581776589155197}\n",
            "[20] Eval metrics for task 4 >> {'accuracy': 96.13, 'loss': 0.0005115894969552756}\n",
            "---------------------------- Task 5 -----------------------\n",
            "[21] Eval metrics for task 1 >> {'accuracy': 24.97, 'loss': 0.017900068736076354}\n",
            "[21] Eval metrics for task 2 >> {'accuracy': 50.73, 'loss': 0.00691539717912674}\n",
            "[21] Eval metrics for task 3 >> {'accuracy': 61.46, 'loss': 0.005344351309537887}\n",
            "[21] Eval metrics for task 4 >> {'accuracy': 81.73, 'loss': 0.002385029190778732}\n",
            "[21] Eval metrics for task 5 >> {'accuracy': 94.71, 'loss': 0.0006962538205087185}\n",
            "[22] Eval metrics for task 1 >> {'accuracy': 26.32, 'loss': 0.018026182413101197}\n",
            "[22] Eval metrics for task 2 >> {'accuracy': 44.29, 'loss': 0.00844734092950821}\n",
            "[22] Eval metrics for task 3 >> {'accuracy': 57.65, 'loss': 0.006241409611701965}\n",
            "[22] Eval metrics for task 4 >> {'accuracy': 78.66, 'loss': 0.0027381992906332017}\n",
            "[22] Eval metrics for task 5 >> {'accuracy': 95.94, 'loss': 0.0005288150433450938}\n",
            "[23] Eval metrics for task 1 >> {'accuracy': 26.13, 'loss': 0.019438474416732788}\n",
            "[23] Eval metrics for task 2 >> {'accuracy': 40.22, 'loss': 0.009738550186157227}\n",
            "[23] Eval metrics for task 3 >> {'accuracy': 52.17, 'loss': 0.007544950032234192}\n",
            "[23] Eval metrics for task 4 >> {'accuracy': 75.07, 'loss': 0.003317468249797821}\n",
            "[23] Eval metrics for task 5 >> {'accuracy': 96.05, 'loss': 0.0005045794058591127}\n",
            "[24] Eval metrics for task 1 >> {'accuracy': 25.05, 'loss': 0.020349068307876587}\n",
            "[24] Eval metrics for task 2 >> {'accuracy': 40.75, 'loss': 0.009626597356796265}\n",
            "[24] Eval metrics for task 3 >> {'accuracy': 52.52, 'loss': 0.007735232675075531}\n",
            "[24] Eval metrics for task 4 >> {'accuracy': 74.66, 'loss': 0.0034154058158397675}\n",
            "[24] Eval metrics for task 5 >> {'accuracy': 96.31, 'loss': 0.0004893404887057841}\n",
            "[25] Eval metrics for task 1 >> {'accuracy': 24.49, 'loss': 0.019291391348838807}\n",
            "[25] Eval metrics for task 2 >> {'accuracy': 37.36, 'loss': 0.01072963581085205}\n",
            "[25] Eval metrics for task 3 >> {'accuracy': 49.19, 'loss': 0.008518594717979432}\n",
            "[25] Eval metrics for task 4 >> {'accuracy': 74.51, 'loss': 0.0034106424331665037}\n",
            "[25] Eval metrics for task 5 >> {'accuracy': 96.72, 'loss': 0.00043869449868798256}\n",
            "final avg-acc 56.45399999999999\n",
            "final avg-forget 49.925\n"
          ]
        }
      ],
      "source": [
        "def train(params):\n",
        "    # benchmark: Rotated MNIST\n",
        "    benchmark = cl.benchmarks.PermutedMNIST(num_tasks=params['num_tasks'],\n",
        "                                           per_task_memory_examples=params['per_task_memory_examples'],)\n",
        "    \n",
        "    # backbone: MLP with 2 hidden layers\n",
        "    backbone = cl.backbones.MLP2Layers(input_dim=784, hidden_dim_1=32, hidden_dim_2=32, output_dim=10)\n",
        "\n",
        "    # Algorithm: A-GEM\n",
        "    algorithm = cl.algorithms.AGEM(backbone, benchmark, params)\n",
        "    # algorithm = cl.algorithms.ERRingBuffer(backbone, benchmark, params)\n",
        "\n",
        "    # Callbacks\n",
        "    metric_manager_callback = cl.callbacks.MetricCollector(num_tasks=params['num_tasks'],\n",
        "                                                           eval_interval='epoch',\n",
        "                                                           epochs_per_task=params['epochs_per_task'])\n",
        "\n",
        "    # Make trainer\n",
        "    trainer = cl.trainer.ContinualTrainer(algorithm, params, callbacks=[metric_manager_callback])\n",
        "\n",
        "    trainer.run()\n",
        "    print(\"final avg-acc\", metric_manager_callback.meters['accuracy'].compute_final())\n",
        "    print(\"final avg-forget\", metric_manager_callback.meters['forgetting'].compute_final())\n",
        "\n",
        "\n",
        "params = make_params()\n",
        "train(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ibbA2luc-AZe"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (461557611.py, line 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    [25] Eval metrics for task 1 >> {'accuracy': 44.18, 'loss': 0.013980306243896484}\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "ref\n",
        "\n",
        "[25] Eval metrics for task 1 >> {'accuracy': 57.84, 'loss': 0.009022631740570069}\n",
        "[25] Eval metrics for task 2 >> {'accuracy': 74.56, 'loss': 0.004014899867773056}\n",
        "[25] Eval metrics for task 3 >> {'accuracy': 83.45, 'loss': 0.002212368157505989}\n",
        "[25] Eval metrics for task 4 >> {'accuracy': 89.72, 'loss': 0.0013752082452178002}\n",
        "[25] Eval metrics for task 5 >> {'accuracy': 96.04, 'loss': 0.000493720968067646}\n",
        "final avg-acc 80.32200000000002\n",
        "final avg-forget 19.782499999999995\n",
        "\n",
        "batch\n",
        "[25] Eval metrics for task 1 >> {'accuracy': 24.49, 'loss': 0.019291391348838807}\n",
        "[25] Eval metrics for task 2 >> {'accuracy': 37.36, 'loss': 0.01072963581085205}\n",
        "[25] Eval metrics for task 3 >> {'accuracy': 49.19, 'loss': 0.008518594717979432}\n",
        "[25] Eval metrics for task 4 >> {'accuracy': 74.51, 'loss': 0.0034106424331665037}\n",
        "[25] Eval metrics for task 5 >> {'accuracy': 96.72, 'loss': 0.00043869449868798256}\n",
        "final avg-acc 56.45399999999999\n",
        "final avg-forget 49.925\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
